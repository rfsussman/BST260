[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BST 260 Introduction to Data Science",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#downloading-course-materials-using-git",
    "href": "index.html#downloading-course-materials-using-git",
    "title": "BST 260 Introduction to Data Science",
    "section": "Downloading course materials using Git",
    "text": "Downloading course materials using Git\nYou can download the quarto files used to create the course notes using Git. You can update files using git pull but you will not be able to change the course notes on the main repository. This means that if you edit the files and then try to update then using git pull you will encounter conflicts. For this reason recommend that you make a copy before editing files. We have edited the .gitignore file so that if you add the word notes to your filenames, git will not track the files. So we recommend that you before editing you make a copy of the file and notes to the filename. For example 01-unix.qmd to 01-unix-notes.qmd.\nYou can download the files using git clone like this:\n\nOpen a terminal and navigate to the directory you want to keep these notes in.\nType git clone  https://github.com/datasciencelabs/2023.git\n\nor using RStudio like this:\n\nGot to https://github.com/datasciencelabs/2023\nClick on the green “Clone or Download” on Github and copy the link.\nOpen RStudio, and go to File &gt; New Project &gt; Version Control &gt; Git, and paste in the link you just copied. Under “Create Project as Sub-directory of”, browse and select a folder where you want the course materials to go.\nPress “Create Project”. This will create a folder called 2023 in the folder you selected in step 3.\nNow, you can open this project using the projects tab in the upper right of RStudio, or going to File &gt; Open Project and then navigating to the 2023 folder and opening the .Rproj file.\n\nOnce you cloned the course repository and want to get updates, you must use git pull to get updates. You can do this in the terminal or on the RStudio’s Git pane.\n\nAssociating an existing directory\nIf you already cloned the repository outside of RStudio, you can associate the directory that was created in that step with RStudio. In RStudio, go to File &gt; New Project &gt; Existing Directory, and then navigate / click on the 2023 folder. Then click “Create Project”. Then you can follow step 5 above to open the project when you launch RStudio.\n\n\nForking the repository\nAn alternative more advanced way to cloning the directory is creating a fork. Forking a repository on GitHub allows you to create a copy of a project under your own GitHub account. This lets you make changes without affecting the original repository. Here’s how you can fork a repository on GitHub:\n\nLog In to GitHub:\n\nMake sure you are logged in to your GitHub account.\n\nNavigate to the Repository:\n\nGo to the main page of the repository you want to fork: https://github.com/datasciencelabs/2023\n\nClick the ‘Fork’ Button:\n\nIn the top-right corner of the repository’s page, you’ll find the “Fork” button. Click on it.\n\nChoose an Account:\n\nIf you are a member of any organizations, GitHub will ask you where you’d like to fork the repository. Choose your personal account unless you want to fork it to an organization.\n\nWait for the Forking Process to Complete:\n\nGitHub will then create a copy of the repository in your account. You’ll see an animation indicating the process, and once it’s done, you’ll be redirected to the forked repository under your account.\n\nClone Your Forked Repository:\n\nTo work with the forked repository on your local machine, you can clone it. Navigate to the main page of your forked repo, click on the green “Code” button, copy the URL, and then use the following command in your terminal or command prompt:\ngit clone [URL_you_copied]\n\n\nYou can continue to update the forked repository by doing the following:\n\nNavigate to Your Local Repository:\n\nOpen a terminal or command prompt.\nNavigate to the directory where you have your forked repository.\n\nAdd the Original Repository as an Upstream Remote:\n\nUse the following command to add the original repository as an upstream remote:\ngit remote add upstream [URL_of_original_repository]\nFor example, if the original repository’s URL is https://github.com/original-owner/original-repo.git, the command would be:\ngit remote add upstream https://github.com/original-owner/original-repo.git\n\nFetch Changes from the Upstream:\n\nUse the following command to fetch changes from the upstream:\ngit fetch upstream\n\nMerge Changes into Your Local Branch:\n\nFirst, ensure you are on the branch into which you want to merge the upstream changes, typically the main or master branch:\ngit checkout main\nThen, merge the changes from the upstream’s main or master branch:\ngit merge upstream/main\n\nPush Changes to Your Forked Repository on GitHub (if needed):\n\nIf you want these changes to reflect in your GitHub fork, push them:\ngit push origin main\n\n\nNow your fork is synchronized with the original repository. Whenever you want to pull in new changes from the original repository in the future, you just need to repeat steps 3-5.\nTo avoid conflicts you sill want to avoid editing the course notes files and instead make copies."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Course description",
    "section": "",
    "text": "This course introduces UNIX/Linux shell, version control with git and GitHub, R programming, data wrangling with dplyr and data.table, data visualization with ggplot2 and shiny, and reproducible document preparation with RStudio, knitr and markdown. We briefly introduce Monte Carlo simulations, statistical modeling, high-dimensional data techniques, and machine learning and how these are applied to real data. Throughout the course, we use motivating case studies and data analysis problem sets based on challenges similar to those you encounter in scientific research.\nLectures will be mostly live coding. We will go over exercises and challenges together but will pause 1-4 times per lectures so students can complete exercises on their own. The midterm questions will be selected from the exercises presented in class. Some time will be dedicated to answering problem set questions. Lectures will not be recorded.\nStudents are required to have a GitHub account and create a repository for the course.\nProblem sets are mostly composed of open ended questions. Submission should be in the form of a scientific report. Problem set submission need to be completely reproducible. Specifically, students are expected to upload a Quarto document to their GitHub class repository that graders can compile into a readable report."
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The schedule is subject to change.\n\n\n\n\n\n\nDate\n\n\nModule\n\n\nTopics\n\n\n\n\nMon, Aug 28\n\n\nProductivity Tools\n\n\n\n RStudio, RStudio Projects, Quarto, Unix\n\n\n\n\n\nWed, Aug 30\n\n\nProductivity Tools\n\n\n\nGit and GitHub      \n\n\n\n\n\nMon, Sep 4\n\n\n\nNo class\n\n\n\nLabor day\n\n\n\n\nWed, Sep 6\n\n\n\nR\n\n\n\n\n\nR Basics: The workspace, data types, coercing, lists, packages, namespaces, help, creating vectors, object oriented programming.\n\n\nVectorization: Vector arithmetics, sapply, split, cut, lapply, subsetting, sorting\n\n\n\n\n\n\nMon, Sep 11\n\n\nR \n\n\n\n\nIntroduction to Tidyverse: tidy data, mutate, select, filter, the pipe, summarize, group_by, sorting, and the purrr package\n\n\nDates and time: Date class and the lubridate package\n\n\n\n\n\n\nWed, Sep 13\n\n\nR\n\n\n\n\nImporting data\n\n\nFile types: binary, ascii, unicode\n\n\nLocalesImporting data\n\n\nDownloading files\n\n\n\n\nThe data.table package\n\n\n\n\n\n\nMon, Sep 18\n\n\nData visualization\n\n\n\n\nVisualizing Distributions: Summary statistics, distributions, histograms, smooth densities, the normal distribution, quantiles, percentiles, and boxplots.\n\n\nGrammar of graphics and the basics of the ggplot2 package\n\n\n\n\n\n\nWed, Sep 20\n\n\nData visualization\n\n\n\n\nData visualization principles\n\n\nggplot2 geometries\n\n\n\n\n\n\nFri, Sep 22\n\n\nProblem set 1 due\n\n\n\n\n\n\nMon, Sep 25\n\n\nData wrangling \n\n\n\n\nReshaping data\n\n\nJoining tables\n\n\n\n\n\n\nWed, Sep 27\n\n\nData wrangling \n\n\n\n\nWeb scraping\n\n\nString processing\n\n\nText mining\n\n\n\n\n\n\n\nMon, Oct 2\n\n\n\nProbability\n\n\n\n\nMonte Carlo simulations\n\n\nRandom Variables\n\n\nCentral Limit Theorem\n\n\nProbability case studies: Roulette, Poker, Birthday problem, Monte Hall, insurance\n\n\n\n\n\n\n\nWed, Oct 4\n\n\n\nInference\n\n\n\n\nPolls\n\n\nGuess the proportion of blue beads competition\n\n\nConfidence intervals\n\n\nData-driven models\n\n\n\n\n\n\n\nFri, Oct 6\n\n\n\nFinal project title due\n\n\n\n Submit title and a describe your plans to obtain data\n\n\n\n\n\n\nMon, Oct 9\n\n\n\nNo class\n\n\nIndigenous Peoples’ Day\n\n\n\n\n\nWed, Oct 11\n\n\n\nInference\n\n\n\n\nBayesian statistics\n\n\nHierarchical Models\n\n\nCase study: election forecasting\n\n\n\n\n\n\n\nMon, Oct 16\n\n\n\nMidterm 1\n\n\nIncludes all topics covered by October 11.\n\n\n\n\n\nWed, Oct 18\n\n\n\nLinear Models\n\n\n\n\nRegression and correlation\n\n\nCase study: is height hereditary?\n\n\nBivariate normal distribution, conditional expectations, least squares estimates\n\n\n\n\n\n\n\n\n\nMon, Oct 23\n\n\n\nLinear Models\n\n\n\n\nMultivariable regression\n\n\nCase study: build a baseball team\n\n\n\n\n\n\n\n\n\nWed, Oct 25\n\n\n\nLinear Models\n\n\n\n\nMeasurement error models\n\n\nTreatment effect models \n\n\nCase study: does a high-fat diet increase weight in mice?\n\n\n\n\n\n\n\nMon, Oct 30\n\n\n\nLinear Models\n\n\n\n\nAssociation tests\n\n\nCorrelation is not causation\n\n\n\n\n\n\n\nWed, Nov 1\n\n\n\nHigh dimensional data\n\n\n\n\nMatrices in R \n\n\nCase study: handwritten digits\n\n\n\n\n\n\n\nFri, Nov 3 \n\n\n\nProblem set 2 due\n\n\nOne paragraph description of projects that includes what dataset will be used.\n\n\n\n\n\nMon, Nov 6\n\n\n\nHigh dimensional data\n\n\n\nDimension reduction: Linear algebra, distance, PCA\n\n\n\n\n\n\nWed, Nov 8\n\n\n\nHigh dimensional data\n\n\n\n\nDimension reduction continued\n\n\nCase study: gene expression differences between ethnic groups.\n\n\n\n\n\n\n\nFri, Nov 10 \n\n\n\nProject description due\n\n\nOne paragraph description of projects that includes what dataset will be used.\n\n\n\n\n\nMon, Nov 13\n\n\n\n\nHigh dimensional data\n\n\n\n\n\nRegularization\n\n\nCase study: Recommendations systems in  movie ratings\n\n\n\n\nMatrix factorization\n\n\n\n\n\n\n\nWed, Nov 15\n\n\n\nHigh dimensional data \n\n\n\n\nIntroduction, definition of concepts, accuracy, test, training and validation sets\n\n\nEvaluation metrics: ROC curves, precision recall curves\n\n\n\n\n\n\n\nMon, Nov 20\n\n\n\nMidterm 2\n\n\nIncludes topics covered until Nov 15.\n\n\n\n\n\nWed, Nov 22\n\n\n\nNo class\n\n\n Thanksgiving\n\n\n\n\n\nMon Nov 27\n\n\n\nMachine Learning\n\n\n\n\nSmoothing\n\n\nCase study: Death rates after natural disasters\n\n\n\n\n\n\n\nWed, Nov 29\n\n\n\nMachine Learning\n\n\n\n\nCross-Validation\n\n\ncaret package\n\n\n\n\n\n\n\nMon, Dec 4\n\n\n\nMachine Learning\n\n\n\n Example of algorithms \n\n\n\nCase study: reading handwritten digits\n\n\n\n\n\n\n\nWed, Dec 6 \n\n\n\nOther topics\n\n\n\nPossible topcis (open to student requests)\n\n\n\nShiny\n\n\nInteractive graphics: plotly\n\n\nAdvanced Quarto\n\n\nResearch topics\n\n\nLarge language models\n\n\nDeep learning\n\n\n\n\n\n\n\nFri Dec 8\n\n\n\nProblem set 3 due\n\n\n\n\n\n\n\nMon, Dec 11\n\n\n\nHelp with project\n\n\n\n \n\n\n\n\n\n\nWed, Dec 13\n\n\n\nHelp with project\n\n\n\n\n\n\n\nWed, Dec 15\n\n\n\nFinal project due"
  },
  {
    "objectID": "01-quarto.html#r-and-rstudio",
    "href": "01-quarto.html#r-and-rstudio",
    "title": "1  Quarto",
    "section": "1.1 R and RStudio",
    "text": "1.1 R and RStudio\nBefore introducing Quarto we need R installed. We highly recommend using RStudio as an IDE for this course. We will be using it in lectures.\n\n1.1.1 Installation\n\nInstall the latest version (4.3.1) of R\nInstall RStudio\n\n\n\n\nrstudio\n\n\n\n\n1.1.2 Basics\nLet’s try a few things together:\n\nOpen a new R script file\nLearn tab complete\nRun commands while editing scripts\nRun the entire script\nMake a plot\nChange options to never save workspace.\n\n\n\n1.1.3 Projects\n\nStart new project in exciting directory.\nStart new project in new directory.\nChange projects."
  },
  {
    "objectID": "01-quarto.html#markdown",
    "href": "01-quarto.html#markdown",
    "title": "1  Quarto",
    "section": "1.2 Markdown",
    "text": "1.2 Markdown\nStart a new Quarto.\n\n1.2.1 Type of editor\n\nSource - See the actual code (WYSIWYG).\nVisual - Partial preview of final document.\n\n\n\n1.2.2 The header\nAt the top you see:\n---\ntitle: \"Untitled\"\n---\nThe things between the --- is the YAML header.\nYou will see it used throughout the Quarto guide.\n\n\n1.2.3 Text formating\nitalics, bold, bold italics\nstrikethrough\ncode\n\n\n1.2.4 Headings\n# Header 1\n## Header 2\n### Header 3\nand so on\n\n\n1.2.5 Links\nJust the link: https://quarto.org/docs/guide/\nLinked text: This is the link to Quarto Guide\n\n\n1.2.6 Images\n\n\n\nFirst week of data science\n\n\nThe image can also be a local file.\n\n\n1.2.7 Lists\nBullets:\n\nbullet 1\n\nsub-bullet 1\nsub-bullet 2\n\nbullet 2\n\nOrdered list\n\nItem 1\nItem 2\n\n\n\n1.2.8 Equations\nInline: \\(Y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\)\nDisplay math:\n\\[\n\\mathbf{Y} = \\mathbf{X\\beta} + \\mathbf{\\varepsilon}\n\\]"
  },
  {
    "objectID": "01-quarto.html#computations",
    "href": "01-quarto.html#computations",
    "title": "1  Quarto",
    "section": "1.3 Computations",
    "text": "1.3 Computations\nThe main reason we use Quarto is because we can include code and execute the code when compiling the document. In R we refer to them as R chunks.\nTo add your own R chunks, you can type the characters above quickly with the key binding command-option-I on the Mac and Ctrl-Alt-I on Windows.\nThis applies to plots as well; the plot will be placed in that position. We can write something like this:\n\nx &lt;- 1\ny &lt;- 2\nx + y\n\n[1] 3\n\n\nBy default, the code will show up as well. To avoid having the code show up, you can use an argument, which are annotated with |# To avoid showing code in the final document, you can use the argument echo: FALSE. For example:\n\n\n[1] 3\n\n\nWe recommend getting into the habit of adding a label to the R code chunks. This will be very useful when debugging, among other situations. You do this by adding a descriptive word like this:\n\nx &lt;- 1\ny &lt;- 2\nx + y\n\n[1] 3\n\n\n\n1.3.1 Academic reports\nQuarto has many nice features that facilitates publishing academic reports in this guide\n\n\n1.3.2 Global execution options\nIf you want to apply an option globally, you can include in the header, under execute. For example adding the following line to the header make code not show up, by default:\nexecute:\n  echo: false\n\n\n1.3.3 More on markdown\nThere is a lot more you can do with R markdown. We highly recommend you continue learning as you gain more experience writing reports in R. There are many free resources on the internet including:\n\nRStudio’s tutorial: https://quarto.org/docs/get-started/hello/rstudio.html\nThe knitR book: https://yihui.name/knitr/\nPandoc’s Markdown in-depth documentation"
  },
  {
    "objectID": "01-quarto.html#sec-knitr",
    "href": "01-quarto.html#sec-knitr",
    "title": "1  Quarto",
    "section": "1.4 knitR",
    "text": "1.4 knitR\nWe use the knitR package to compile Quarto. The specific function used to compile is the knit function, which takes a file name as input. RStudio provides the Render button that makes it easier to compile the document.\nNote that the first time you click on the Render button, a dialog box may appear asking you to install packages you need. Once you have installed the packages, clicking Render will compile your Quarto file and the resulting document will pop up.\nThis particular example produces an html document which you can see in your working directory. To view it, open a terminal and list the files. You can open the file in a browser and use this to present your analysis. You can also produce a PDF or Microsoft document by changing:\nformat: html to format: pdf or format: docx. We can also produce documents that render on GitHub using format: gfm, which stands for GitHub flavored markdown, a convenient way to share your reports."
  },
  {
    "objectID": "01-quarto.html#exercises",
    "href": "01-quarto.html#exercises",
    "title": "1  Quarto",
    "section": "1.5 Exercises",
    "text": "1.5 Exercises\n\nWrite a Quarto document that defines variables \\(a=1, b=-1, c=-2\\) and print out the solutions to \\(f(x) = ax^2+bx+c=0\\). Do not report complex solutions, only real numbers.\nInclude a graph of \\(f(x)\\) versus \\(x\\) for \\(x \\in (-5,5)\\).\n\nThis is how you make a plot of a quadratic function:\n\na &lt;- 1 \nb &lt;- -1\nc &lt;- -2\nx &lt;- seq(-5, 5, length = 300)\nplot(x, a*x^2 + b*x + c, type = \"l\")\nabline(h = 0, lty = 2)\n\n\n\n\n\nGenerate a PDF report using knitr. Do not show the R code, only the solutions and explanations of what the reader is seeing.\nErase the PDF report and reproduce it but this time using \\(a=1, b=2, c=5\\).\nErase the PDF report and reproduce it but this time using \\(a=1, b=3, c=2\\).\nCreate an HTML page with the results for this last set of values, but this time showing the code."
  },
  {
    "objectID": "02-unix.html#naming-convention",
    "href": "02-unix.html#naming-convention",
    "title": "2  Unix",
    "section": "2.1 Naming convention",
    "text": "2.1 Naming convention\nIn general you want to name your files in a way that is related to their contents and specifies how they relate to other files. The Smithsonian Data Management Best Practices has “five precepts of file naming and organization” and they are:\n\n\n\nHave a distinctive, human-readable name that gives an indication of the content.\nFollow a consistent pattern that is machine-friendly.\nOrganize files into directories (when necessary) that follow a consistent pattern.\nAvoid repetition of semantic elements among file and directory names.\nHave a file extension that matches the file format (no changing extensions!)\n\n\n\nFor specific recommendations we highly recommend you follow The Tidyverse Style Guide1."
  },
  {
    "objectID": "02-unix.html#the-terminal",
    "href": "02-unix.html#the-terminal",
    "title": "2  Unix",
    "section": "2.2 The terminal",
    "text": "2.2 The terminal\n\necho \"Hello world\"\n\nHello world"
  },
  {
    "objectID": "02-unix.html#sec-filesystem",
    "href": "02-unix.html#sec-filesystem",
    "title": "2  Unix",
    "section": "2.3 The filesystem",
    "text": "2.3 The filesystem\n\n2.3.1 Directories and subdirectories\n\n\n\nfilesystem\n\n\n\n\n2.3.2 The home directory\n\n\n\n\n\n\nHome directory in Windows\n\n\n\n\n\n\n\nHome directory in MacOS\n\n\n\n\n\nThe structure on Windows looks something like this:\n\nAnd on MacOS something like this:"
  },
  {
    "objectID": "02-unix.html#working-directory",
    "href": "02-unix.html#working-directory",
    "title": "2  Unix",
    "section": "2.4 Working directory",
    "text": "2.4 Working directory\nThe working directory is the directly you are currently in. Later we will see that we can move to other directories using the command line. It’s similar to clicking on folders.\nYou can see your working directory like this:\n\npwd\n\n/Users/rafa/Documents/teaching/bst260/2023\n\n\nIn R we can use\n\ngetwd()\n\n[1] \"/Users/rafa/Documents/teaching/bst260/2023\""
  },
  {
    "objectID": "02-unix.html#sec-paths",
    "href": "02-unix.html#sec-paths",
    "title": "2  Unix",
    "section": "2.5 Paths",
    "text": "2.5 Paths\nThis string returned in previous command is full path to working directory.\nThe full path to your home directory is stored in an environment variable, discussed in more detail later:\n\necho $HOME\n\n/Users/rafa\n\n\nIn Unix, we use the shorthand ~ as a nickname for your home directory\nExample: the full path for docs (in image above) can be written like this ~/docs.\nMost terminals will show the path to your working directory right on the command line.\nExercise: Open a terminal window and see if the working directory is listed."
  },
  {
    "objectID": "02-unix.html#unix-commands",
    "href": "02-unix.html#unix-commands",
    "title": "2  Unix",
    "section": "2.6 Unix commands",
    "text": "2.6 Unix commands\n\n2.6.1 ls: Listing directory content\n\n\nls\n\n\n\n2.6.2 mkdir and rmdir: make and remove a directory\n\nmkdir projects\n\nIf you do this correctly, nothing will happen: no news is good news. If the directory already exists, you will get an error message and the existing directory will remain untouched.\nTo confirm that you created these directories, you can list the directories:\n\nls\n\nYou should see the directories we just created listed.\n\nmkdir docs teaching\n\nIf you made a mistake and need to remove the directory, you can use the command rmdir to remove it.\n\nmkdir junk\nrmdir junk\n\n\n\n2.6.3 cd: navigating the filesystem by changing directories\n\ncd projects\n\nTo check that the working directory changed, we can use a command we previously learned to see our location:\n\npwd"
  },
  {
    "objectID": "02-unix.html#autocomplete",
    "href": "02-unix.html#autocomplete",
    "title": "2  Unix",
    "section": "2.7 Autocomplete",
    "text": "2.7 Autocomplete\nIn Unix you can auto-complete by hitting tab. This means that we can type cd d then hit tab. Unix will either auto-complete if docs is the only directory/file starting with d or show you the options. Try it out! Using Unix without auto-complete will make it unbearable.\n\n2.7.1 cd continued\nGoing back one:\n\ncd ..\n\nGoing home:\n\ncd ~\n\nor simply:\n\ncd\n\nStating put (later we see why useful)\n\ncd .\n\nGoing far:\n\ncd /c/Users/yourusername/projects\n\nUsing relative paths:\n\ncd ../..\n\nGoing to previous working directory\n\ncd -"
  },
  {
    "objectID": "02-unix.html#practice",
    "href": "02-unix.html#practice",
    "title": "2  Unix",
    "section": "2.8 Practice",
    "text": "2.8 Practice\nLet’s explore some examples of navigating a filesystem using the command-line. Download and expand this file into a temporary directory and you will have the data struct in the following image.\n\n\n\nPractice file system\n\n\n\nSuppose our working directory is ~/projects, move to figs in project-1.\n\n\ncd project-1/figs\n\n\nNow suppose our working directory is ~/projects. Move to reports in docs in two different ways:\n\nThis is a relative path:\n\ncd ../docs/reports\n\nThe full path:\n\ncd ~/docs/reports ## assuming ~ is hometo\n\n\nSuppose we are in ~/projects/project-1/figs and want to change to ~/projects/project-2, show two different ways, one with relative path and one with full path.\n\nThis is with relative path\n\ncd ../../projects-2\n\nWith a full path\n\ncd ~/projects/proejcts-2 ## assuming home is ~"
  },
  {
    "objectID": "02-unix.html#more-unix-commands",
    "href": "02-unix.html#more-unix-commands",
    "title": "2  Unix",
    "section": "2.9 More Unix commands",
    "text": "2.9 More Unix commands\n\n2.9.1 mv: moving files\n\nmv path-to-file path-to-destination-directory\n\nFor example, if we want to move the file cv.tex from resumes to reports, you could use the full paths like this:\n\nmv ~/docs/resumes/cv.tex ~/docs/reports/\n\nYou can also use relative paths. So you could do this:\n\ncd ~/docs/resumes\nmv cv.tex ../reports/\n\nor this:\n\ncd ~/docs/reports/\nmv ../resumes/cv.tex ./\n\nWe can also use mv to change the name of a file.\n\ncd ~/docs/resumes\nmv cv.tex resume.tex\n\nWe can also combine the move and a rename. For example:\n\ncd ~/docs/resumes\nmv cv.tex ../reports/resume.tex\n\nAnd we can move entire directories. To move the resumes directory into reports, we do as follows:\n\nmv ~/docs/resumes ~/docs/reports/\n\nIt is important to add the last / to make it clear you do not want to rename the resumes directory to reports, but rather move it into the reports directory.\n\n\n2.9.2 cp: copying files\nThe command cp behaves similar to mv except instead of moving, we copy the file, meaning that the original file stays untouched.\n\n\n2.9.3 rm: removing files\nIn point-and-click systems, we remove files by dragging and dropping them into the trash or using a special click on the mouse. In Unix, we use the rm command.\n\n\n\n\n\n\nWarning\n\n\n\nUnlike throwing files into the trash, rm is permanent. Be careful!\n\n\nThe general way it works is as follows:\n\nrm filename\n\nYou can actually list files as well like this:\n\nrm filename-1 filename-2 filename-3\n\nYou can use full or relative paths. To remove directories, you will have to learn about arguments, which we do later.\n\n\n2.9.4 less: looking at a file\nOften you want to quickly look at the content of a file. If this file is a text file, the quickest way to do is by using the command less. To look a the file cv.tex, you do this:\n\ncd ~/docs/resumes\nless cv.tex \n\nTo exit the viewer, you type q. If the files are long, you can use the arrow keys to move up and down. There are many other keyboard commands you can use within less to, for example, search or jump pages."
  },
  {
    "objectID": "02-unix.html#sec-prep-project",
    "href": "02-unix.html#sec-prep-project",
    "title": "2  Unix",
    "section": "2.10 Preparing for a data science project",
    "text": "2.10 Preparing for a data science project\nWe are now ready to prepare a directory for a project. We will use the US murders project2 as an example.\nYou should start by creating a directory where you will keep all your projects. We recommend a directory called projects in your home directory. To do this you would type:\n\ncd ~\nmkdir projects\n\nOur project relates to gun violence murders so we will call the directory for our project murders. It will be a subdirectory in our projects directories. In the murders directory, we will create two subdirectories to hold the raw data and intermediate data. We will call these data and rda, respectively.\nOpen a terminal and make sure you are in the home directory:\n\ncd ~\n\nNow run the following commands to create the directory structure we want. At the end, we use ls and pwd to confirm we have generated the correct directories in the correct working directory:\n\ncd projects\nmkdir murders\ncd murders\nmkdir data rdas \nls\npwd\n\nNote that the full path of our murders dataset is ~/projects/murders.\nSo if we open a new terminal and want to navigate into that directory we type:\n\ncd projects/murders"
  },
  {
    "objectID": "02-unix.html#text-editors",
    "href": "02-unix.html#text-editors",
    "title": "2  Unix",
    "section": "2.11 Text editors",
    "text": "2.11 Text editors\nIn the course we will be using RStudio to edit files. But there will be situations in where this is not the most efficient approach. You might also need to write R code on a server that does not have RStudio installed. For this reason you need to learn to use a command-line text editors or terminal-based text editors. A key feature of these is that you can do everything you need on a terminal without the need for graphical interface. This is often necessary when using remote servers or computers you are not sitting in front off.\nCertainly! Command-line text editors are essential tools, especially for system administrators, developers, and other users who frequently work in a terminal environment. Here are some of the most popular command-line text editors:\n\nNano - Easy to use and beginner-friendly.\n\nFeatures: Simple interface, easy-to-use command prompts at the bottom of the screen, syntax highlighting.\n\nPico - Originally part of the Pine email client (Pico = PIne COmposer). It’s a simple editor and was widely used before Nano came around.\nVi or Vim - Vi is one of the oldest text editors and comes pre-installed on many UNIX systems. It is harder to use than Nano and Pico but is much more powerful. Vim is an enhanced version of Vi.\nEmacs - Another old and powerful text editor. It’s known for being extremely extensible.\n\nTo use these to edit a file you type, for example,\n\nnano filename"
  },
  {
    "objectID": "02-unix.html#advanced-unix",
    "href": "02-unix.html#advanced-unix",
    "title": "2  Unix",
    "section": "2.12 Advanced Unix",
    "text": "2.12 Advanced Unix\n\n2.12.1 Arguments\n\nrm -r directory-name\n\nall files, subdirectories, files in subdirectories, subdirectories in subdirectories, and so on, will be removed. This is equivalent to throwing a folder in the trash, except you can’t recover it. Once you remove it, it is deleted for good. Often, when you are removing directories, you will encounter files that are protected. In such cases, you can use the argument -f which stands for force.\nYou can also combine arguments. For instance, to remove a directory regardless of protected files, you type:\n\nrm -rf directory-name\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember that once you remove there is no going back, so use this command very carefully.\n\n\nA command that is often called with argument is ls. Here are some examples:\n\nls -a \n\n\nls -l \n\nIt is often useful to see files in chronological order. For that we use:\n\nls -t \n\nand to reverse the order of how files are shown you can use:\n\nls -r \n\nWe can combine all these arguments to show more information for all files in reverse chronological order:\n\nls -lart \n\nEach command has a different set of arguments. In the next section, we learn how to find out what they each do.\n\n\n2.12.2 Getting help\n\nman ls\n\nor\n\nls --help\n\n\n\n2.12.3 Pipes\n\nman ls | less\n\nor in Git Bash:\n\nls --help | less \n\nThis is also useful when listing files with many files. We can type:\n\nls -lart | less \n\n\n\n2.12.4 Wild cards\n\nls *.html\n\nTo remove all html files in a directory, we would type:\n\nrm *.html\n\nThe other useful wild card is the ? symbol.\n\nrm file-???.html\n\nThis will only remove files with that format.\nWe can combine wild cards. For example, to remove all files with the name file-001 regardless of suffix, we can type:\n\nrm file-001.* \n\n\n\n\n\n\n\nWarning\n\n\n\nCombining rm with the * wild card can be dangerous. There are combinations of these commands that will erase your entire filesystem without asking “are you sure?”. Make sure you understand how it works before using this wild card with the rm command.**\n\n\n\n\n2.12.5 Environment variables\nEarlier we saw this:\n\necho $HOME \n\nYou can see them all by typing:\n\nenv\n\nYou can change some of these environment variables. But their names vary across different shells. We describe shells in the next section.\n\n\n2.12.6 Shells\n\necho $SHELL\n\nThe most common one is bash.\nOnce you know the shell, you can change environmental variables. In Bash Shell, we do it using export variable value. To change the path, described in more detail soon, type: (Don’t actually run this command though!)\n\nexport PATH = /usr/bin/\n\n\n\n2.12.7 Executables\n\n\nwhich git\n\nThat directory is probably full of program files. The directory /usr/bin usually holds many program files. If you type:\n\nls /usr/bin\n\nin your terminal, you will see several executable files.\nThere are other directories that usually hold program files. The Application directory in the Mac or Program Files directory in Windows are examples.\nTo see where your system looks:\n\necho $PATH\n\nyou will see a list of directories separated by :. The directory /usr/bin is probably one of the first ones on the list.\nIf your command is called my-ls, you can type:\n\n./my-ls\n\nOnce you have mastered the basics of Unix, you should consider learning to write your own executables as they can help alleviate repetitive work.\n\n\n2.12.8 Permissions and file types\nIf you type:\n\nls -l\n\nAt the beginning, you will see a series of symbols like this -rw-r--r--. This string indicates the type of file: regular file -, directory d, or executable x. This string also indicates the permission of the file: is it readable? writable? executable? Can other users on the system read the file? Can other users on the system edit the file? Can other users execute if the file is executable? This is more advanced than what we cover here, but you can learn much more in a Unix reference book.\n\n\n2.12.9 Commands you should learn\n\ncurl - download data from the internet.\ntar - archive files and subdirectories of a directory into one file.\nssh - connect to another computer.\nfind - search for files by filename in your system.\ngrep - search for patterns in a file.\nawk/sed - These are two very powerful commands that permit you to find specific strings in files and change them.\nln - create a symbolic link. We do not recommend its use, but you should be familiar with it."
  },
  {
    "objectID": "02-unix.html#resources",
    "href": "02-unix.html#resources",
    "title": "2  Unix",
    "section": "2.13 Resources",
    "text": "2.13 Resources\nTo get started.\n\nhttps://www.codecademy.com/learn/learn-the-command-line\nhttps://www.edx.org/course/introduction-linux-linuxfoundationx-lfs101x-1\nhttps://www.coursera.org/learn/unix"
  },
  {
    "objectID": "02-unix.html#exercises",
    "href": "02-unix.html#exercises",
    "title": "2  Unix",
    "section": "2.14 Exercises",
    "text": "2.14 Exercises\nYou are not allowed to use RStudio or point and click for any of the exercises below. Open a text file called commands.txt using a text editor and keep a log of the commands you use in the exercises below. If you want to take notes, you can use # to distinguish notes from commands.\n\nDecide on a directory where you will save your class materials. Navigate into the directory using a full path.\nMake a directory called project-1 and cd into that directory.\nMake directors called data: data, rdas, code, and docs.\nUse curl or wget to download the file https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv and store it in rdas.\nCreate a R file in the code directory called code-1.R, write the following code in the file so that if the working directory is code it reads in the csv file you just downloaded. Use only relative paths.\n\n\nfilename &lt;- \"\"\ndat &lt;- read.csv(filename)\n\n\nAdd the following line to your R code so that it saves the file to the rdas directory. Use only relative paths.\n\n\nout &lt;- \"\"\ndat &lt;- save(dat, file = out)\n\n\nCreate a file code-2.R in the code directory. Use the following command to add a line to the file.\n\necho \"load('../rdas/murders.rda')\" &gt; code/code-2.R\nCheck to see if the line of code as added without opening a text editor.\n\nNavigate to the code directory and list all the files ending in .R.\nNavigate to the project-1 directory. Without navigating away, change the name of code-1.R to import.R, but keep the file in the same directory.\nChange the name of the project directory to murders. Describe what you have to change so the R script sill does the right thing and how this would be different if you had used full paths.\nBonus : Navigate to the murders directory. Read the man page for the find function. Use find to list all the files ending in .R."
  },
  {
    "objectID": "02-unix.html#footnotes",
    "href": "02-unix.html#footnotes",
    "title": "2  Unix",
    "section": "",
    "text": "https://style.tidyverse.org/↩︎\nhttps://github.com/rairizarry/murders↩︎"
  },
  {
    "objectID": "03-git.html#why-use-git-and-github",
    "href": "03-git.html#why-use-git-and-github",
    "title": "3  Git and GitHub",
    "section": "3.1 Why use Git and GitHub?",
    "text": "3.1 Why use Git and GitHub?\n\nSharing.\nCollaborating.\nVersion control.\n\nWe focus on the sharing aspects of Git and GitHub, but introduce some of the basics that permit you to collaborate and version control."
  },
  {
    "objectID": "03-git.html#what-is-git",
    "href": "03-git.html#what-is-git",
    "title": "3  Git and GitHub",
    "section": "3.2 What is Git?",
    "text": "3.2 What is Git?\n\n\n\nArt by: Allison Horst"
  },
  {
    "objectID": "03-git.html#what-is-github",
    "href": "03-git.html#what-is-github",
    "title": "3  Git and GitHub",
    "section": "3.3 What is GitHub?",
    "text": "3.3 What is GitHub?\nBasically, it’s a service that hosts the remote repository (repo) on the web. This facilitates collaboration and sharing greatly.\nThere many other features such as\n\na recognition system: reward, badges and stars, for example.\nhosting web pages, like the class notes for example.\nforks and pull requests,\nissue tracking\nautomation tools\n\nIt has been describes a social network for software developers.\nThe main tool behind GitHub, is Git.\nSimilar to how to how main tool behind RStudio, is R."
  },
  {
    "objectID": "03-git.html#github-accounts",
    "href": "03-git.html#github-accounts",
    "title": "3  Git and GitHub",
    "section": "3.4 GitHub accounts",
    "text": "3.4 GitHub accounts\nOnce you have a GitHub account, you are ready to connect Git and RStudio to this account.\nA first step is to let Git know who we are. This will make it easier to connect with GitHub. We start by opening a terminal window in RStudio (remember you can get one through Tools in the menu bar). Now we use the git config command to tell Git who we are. We will type the following two commands in our terminal window:\ngit config --global user.name \"Your Name\"\ngit config --global user.mail \"your@email.com\"\nConsider adding a profile README.md. Instructions are here\nLooks like this"
  },
  {
    "objectID": "03-git.html#repositories",
    "href": "03-git.html#repositories",
    "title": "3  Git and GitHub",
    "section": "3.5 Repositories",
    "text": "3.5 Repositories\nYou are now ready to create a GitHub repository (repo). This will be your remote repo.\nThe general idea is that you will have at least two copies of your code: one on your computer and one on GitHub. If you add collaborators to this repo, then each will have a copy on their computer. The GitHub copy is usually considered the main (previously called master) copy that each collaborator syncs to. Git will help you keep all the different copies synced.\nLet’s go make one on GitHub…\nThen create a directory on your computer, this will be the local repo, and connect it to the Github repository.\nFirst copy and paste the location of your git repository\nIt should look something like this:\nhttps://github.com/your-username/your-repo-name.git\ngit init\ngit remote add origin &lt;remote-url&gt;\nNow the two are connected."
  },
  {
    "objectID": "03-git.html#sec-git-overview",
    "href": "03-git.html#sec-git-overview",
    "title": "3  Git and GitHub",
    "section": "3.6 Overview of Git",
    "text": "3.6 Overview of Git\nThe main actions in Git are to:\n\npull changes from the remote repo, in this case the GitHub repo\nadd files, or as we say in the Git lingo stage files\ncommit changes to the local repo\npush changes to the remote repo, in our case the GitHub repo\n\n\n\n\nFrom Meme Git Compilation by Lulu Ilmaknun Qurotaini\n\n\n\n3.6.1 The four areas of Git\n\n\n\n3.6.2 Status\n\ngit status filename\n\n\n3.6.3 Add\nUse git add to move put file to staging area.\n\ngit add &lt;filename&gt;\ngit status &lt;filename&gt;\n\n\n3.6.4 Commit\nUse\ngit commit -m \"must add comment\"\nto move all the added files to the local repository. This file is now tracked and a copy of this version is kept going forward… this is like adding V1 to your filename.\nYou can commit files directly without using add by explicitely writing the files at the end of the commit:\ngit commit -m \"must add comment\" &lt;filename&gt;\n\n\n\n3.6.5 Push\nTo move to upstream repo we use\ngit push -u origin main\nThe -u flag sets the upstream, so in the future, you can simply use git push to push changes. So going forward we can just type:\ngit push\nHere we need to be careful as if collaborating this will affect the work of others. It might also create a conflict.\n\n\n\n3.6.6 Fetch\nTo update our local repository to the remote one we use\ngit fetch\n\n\n\n3.6.7 Merge\nOnce we are sure this is good, we can merge with our local files\ngit merge\n\n\n\n3.6.8 Pull\nIt is common to want to just skip the fetch step and just update everything. For this we use\ngit pull\n\n\n\n3.6.9 Checkout\n If you want to pull down a specific file you from the remote repo you can use:\ngit checkout filename\nBut if you have a newer version in your local repository this will create a conflict. If you are sure you want to get rid of your local copy you can remove and then checkout.\nYou can also use checkout to pull older version:\ngit checkout &lt;commit-id&gt; &lt;filename&gt;\nYou can get the commit-id either on the GitHub webpage or using\ngit log filename\n\n\n\n\n\n\nNote\n\n\n\nIf you are asked for passwords when connecting or pushing things to you want to read this and avoid this. It will be impossible to use if you have to enter a password each time you push."
  },
  {
    "objectID": "03-git.html#branches",
    "href": "03-git.html#branches",
    "title": "3  Git and GitHub",
    "section": "3.7 Branches",
    "text": "3.7 Branches\nGit can be even more complex. We can have several branches. These are useful for working in parallel or testing stuff out that might not make the main repo.\n\n\n\nArt by: Allison Horst\n\n\nWe wont go over this. But you should at least now these three commands\ngit remote -v\ngit brach"
  },
  {
    "objectID": "03-git.html#clone",
    "href": "03-git.html#clone",
    "title": "3  Git and GitHub",
    "section": "3.8 Clone",
    "text": "3.8 Clone\n\nIf you\ngit clone &lt;repo-url&gt;\npwd\nmkdir git-example\ncd git-example\ngit clone https://github.com/rairizarry/murders.git\ncd murders"
  },
  {
    "objectID": "03-git.html#using-git-in-rstudio",
    "href": "03-git.html#using-git-in-rstudio",
    "title": "3  Git and GitHub",
    "section": "3.9 Using Git in RStudio",
    "text": "3.9 Using Git in RStudio\nGo to file, new project, version control, and follow the instructions. Then notice the Git tab.\n For more memes see Meme Git Compilation by Lulu Ilmaknun"
  },
  {
    "objectID": "04-r-basics.html#packages",
    "href": "04-r-basics.html#packages",
    "title": "4  R Basics",
    "section": "4.1 Packages",
    "text": "4.1 Packages\nUse install.packages to install the dslabs package.\nTryout the following functions: sessionInfo, installed.packages"
  },
  {
    "objectID": "04-r-basics.html#prebuilt-functions",
    "href": "04-r-basics.html#prebuilt-functions",
    "title": "4  R Basics",
    "section": "4.2 Prebuilt functions",
    "text": "4.2 Prebuilt functions\nMuch of what we do in R is called prebuilt functions. Today we are using: ls, rm, library, search, factor, list, exists, str, typeof, class and maybe more.\nYou can see the code for a function by typing it without the parenthesis:\nTry this:\n\nls"
  },
  {
    "objectID": "04-r-basics.html#help-system",
    "href": "04-r-basics.html#help-system",
    "title": "4  R Basics",
    "section": "4.3 Help system",
    "text": "4.3 Help system\nIn R you can use ? or help to learn more about functions.\nYou can learn about function using\nhelp(\"ls\")\nor\n?ls"
  },
  {
    "objectID": "04-r-basics.html#the-workspace",
    "href": "04-r-basics.html#the-workspace",
    "title": "4  R Basics",
    "section": "4.4 The workspace",
    "text": "4.4 The workspace\nDefine a variable.\nUse ls to see if it’s there. Also take a look at the Environment tab in RStudio.\nUse rm to remove the variable you defined."
  },
  {
    "objectID": "04-r-basics.html#variable-name-convention",
    "href": "04-r-basics.html#variable-name-convention",
    "title": "4  R Basics",
    "section": "4.5 Variable name convention",
    "text": "4.5 Variable name convention\nA nice convention to follow is to use meaningful words that describe what is stored, use only lower case, and use underscores as a substitute for spaces.\nFor more we recommend this guide."
  },
  {
    "objectID": "04-r-basics.html#data-types",
    "href": "04-r-basics.html#data-types",
    "title": "4  R Basics",
    "section": "4.6 Data types",
    "text": "4.6 Data types\nThe main data types in R are:\n\nOne dimensional vectors: numeric, integer, logical, complex, characters.\nFactors\nLists: this includes data frames\nArrays: Matrices are the most widely used\nDate and time\ntibble\nS4 objects\n\nMany errors in R come from confusing data types. Let’s learn what these data types are and useful tools to help us.\nstr stands for structure, gives us information about an object.\ntypeof gives you the basic data type of the object. It reveals the lower-level, more fundamental type of an object in R’s memory.\nclass This function returns the class attribute of an object. The class of an object is essentially type_of at a higher, often user-facing level.\n\nlibrary(dslabs)\n\n\ntypeof(murders)\n\n[1] \"list\"\n\n\n\nclass(murders)\n\n[1] \"data.frame\"\n\n\n\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ..."
  },
  {
    "objectID": "04-r-basics.html#data-frames",
    "href": "04-r-basics.html#data-frames",
    "title": "4  R Basics",
    "section": "4.7 Data frames",
    "text": "4.7 Data frames\nDate frames are the most common class used in data analysis. It is like a spreadsheet. Rows represents observations and columns variables. Each variable can be a different data type.\nYou can add columns like this:\n\nmurders$pop_rank &lt;- rank(murders$population)\nhead(murders)\n\n       state abb region population total pop_rank\n1    Alabama  AL  South    4779736   135       29\n2     Alaska  AK   West     710231    19        5\n3    Arizona  AZ   West    6392017   232       36\n4   Arkansas  AR  South    2915918    93       20\n5 California  CA   West   37253956  1257       51\n6   Colorado  CO   West    5029196    65       30\n\n\nYou can access columns with the $\n\nmurders$population\n\n [1]  4779736   710231  6392017  2915918 37253956  5029196  3574097   897934\n [9]   601723 19687653  9920000  1360301  1567582 12830632  6483802  3046355\n[17]  2853118  4339367  4533372  1328361  5773552  6547629  9883640  5303925\n[25]  2967297  5988927   989415  1826341  2700551  1316470  8791894  2059179\n[33] 19378102  9535483   672591 11536504  3751351  3831074 12702379  1052567\n[41]  4625364   814180  6346105 25145561  2763885   625741  8001024  6724540\n[49]  1852994  5686986   563626\n\n\nbut also []\n\nmurders[1:5,]\n\n       state abb region population total pop_rank\n1    Alabama  AL  South    4779736   135       29\n2     Alaska  AK   West     710231    19        5\n3    Arizona  AZ   West    6392017   232       36\n4   Arkansas  AR  South    2915918    93       20\n5 California  CA   West   37253956  1257       51\n\n\n\nmurders[1:5, 1:2]\n\n       state abb\n1    Alabama  AL\n2     Alaska  AK\n3    Arizona  AZ\n4   Arkansas  AR\n5 California  CA\n\n\n\nmurders[1:5, c(\"state\", \"abb\")]\n\n       state abb\n1    Alabama  AL\n2     Alaska  AK\n3    Arizona  AZ\n4   Arkansas  AR\n5 California  CA"
  },
  {
    "objectID": "04-r-basics.html#with",
    "href": "04-r-basics.html#with",
    "title": "4  R Basics",
    "section": "4.8 with",
    "text": "4.8 with\nThe function with let’s us use the column names as objects:\n\nwith(murders, length(state))\n\n[1] 51"
  },
  {
    "objectID": "04-r-basics.html#vectors",
    "href": "04-r-basics.html#vectors",
    "title": "4  R Basics",
    "section": "4.9 Vectors",
    "text": "4.9 Vectors\nThe columns of data frames are one dimensional (atomic) vectors.\nHere is an example:\n\nlength(murders$population)\n\n[1] 51\n\n\nHow to create vectors:\n\nx &lt;- c(\"b\", \"s\", \"t\", \" \", \"2\", \"6\", \"0\")\n\nSequences are particularly useful:\n\nseq(1, 10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq(1, 9, 2)\n\n[1] 1 3 5 7 9\n\n\n\n1:10\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nseq_along(x)\n\n[1] 1 2 3 4 5 6 7"
  },
  {
    "objectID": "04-r-basics.html#factors",
    "href": "04-r-basics.html#factors",
    "title": "4  R Basics",
    "section": "4.10 Factors",
    "text": "4.10 Factors\nOne key data type distinction is factors versus characters:\n\ntypeof(murders$state)\n\n[1] \"character\"\n\ntypeof(murders$region)\n\n[1] \"integer\"\n\n\nFactors store levels and then the label of each level. They are very useful for categorical data.\n\nx &lt;- murders$region\nlevels(x)\n\n[1] \"Northeast\"     \"South\"         \"North Central\" \"West\"         \n\n\n\n4.10.1 Categories based on strata\nThe function cut is useful for converting numbers into categories\n\nwith(murders, cut(population, \n                  c(0, 10^6, 10^7, Inf)))\n\n [1] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]  \n [6] (1e+06,1e+07] (1e+06,1e+07] (0,1e+06]     (0,1e+06]     (1e+07,Inf]  \n[11] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[16] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[21] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[26] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[31] (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07] (0,1e+06]    \n[36] (1e+07,Inf]   (1e+06,1e+07] (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[41] (1e+06,1e+07] (0,1e+06]     (1e+06,1e+07] (1e+07,Inf]   (1e+06,1e+07]\n[46] (0,1e+06]     (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07] (1e+06,1e+07]\n[51] (0,1e+06]    \nLevels: (0,1e+06] (1e+06,1e+07] (1e+07,Inf]\n\n\n\nmurders$size &lt;- cut(murders$population, c(0, 10^6, 10^7, Inf), \n            labels = c(\"small\", \"medium\", \"large\"))\nmurders[1:6,c(\"state\", \"size\")]\n\n       state   size\n1    Alabama medium\n2     Alaska  small\n3    Arizona medium\n4   Arkansas medium\n5 California  large\n6   Colorado medium\n\n\n\n\n4.10.2 changing levels\nYou can change the levels (this will come in handy when we learn linear models)\nOrder levels alphabetically:\n\nfactor(x, levels = sort(levels(murders$region)))\n\n [1] South         West          West          South         West         \n [6] West          Northeast     South         South         South        \n[11] South         West          West          North Central North Central\n[16] North Central North Central South         South         Northeast    \n[21] South         Northeast     North Central North Central South        \n[26] North Central West          North Central West          Northeast    \n[31] Northeast     West          Northeast     South         North Central\n[36] North Central South         West          Northeast     Northeast    \n[41] South         North Central South         South         West         \n[46] Northeast     South         West          South         North Central\n[51] West         \nLevels: North Central Northeast South West\n\n\nMake west the first level:\n\nx &lt;- relevel(x, ref = \"West\")\n\nOrder levels by population size:\n\nx &lt;- reorder(murders$region, murders$population, sum)\n\nFactors are more efficient:\n\nx &lt;- sample(murders$state[c(5,33,44)], 10^7, replace = TRUE)\ny &lt;- factor(x)\nobject.size(x)\n\n80000232 bytes\n\nobject.size(y)\n\n40000648 bytes\n\n\n\nsystem.time({x &lt;- tolower(x)})\n\n   user  system elapsed \n  1.460   0.012   1.473 \n\n\nExercise: How can we make this go much faster?\n\nsystem.time({levels(y) &lt;- tolower(levels(y))})\n\n   user  system elapsed \n  0.019   0.003   0.022 \n\n\nFactors can be confusing:\n\nx &lt;- factor(c(\"3\",\"2\",\"1\"), levels = c(\"3\",\"2\",\"1\"))\nas.numeric(x)\n\n[1] 1 2 3\n\n\n\nx[1]\n\n[1] 3\nLevels: 3 2 1\n\nlevels(x[1])\n\n[1] \"3\" \"2\" \"1\"\n\ntable(x[1])\n\n\n3 2 1 \n1 0 0 \n\nz &lt;- x[1]\nz &lt;- droplevels(z)\n\n\nx[1] &lt;- \"4\"\n\nWarning in `[&lt;-.factor`(`*tmp*`, 1, value = \"4\"): invalid factor level, NA\ngenerated\n\nx\n\n[1] &lt;NA&gt; 2    1   \nLevels: 3 2 1"
  },
  {
    "objectID": "04-r-basics.html#nas",
    "href": "04-r-basics.html#nas",
    "title": "4  R Basics",
    "section": "4.11 NAs",
    "text": "4.11 NAs\nNA stands for not available. We will see many NAs if we analyze data generally.\n\nx &lt;- as.numeric(\"a\")\n\nWarning: NAs introduced by coercion\n\n\n\nis.na(x)\n\n[1] TRUE\n\n\n\nis.na(\"a\")\n\n[1] FALSE\n\n\n\n1 + 2 + NA\n\n[1] NA\n\n\nWhen used with logicals behaves like FALSE\n\nTRUE & NA\n\n[1] NA\n\nTRUE | NA\n\n[1] TRUE\n\n\nBut is is not FALSE. Try this:\n\nif (NA) print(1) else print(0)\n\nA related constant is NaN which stands for not a number. It is a numeric that is not a number.\n\nclass(0/0)\n\n[1] \"numeric\"\n\nsqrt(-1)\n\nWarning in sqrt(-1): NaNs produced\n\n\n[1] NaN\n\nlog(-1)\n\nWarning in log(-1): NaNs produced\n\n\n[1] NaN\n\n0/0\n\n[1] NaN"
  },
  {
    "objectID": "04-r-basics.html#coercing",
    "href": "04-r-basics.html#coercing",
    "title": "4  R Basics",
    "section": "4.12 coercing",
    "text": "4.12 coercing\nWhen you do something nonsensical with data types, R tries to figure out what you mean. This can cause confusion and unnoticed errors. So it’s important to understand how and when it happens. Here are some examples:\n\ntypeof(1L)\n\n[1] \"integer\"\n\ntypeof(1)\n\n[1] \"double\"\n\ntypeof(1 + 1L)\n\n[1] \"double\"\n\n\n\nc(\"a\", 1, 2)\n\n[1] \"a\" \"1\" \"2\"\n\n\n\nTRUE + FALSE\n\n[1] 1\n\n\n\nfactor(\"a\") == \"a\"\n\n[1] TRUE\n\nidentical(factor(\"a\"), \"a\")\n\n[1] FALSE\n\n\nYou want to avoid automatic coercion and instead explicitly do it. Most coercion functions start with as.\n\nx &lt;- factor(c(\"a\",\"b\",\"b\",\"c\"))\nas.character(x)\n\n[1] \"a\" \"b\" \"b\" \"c\"\n\nas.numeric(x)\n\n[1] 1 2 2 3\n\n\n\nx &lt;- c(\"12323\", \"12,323\")\nas.numeric(x)\n\nWarning: NAs introduced by coercion\n\n\n[1] 12323    NA\n\nreadr::parse_guess(x)\n\n[1] 12323 12323"
  },
  {
    "objectID": "04-r-basics.html#lists",
    "href": "04-r-basics.html#lists",
    "title": "4  R Basics",
    "section": "4.13 lists",
    "text": "4.13 lists\nData frames are a type of list. List permit components of different types and, unlike data frames, length\n\nx &lt;- list(name = \"John\", id = 112, grades = c(95, 87, 92))\n\nYou can access components in different ways:\n\nx$name\n\n[1] \"John\"\n\nx[[1]]\n\n[1] \"John\"\n\nx[[\"name\"]]\n\n[1] \"John\""
  },
  {
    "objectID": "04-r-basics.html#matrics",
    "href": "04-r-basics.html#matrics",
    "title": "4  R Basics",
    "section": "4.14 matrics",
    "text": "4.14 matrics\nMatrices are another widely used data type. They are similar to data frames except all entries need to be of the same type.\nWe will learn more about matrices in the High Dimensional data Analysis part of the class."
  },
  {
    "objectID": "04-r-basics.html#functions",
    "href": "04-r-basics.html#functions",
    "title": "4  R Basics",
    "section": "4.15 functions",
    "text": "4.15 functions\nYou can define your own function. The form is like this:\n\nf &lt;- function(x, y, z = 0){\n  ### do calculations with x, y, z to compute object\n  ## return(object)\n}\n\nHere is an example of a function that sums \\(1,2,\\dots,n\\)\n\ns &lt;- function(n){\n   return(sum(1:n))\n}"
  },
  {
    "objectID": "04-r-basics.html#lexical-scope",
    "href": "04-r-basics.html#lexical-scope",
    "title": "4  R Basics",
    "section": "4.16 Lexical scope",
    "text": "4.16 Lexical scope\n\nf &lt;- function(x){\n  cat(\"y is\", y,\"\\n\")\n  y &lt;- x\n  cat(\"y is\", y,\"\\n\")\n  return(y)\n}\ny &lt;- 2\nf(3)\n\ny is 2 \ny is 3 \n\n\n[1] 3\n\ny &lt;- f(3)\n\ny is 2 \ny is 3 \n\ny\n\n[1] 3"
  },
  {
    "objectID": "04-r-basics.html#namespaces",
    "href": "04-r-basics.html#namespaces",
    "title": "4  R Basics",
    "section": "4.17 Namespaces",
    "text": "4.17 Namespaces\nLook at this function.\n\nfilter\nlibrary(dplyr)\nfilter\n\nNote this is just the Global Environment.\nUse search to see other environments.\nNote all the functions in stats\nYou can explicitly say which you want:\n\nstats::filter\ndplyr::filter\n\nTry to understand this example:\n\nexists(\"murders\")\n\n[1] TRUE\n\nlibrary(dslabs)\nexists(\"murders\")\n\n[1] TRUE\n\nmurders &lt;- murders\nmurders2 &lt;- murders\nrm(murders)\nexists(\"murders\")\n\n[1] TRUE\n\ndetach(\"package:dslabs\")\nexists(\"murders\")\n\n[1] FALSE\n\nexists(\"murders2\")\n\n[1] TRUE"
  },
  {
    "objectID": "04-r-basics.html#object-oriented-programming",
    "href": "04-r-basics.html#object-oriented-programming",
    "title": "4  R Basics",
    "section": "4.18 object oriented programming",
    "text": "4.18 object oriented programming\nR uses object oriented programming. It uses to approaches referred to as S3 and S4. The original S3 is more common.\nWhat does this mean?\n\nclass(co2)\n\n[1] \"ts\"\n\nplot(co2)\n\n\n\n\n\nplot(as.numeric(co2))\n\n\n\n\nSee the difference? The first one actually calls the function\n\nplot.ts\n\nNotice all the plot functions that start with plot.\nThe function plot will call different functions depending on the class of the arguments:\n\nplot\n\nfunction (x, y, ...) \nUseMethod(\"plot\")\n&lt;bytecode: 0x1187f9d50&gt;\n&lt;environment: namespace:base&gt;"
  },
  {
    "objectID": "04-r-basics.html#exercises",
    "href": "04-r-basics.html#exercises",
    "title": "4  R Basics",
    "section": "4.19 Exercises",
    "text": "4.19 Exercises\n\nWhat is the sum of the first 100 positive integers? The formula for the sum of integers \\(1\\) through \\(n\\) is \\(n(n+1)/2\\). Define \\(n=100\\) and then use R to compute the sum of \\(1\\) through \\(100\\) using the formula. What is the sum?\n\n\nn &lt;- 100\nn*(n + 1) / 2\n\n[1] 5050\n\n\n\nNow use the same formula to compute the sum of the integers from 1 through 1,000.\n\n\nn &lt;- 1000\nn*(n + 1) / 2\n\n[1] 500500\n\n\n\nNow use the functions seq and sum to compute the sum with R for any n, rather than a formula.\n\n\nn &lt;- 100\nx &lt;- seq(1, 100)\nsum(x)\n\n[1] 5050\n\n\n\nIn math and programming, we say that we evaluate a function when we replace the argument with a given number. So if we type sqrt(4), we evaluate the sqrt function. In R, you can evaluate a function inside another function. The evaluations happen from the inside out. Use one line of code to compute the log, in base 10, of the square root of 100.\n\n\nlog(sqrt(100), base = 10)\n\n[1] 1\n\nlog10(sqrt(100))\n\n[1] 1\n\n\n\nMake sure the US murders dataset is loaded. Use the function str to examine the structure of the murders object. What are the column names used by the data frame for these five variables?\n\n\nlibrary(dslabs)\nstr(murders)\n\n'data.frame':   51 obs. of  5 variables:\n $ state     : chr  \"Alabama\" \"Alaska\" \"Arizona\" \"Arkansas\" ...\n $ abb       : chr  \"AL\" \"AK\" \"AZ\" \"AR\" ...\n $ region    : Factor w/ 4 levels \"Northeast\",\"South\",..: 2 4 4 2 4 4 1 2 2 2 ...\n $ population: num  4779736 710231 6392017 2915918 37253956 ...\n $ total     : num  135 19 232 93 1257 ..."
  },
  {
    "objectID": "05-vectorization.html#arithmetics",
    "href": "05-vectorization.html#arithmetics",
    "title": "5  Vectorization",
    "section": "5.1 Arithmetics",
    "text": "5.1 Arithmetics\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\n\nConvert to meters:\n\nheights * 2.54 / 100\n\n [1] 1.7526 1.5748 1.6764 1.7780 1.7780 1.8542 1.7018 1.8542 1.7018 1.7780\n\n\nDifference from the average:\n\navg &lt;- mean(heights)\nheights - avg \n\n [1]  0.3 -6.7 -2.7  1.3  1.3  4.3 -1.7  4.3 -1.7  1.3\n\n\nExercise: compute the height in standardized units\n\ns &lt;- sd(heights)\n(heights - avg) / s\n\n [1]  0.08995503 -2.00899575 -0.80959530  0.38980515  0.38980515  1.28935548\n [7] -0.50974519  1.28935548 -0.50974519  0.38980515\n\n# can also use scale(heights)\n\nIf it’s two vectors, it does it component wise:\n\nheights &lt;- c(69, 62, 66, 70, 70, 73, 67, 73, 67, 70)\nerror &lt;- rnorm(length(heights), 0, 0.1)\nheights + error\n\n [1] 69.09288 61.96118 65.89917 70.02886 69.98994 73.00737 66.93650 73.08543\n [9] 66.95941 70.09198\n\n\nExercise:\nAdd a column to the murders dataset with the murder rate in per 100,000.\n\nlibrary(dslabs)\nmurders$rate &lt;- with(murders, total / population * 10^5)"
  },
  {
    "objectID": "05-vectorization.html#functions-that-vectorize",
    "href": "05-vectorization.html#functions-that-vectorize",
    "title": "5  Vectorization",
    "section": "5.2 Functions that vectorize",
    "text": "5.2 Functions that vectorize\nMost arithmetic functions work on vectors\n\nx &lt;- 1:10\nsqrt(x)\n\n [1] 1.000000 1.414214 1.732051 2.000000 2.236068 2.449490 2.645751 2.828427\n [9] 3.000000 3.162278\n\nlog(x)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\n2^x\n\n [1]    2    4    8   16   32   64  128  256  512 1024\n\n\nNote that the conditional function if-else does not vectorize. A particularly useful function is a vectorized version ifelse. Here is an example:\n\na &lt;- c(0, 1, 2, -4, 5)\nifelse(a &gt; 0, 1/a, NA)\n\n[1]  NA 1.0 0.5  NA 0.2\n\n\nOther conditional functions, such as any and all, do vectorize."
  },
  {
    "objectID": "05-vectorization.html#indexing",
    "href": "05-vectorization.html#indexing",
    "title": "5  Vectorization",
    "section": "5.3 Indexing",
    "text": "5.3 Indexing\nVectorization also works for logical relationships:\n\nind &lt;- murders$population &lt; 10^6\n\nYou can subset a vector using these:\n\nmurders$state[ind]\n\n[1] \"Alaska\"               \"Delaware\"             \"District of Columbia\"\n[4] \"Montana\"              \"North Dakota\"         \"South Dakota\"        \n[7] \"Vermont\"              \"Wyoming\"             \n\n\nYou can also use vectorization to apply logical operators:\n\nind &lt;- murders$population &lt; 10^6 & murders$region == \"West\"\nmurders$state[ind]\n\n[1] \"Alaska\"  \"Montana\" \"Wyoming\""
  },
  {
    "objectID": "05-vectorization.html#split",
    "href": "05-vectorization.html#split",
    "title": "5  Vectorization",
    "section": "5.4 split",
    "text": "5.4 split\nSplit is a useful function to get indexes using a factor.\n\ninds &lt;- with(murders, split(seq_along(region), region))\nmurders$state[inds$West]\n\n [1] \"Alaska\"     \"Arizona\"    \"California\" \"Colorado\"   \"Hawaii\"    \n [6] \"Idaho\"      \"Montana\"    \"Nevada\"     \"New Mexico\" \"Oregon\"    \n[11] \"Utah\"       \"Washington\" \"Wyoming\""
  },
  {
    "objectID": "05-vectorization.html#functions-for-subsetting",
    "href": "05-vectorization.html#functions-for-subsetting",
    "title": "5  Vectorization",
    "section": "5.5 Functions for subsetting",
    "text": "5.5 Functions for subsetting\nThe functions which, match and the operator %in% are useful for sub-setting\nHere are some examples:\n\nind &lt;- which(murders$state == \"California\")\nind\n\n[1] 5\n\nmurders[ind,]\n\n       state abb region population total     rate\n5 California  CA   West   37253956  1257 3.374138\n\n\n\nind &lt;- match(c(\"New York\", \"Florida\", \"Texas\"), murders$state)\nind\n\n[1] 33 10 44\n\n\n\nc(\"Boston\", \"Dakota\", \"Washington\") %in% murders$state\n\n[1] FALSE FALSE  TRUE"
  },
  {
    "objectID": "05-vectorization.html#sapply",
    "href": "05-vectorization.html#sapply",
    "title": "5  Vectorization",
    "section": "5.6 sapply",
    "text": "5.6 sapply\nYou can apply functions that don’t vectorize. Like this one:\n\ns &lt;- function(n){\n   return(sum(1:n))\n}\n\nTry it on a vector:\n\nns &lt;- c(25, 100, 1000)\ns(ns)\n\nWarning in 1:n: numerical expression has 3 elements: only the first used\n\n\n[1] 325\n\n\nWe can use sapply\n\nsapply(ns, s)\n\n[1]    325   5050 500500\n\n\nsapply will work on any vector, including lists."
  },
  {
    "objectID": "05-vectorization.html#exercises",
    "href": "05-vectorization.html#exercises",
    "title": "5  Vectorization",
    "section": "5.7 Exercises",
    "text": "5.7 Exercises\nNow we are ready to help your friend. Let’s give them options of places with low murders rates, mountains, and not too small.\nFor the following exercises do no load any packages other than dslabs.\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths. Show all variables.\n\n\nif (exists(\"murders\")) rm(murders)\nlibrary(dslabs)\n\nmurders$rate &lt;- with(murders, total/population*10^5)\nmurders[murders$rate &lt; 1,]\n\n           state abb        region population total      rate\n12        Hawaii  HI          West    1360301     7 0.5145920\n13         Idaho  ID          West    1567582    12 0.7655102\n16          Iowa  IA North Central    3046355    21 0.6893484\n20         Maine  ME     Northeast    1328361    11 0.8280881\n24     Minnesota  MN North Central    5303925    53 0.9992600\n30 New Hampshire  NH     Northeast    1316470     5 0.3798036\n35  North Dakota  ND North Central     672591     4 0.5947151\n38        Oregon  OR          West    3831074    36 0.9396843\n42  South Dakota  SD North Central     814180     8 0.9825837\n45          Utah  UT          West    2763885    22 0.7959810\n46       Vermont  VT     Northeast     625741     2 0.3196211\n51       Wyoming  WY          West     563626     5 0.8871131\n\n\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths and in the West of the US. Don’t show the region variable.\n\n\nmurders[murders$rate &lt; 1 & murders$region == \"West\",]\n\n     state abb region population total      rate\n12  Hawaii  HI   West    1360301     7 0.5145920\n13   Idaho  ID   West    1567582    12 0.7655102\n38  Oregon  OR   West    3831074    36 0.9396843\n45    Utah  UT   West    2763885    22 0.7959810\n51 Wyoming  WY   West     563626     5 0.8871131\n\n\n\nShow the largest state with a rate less than 1 per 100,000.\n\n\ndat &lt;- murders[murders$rate &lt; 1,]\ndat[which.max(dat$population),]\n\n       state abb        region population total    rate\n24 Minnesota  MN North Central    5303925    53 0.99926\n\n\n\nShow the state with a population of more than 10 million with the lowest rate.\n\n\ndat &lt;- murders[murders$population &gt;= 10^7,]\ndat[which.min(dat$rate),]\n\n      state abb    region population total    rate\n33 New York  NY Northeast   19378102   517 2.66796\n\n\n\nCompute the rate for each region of the US.\n\n\nindexes &lt;- split(1:nrow(murders), murders$region)\nsapply(indexes, function(ind) {\n  sum(murders$total[ind])/sum(murders$population[ind])*10^5\n})\n\n    Northeast         South North Central          West \n     2.655592      3.626558      2.731334      2.656175 \n\n\nMore practice exercises:\n\nCreate a vector of numbers that starts at 6, does not pass 55, and adds numbers in increments of 4/7: 6, 6 + 4/7, 6 + 8/7, and so on. How many numbers does the list have? Hint: use seq and length.\nMake this data frame:\n\n\ntemp &lt;- c(35, 88, 42, 84, 81, 30)\ncity &lt;- c(\"Beijing\", \"Lagos\", \"Paris\", \"Rio de Janeiro\", \n          \"San Juan\", \"Toronto\")\ncity_temps &lt;- data.frame(name = city, temperature = temp)\n\nConvert the temperatures to Celsius.\n\nCompute the following sum\n\n\\[\nS_n = 1+1/2^2 + 1/3^2 + \\dots 1/n^2\n\\]\nShow that as \\(n\\) gets bigger we get closer \\(\\pi^2/6\\).\n\nUse the %in% operator and the predefined object state.abb to create a logical vector that answers the question: which of the following are actual abbreviations: MA, ME, MI, MO, MU?\nExtend the code you used in the previous exercise to report the one entry that is not an actual abbreviation. Hint: use the ! operator, which turns FALSE into TRUE and viceversa, then which to obtain an index.\nShow all variables for New York, California, and Texas, in that order."
  },
  {
    "objectID": "06-tidyverse.html#tidy-data",
    "href": "06-tidyverse.html#tidy-data",
    "title": "6  Tidyverse",
    "section": "6.1 Tidy data",
    "text": "6.1 Tidy data\nThis is tidy:\n\n\n      country year fertility\n1     Germany 1960      2.41\n2 South Korea 1960      6.16\n3     Germany 1961      2.44\n4 South Korea 1961      5.99\n5     Germany 1962      2.47\n6 South Korea 1962      5.79\n\n\nOriginally, the data was in the following format:\n\n\n      country 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970\n1     Germany 2.41 2.44 2.47 2.49 2.49 2.48 2.44 2.37 2.28 2.17 2.04\n2 South Korea 6.16 5.99 5.79 5.57 5.36 5.16 4.99 4.85 4.73 4.62 4.53\n\n\nNot tidy.\nPart of what we learn in the data wrangling part of the class is to make data tidy."
  },
  {
    "objectID": "06-tidyverse.html#adding-a-column-with-mutate",
    "href": "06-tidyverse.html#adding-a-column-with-mutate",
    "title": "6  Tidyverse",
    "section": "6.2 Adding a column with mutate",
    "text": "6.2 Adding a column with mutate\n\nmurders &lt;- mutate(murders, rate = total/population*100000)\n\nNotice that here we used total and population inside the function, which are objects that are not defined in our workspace. But why don’t we get an error? This is non-standard evaluation where the context is used to know what variable names means."
  },
  {
    "objectID": "06-tidyverse.html#subsetting-with-filter",
    "href": "06-tidyverse.html#subsetting-with-filter",
    "title": "6  Tidyverse",
    "section": "6.3 Subsetting with filter",
    "text": "6.3 Subsetting with filter\n\nfilter(murders, rate &lt;= 0.71)\n\n          state abb        region population total      rate\n1        Hawaii  HI          West    1360301     7 0.5145920\n2          Iowa  IA North Central    3046355    21 0.6893484\n3 New Hampshire  NH     Northeast    1316470     5 0.3798036\n4  North Dakota  ND North Central     672591     4 0.5947151\n5       Vermont  VT     Northeast     625741     2 0.3196211"
  },
  {
    "objectID": "06-tidyverse.html#selecting-columns-with-select",
    "href": "06-tidyverse.html#selecting-columns-with-select",
    "title": "6  Tidyverse",
    "section": "6.4 Selecting columns with select",
    "text": "6.4 Selecting columns with select\n\nnew_table &lt;- select(murders, state, region, rate)\nfilter(new_table, rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211"
  },
  {
    "objectID": "06-tidyverse.html#the-pipe-or",
    "href": "06-tidyverse.html#the-pipe-or",
    "title": "6  Tidyverse",
    "section": "6.5 The pipe: |> or %>%",
    "text": "6.5 The pipe: |&gt; or %&gt;%\nWe use the pipe to chain a series of operations… for example if we want to select columns and then filter rows we chain like this:\n\\[ \\mbox{original data }\n\\rightarrow \\mbox{ select }\n\\rightarrow \\mbox{ filter } \\]\nThe code looks like this:\n\nmurders |&gt; select(state, region, rate) |&gt; filter(rate &lt;= 0.71)\n\n          state        region      rate\n1        Hawaii          West 0.5145920\n2          Iowa North Central 0.6893484\n3 New Hampshire     Northeast 0.3798036\n4  North Dakota North Central 0.5947151\n5       Vermont     Northeast 0.3196211\n\n\nThe object on the left of the pipe is used as the first argument for the function on the right.\nThe second argument becomes the first, the third the second, and so on…\n\n16 |&gt; sqrt() |&gt; log(base = 2)\n\n[1] 2"
  },
  {
    "objectID": "06-tidyverse.html#summarizing-data",
    "href": "06-tidyverse.html#summarizing-data",
    "title": "6  Tidyverse",
    "section": "6.6 Summarizing data",
    "text": "6.6 Summarizing data\nHere is how it works:\n\nmurders |&gt; summarize(avg = mean(rate))\n\n       avg\n1 2.779125\n\n\nLet’s compute murder rate for the US. Is the above it?\nNo the rate is NOT the average of rates.\n\nmurders |&gt; summarize(rate = sum(total)/sum(population)*100000)\n\n      rate\n1 3.034555\n\n\n\n6.6.1 Multiple summaries\nWe want the median, minimum and max population size:\n\nmurders |&gt; summarize(median = median(population), min = min(population), max = max(population))\n\n   median    min      max\n1 4339367 563626 37253956\n\n\nWhy don’t we use quantiles?\n\nmurders |&gt; summarize(quantiles = quantile(population, c(0.5, 0, 1)))\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\n\n  quantiles\n1   4339367\n2    563626\n3  37253956\n\n\nFor multiple summaries we use reframe\n\nmurders |&gt; reframe(quantiles = quantile(population, c(0.5, 0, 1)))\n\n  quantiles\n1   4339367\n2    563626\n3  37253956\n\n\nHowever, if we want a column per summary, as the summarize call above, we have to define a function that returns a data frame like this:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], min = qs[2], max = qs[3])\n}\n\nThen we can call summarize as above:\n\nmurders |&gt; summarize(median_min_max(population))\n\n   median    min      max\n1 4339367 563626 37253956\n\n\n\n\n6.6.2 Group then summarize with group_by\nLet’s compute murder rate by region.\nTake a close look at this output? How is it different than the original?\n\nmurders |&gt; group_by(region)\n\n# A tibble: 51 × 6\n# Groups:   region [4]\n   state                abb   region    population total  rate\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Alabama              AL    South        4779736   135  2.82\n 2 Alaska               AK    West          710231    19  2.68\n 3 Arizona              AZ    West         6392017   232  3.63\n 4 Arkansas             AR    South        2915918    93  3.19\n 5 California           CA    West        37253956  1257  3.37\n 6 Colorado             CO    West         5029196    65  1.29\n 7 Connecticut          CT    Northeast    3574097    97  2.71\n 8 Delaware             DE    South         897934    38  4.23\n 9 District of Columbia DC    South         601723    99 16.5 \n10 Florida              FL    South       19687653   669  3.40\n# ℹ 41 more rows\n\n\nNote the Groups: region [4] when we print the object. Although not immediately obvious from its appearance, this is now a special data frame called a grouped data frame, and dplyr functions, in particular summarize, will behave differently when acting on this object.\n\nmurders |&gt; group_by(region) |&gt; summarize(rate = sum(total) / sum(population) * 100000)\n\n# A tibble: 4 × 2\n  region         rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Northeast      2.66\n2 South          3.63\n3 North Central  2.73\n4 West           2.66\n\n\nThe summarize function applies the summarization to each group separately.\nFor another example, let’s compute the median, minimum, and maximum population in the four regions of the country using the median_min_max defined above:\n\nmurders |&gt; group_by(region) |&gt; summarize(median_min_max(population))\n\n# A tibble: 4 × 4\n  region          median    min      max\n  &lt;fct&gt;            &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 Northeast     3574097  625741 19378102\n2 South         4625364  601723 25145561\n3 North Central 5495456. 672591 12830632\n4 West          2700551  563626 37253956"
  },
  {
    "objectID": "06-tidyverse.html#ungroup",
    "href": "06-tidyverse.html#ungroup",
    "title": "6  Tidyverse",
    "section": "6.7 ungroup",
    "text": "6.7 ungroup\nYou can also summarize a variable but not collapse the dataset. We use mutate instead of summarize. Here is an example where we add a column with the population in each region and the number of states in the region, shown for each state. When we do this, we usually want to ungroup before continuing our analysis.\n\nmurders |&gt; group_by(region) |&gt; \n  mutate(region_pop = sum(population), n = n()) |&gt;\n  ungroup()\n\n# A tibble: 51 × 8\n   state                abb   region    population total  rate region_pop     n\n   &lt;chr&gt;                &lt;chr&gt; &lt;fct&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n 1 Alabama              AL    South        4779736   135  2.82  115674434    17\n 2 Alaska               AK    West          710231    19  2.68   71945553    13\n 3 Arizona              AZ    West         6392017   232  3.63   71945553    13\n 4 Arkansas             AR    South        2915918    93  3.19  115674434    17\n 5 California           CA    West        37253956  1257  3.37   71945553    13\n 6 Colorado             CO    West         5029196    65  1.29   71945553    13\n 7 Connecticut          CT    Northeast    3574097    97  2.71   55317240     9\n 8 Delaware             DE    South         897934    38  4.23  115674434    17\n 9 District of Columbia DC    South         601723    99 16.5   115674434    17\n10 Florida              FL    South       19687653   669  3.40  115674434    17\n# ℹ 41 more rows\n\n\n\n6.7.1 pull\nTidyverse function always returns a data frame. Even if its just one number.\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  class()\n\n[1] \"data.frame\"\n\n\nTo get a number use pull\n\nmurders |&gt; \n  summarize(rate = sum(total)/sum(population)*100000) |&gt;\n  pull(rate) \n\n[1] 3.034555"
  },
  {
    "objectID": "06-tidyverse.html#sorting-data-frames",
    "href": "06-tidyverse.html#sorting-data-frames",
    "title": "6  Tidyverse",
    "section": "6.8 Sorting data frames",
    "text": "6.8 Sorting data frames\nStates order by rate\n\nmurders |&gt; arrange(rate) |&gt; head()\n\n          state abb        region population total      rate\n1       Vermont  VT     Northeast     625741     2 0.3196211\n2 New Hampshire  NH     Northeast    1316470     5 0.3798036\n3        Hawaii  HI          West    1360301     7 0.5145920\n4  North Dakota  ND North Central     672591     4 0.5947151\n5          Iowa  IA North Central    3046355    21 0.6893484\n6         Idaho  ID          West    1567582    12 0.7655102\n\n\nIf we want decreasing we can either use the negative or, for more readability, use desc:\n\nmurders |&gt; arrange(desc(rate)) |&gt; head()\n\n                 state abb        region population total      rate\n1 District of Columbia  DC         South     601723    99 16.452753\n2            Louisiana  LA         South    4533372   351  7.742581\n3             Missouri  MO North Central    5988927   321  5.359892\n4             Maryland  MD         South    5773552   293  5.074866\n5       South Carolina  SC         South    4625364   207  4.475323\n6             Delaware  DE         South     897934    38  4.231937\n\n\nWe can use two variables as well:\n\nmurders |&gt; arrange(region, desc(rate)) |&gt; head(11)\n\n                  state abb    region population total       rate\n1          Pennsylvania  PA Northeast   12702379   457  3.5977513\n2            New Jersey  NJ Northeast    8791894   246  2.7980319\n3           Connecticut  CT Northeast    3574097    97  2.7139722\n4              New York  NY Northeast   19378102   517  2.6679599\n5         Massachusetts  MA Northeast    6547629   118  1.8021791\n6          Rhode Island  RI Northeast    1052567    16  1.5200933\n7                 Maine  ME Northeast    1328361    11  0.8280881\n8         New Hampshire  NH Northeast    1316470     5  0.3798036\n9               Vermont  VT Northeast     625741     2  0.3196211\n10 District of Columbia  DC     South     601723    99 16.4527532\n11            Louisiana  LA     South    4533372   351  7.7425810"
  },
  {
    "objectID": "06-tidyverse.html#exercises",
    "href": "06-tidyverse.html#exercises",
    "title": "6  Tidyverse",
    "section": "6.9 Exercises",
    "text": "6.9 Exercises"
  },
  {
    "objectID": "06-tidyverse.html#exercises-1",
    "href": "06-tidyverse.html#exercises-1",
    "title": "6  Tidyverse",
    "section": "6.10 Exercises",
    "text": "6.10 Exercises\nLet’s redo the exercises from previous chapter but now with tidyverse:\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths. Show all variables.\n\n\nmurders &lt;- mutate(murders, rate = total/population*10^5)\nfilter(murders, rate &lt; 1)\n\n           state abb        region population total      rate\n1         Hawaii  HI          West    1360301     7 0.5145920\n2          Idaho  ID          West    1567582    12 0.7655102\n3           Iowa  IA North Central    3046355    21 0.6893484\n4          Maine  ME     Northeast    1328361    11 0.8280881\n5      Minnesota  MN North Central    5303925    53 0.9992600\n6  New Hampshire  NH     Northeast    1316470     5 0.3798036\n7   North Dakota  ND North Central     672591     4 0.5947151\n8         Oregon  OR          West    3831074    36 0.9396843\n9   South Dakota  SD North Central     814180     8 0.9825837\n10          Utah  UT          West    2763885    22 0.7959810\n11       Vermont  VT     Northeast     625741     2 0.3196211\n12       Wyoming  WY          West     563626     5 0.8871131\n\n\n\nShow the subset of murders showing states with less than 1 per 100,000 deaths and in the West of the US. Don’t show the region variable.\n\n\nfilter(murders, rate &lt; 1 & region == \"West\")\n\n    state abb region population total      rate\n1  Hawaii  HI   West    1360301     7 0.5145920\n2   Idaho  ID   West    1567582    12 0.7655102\n3  Oregon  OR   West    3831074    36 0.9396843\n4    Utah  UT   West    2763885    22 0.7959810\n5 Wyoming  WY   West     563626     5 0.8871131\n\n\n\nShow the largest state with a rate less than 1 per 100,000.\n\n\nmurders |&gt; filter(rate &lt; 1) |&gt; slice_max(population)\n\n      state abb        region population total    rate\n1 Minnesota  MN North Central    5303925    53 0.99926\n\n\n\nShow the state with a population of more than 10 million with the lowest rate.\n\n\nmurders |&gt; filter(population &gt; 10^7) |&gt; slice_min(rate)\n\n     state abb    region population total    rate\n1 New York  NY Northeast   19378102   517 2.66796\n\n\n\nCompute the rate for each region of the US.\n\n\nmurders |&gt; group_by(region) |&gt; summarize(rate = sum(total)/sum(population)*10^5)\n\n# A tibble: 4 × 2\n  region         rate\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Northeast      2.66\n2 South          3.63\n3 North Central  2.73\n4 West           2.66\n\n\nFor the next exercises we will be using the data from the survey collected by the United States National Center for Health Statistics (NCHS). This center has conducted a series of health and nutrition surveys since the 1960’s. Starting in 1999, about 5,000 individuals of all ages have been interviewed every year and they complete the health examination component of the survey. Part of the data is made available via the NHANES package. Once you install the NHANES package, you can load the data like this:\n\nlibrary(NHANES)\n\n\nCheck for consistency between Race1 and Race3. Do any rows have different entries?\n\n\nNHANES |&gt; filter(!is.na(Race1) & !is.na(Race3)) |&gt; \n  filter(as.character(Race1) != as.character(Race3)) |&gt;\n  count(Race1, Race3)\n\n# A tibble: 1 × 3\n  Race1 Race3     n\n  &lt;fct&gt; &lt;fct&gt; &lt;int&gt;\n1 Other Asian   288\n\n\n\nDefine a new race variable that has as few NA and Other as possible.\n\n\ndat &lt;- NHANES %&gt;% mutate(Race = Race3) |&gt;\n  mutate(Race = if_else(is.na(Race), Race1, Race))\n\n\nCompute proportion of individuals that smoked at the time of the survey, by race category and gender. Keep track of how many people answered the question. Order the result by the number that answered. Read the help file for NHANES carefully before doing this one.\n\n\ndat |&gt; group_by(Gender, Race) |&gt; \n  summarize(n = sum(!is.na(Smoke100)), \n            smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE)) |&gt;\n  mutate(smoke = smoke/n) |&gt;\n  arrange(desc(n))\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12 × 4\n# Groups:   Gender [2]\n   Gender Race         n  smoke\n   &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n 1 female White     2472 0.185 \n 2 male   White     2377 0.219 \n 3 female Black      442 0.204 \n 4 male   Black      379 0.301 \n 5 male   Mexican    339 0.212 \n 6 female Mexican    262 0.111 \n 7 female Hispanic   218 0.0963\n 8 male   Hispanic   198 0.278 \n 9 female Other      177 0.203 \n10 male   Other      162 0.309 \n11 female Asian      112 0.0357\n12 male   Asian       97 0.175 \n\n\n\nCreate a new dataset that combines the Mexican and Hispanic, and removes the Other category. Hint: use the function forcats::fct_collapse().\n\n\ndat &lt;- dat |&gt; \n  mutate(Race = forcats::fct_collapse(Race, Hispanic = c(\"Hispanic\", \"Mexican\"))) |&gt;\n  filter(Race != \"Other\") |&gt;\n  mutate(Race = droplevels(Race))\n\n\nRecompute proportion of individuals that smoke now by race category and gender. Order by rate of smokers.\n\n\ndat |&gt; group_by(Gender, Race) |&gt; \n  summarize(n = sum(!is.na(Smoke100)), smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE)) |&gt;\n  mutate(smoke = smoke/n) |&gt;\n  arrange(desc(smoke))\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 4\n# Groups:   Gender [2]\n  Gender Race         n  smoke\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 male   Black      379 0.301 \n2 male   Hispanic   537 0.236 \n3 male   White     2377 0.219 \n4 female Black      442 0.204 \n5 female White     2472 0.185 \n6 male   Asian       97 0.175 \n7 female Hispanic   480 0.104 \n8 female Asian      112 0.0357\n\n\n\nCompute the median age by race category and gender order by Age.\n\n\ndat |&gt; group_by(Gender, Race) |&gt;\n  summarize(Age = median(Age)) |&gt;\n  arrange(Gender, Age)\n\n`summarise()` has grouped output by 'Gender'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 8 × 3\n# Groups:   Gender [2]\n  Gender Race       Age\n  &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt;\n1 female Hispanic    27\n2 female Black       33\n3 female Asian       37\n4 female White       42\n5 male   Hispanic    28\n6 male   Black       30\n7 male   Asian       31\n8 male   White       40\n\n\n\nNow redo the smoking rate calculation by age group. But first, remove individuals with no group and remove any age groups for which less than 10 people answered the question. Within each age group and Gender order by percent that smokes.\n\n\nres &lt;- dat |&gt; \n  filter(!is.na(AgeDecade)) |&gt;\n  group_by(AgeDecade) |&gt; \n  mutate(n = sum(!is.na(Smoke100))) |&gt; \n  ungroup() |&gt;\n  filter(n &gt;= 10) |&gt;\n  group_by(AgeDecade, Gender, Race) |&gt;\n  summarize(n = sum(!is.na(Smoke100)), \n            smoke = sum(SmokeNow == \"Yes\", na.rm = TRUE), \n            .groups = \"drop\") |&gt; ## This is similar to running ungroup() in a next step\n  mutate(smoke = smoke/n) |&gt;\n  arrange(AgeDecade, Gender, desc(smoke))\n\n## Bonus: a plot\nres |&gt; ggplot(aes(AgeDecade, smoke, color = Race)) +\n  geom_point() + \n  geom_line() + \n  facet_wrap(~Gender)\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?"
  },
  {
    "objectID": "07-dates-and-times.html#the-date-data-type",
    "href": "07-dates-and-times.html#the-date-data-type",
    "title": "7  Dates and times",
    "section": "7.1 The date data type",
    "text": "7.1 The date data type\nWe have described three main types of vectors: numeric, character, and logical. When analyzing data, we often encounter variables that are dates. Although we can represent a date with a string, for example September 27, 2023`, once we pick a reference day, referred to as the epoch by computer programmers, they can be converted to numbers by calculating the number of days since the epoch. In R and Unix, the epoch is defined as January 1, 1970. So, for example, January 2, 1970 is day 1, December 31, 1969 is day -1, and so on.\n\nx &lt;- as.Date(\"1970-01-01\")\ntypeof(x)\n\n[1] \"double\"\n\nclass(x)\n\n[1] \"Date\"\n\nas.numeric(x)\n\n[1] 0\n\n\n\nx &lt;- Sys.Date()\nas.numeric(x)\n\n[1] 19627\n\n\nThe date class let’s R know that it is date so you can extract year, months, days of the week etc…\nYou can make them look good using the format function:\n\nformat(x, \"%B %d, %Y\")\n\n[1] \"September 27, 2023\"\n\n\nThere are many formats:\n\nformat(x, \"%b %d %y\")\n\n[1] \"Sep 27 23\"\n\n\nTo see all the possibilities you can consult the data and time formats cheat sheet"
  },
  {
    "objectID": "07-dates-and-times.html#predefined-objects",
    "href": "07-dates-and-times.html#predefined-objects",
    "title": "7  Dates and times",
    "section": "7.2 Predefined objects",
    "text": "7.2 Predefined objects\n\nmonth.name\n\n [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\" \n\nmonth.abb\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\""
  },
  {
    "objectID": "07-dates-and-times.html#sec-lubridate",
    "href": "07-dates-and-times.html#sec-lubridate",
    "title": "7  Dates and times",
    "section": "7.3 The lubridate package",
    "text": "7.3 The lubridate package\nThe lubridate package provides tools to work with date and times.\n\nlibrary(lubridate)\n\nAn example of the many useful functions is as_date\n\nas_date(0)\n\n[1] \"1970-01-01\"\n\n\nAnother one is\n\ntoday()\n\n[1] \"2023-09-27\"\n\n\nWe can generate random dates like this:\n\nset.seed(2013 - 9 - 10)\nn &lt;- 10 \ndates &lt;- as_date(sample(0:as.numeric(today()), n, replace = TRUE))\n\nThe functions year, month and day extract those values:\n\ndata.frame(date = dates, month = month(dates), day = day(dates), year = year(dates))\n\n         date month day year\n1  1987-10-06    10   6 1987\n2  2020-10-02    10   2 2020\n3  1990-05-23     5  23 1990\n4  2009-03-04     3   4 2009\n5  1977-01-30     1  30 1977\n6  1986-03-06     3   6 1986\n7  2022-02-02     2   2 2022\n8  1977-07-29     7  29 1977\n9  2022-10-11    10  11 2022\n10 2007-11-08    11   8 2007\n\n\nWe can also extract the month labels:\n\nmonth(dates, label = TRUE)\n\n [1] Oct Oct May Mar Jan Mar Feb Jul Oct Nov\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nAnother useful set of functions are the parsers that convert strings into dates. The function ymd assumes the dates are in the format YYYY-MM-DD and tries to parse as well as possible.\n\nx &lt;- c(20090101, \"2009-01-02\", \"2009 01 03\", \"2009-1-4\",\n       \"2009-1, 5\", \"Created on 2009 1 6\", \"200901 !!! 07\")\nymd(x)\n\n[1] \"2009-01-01\" \"2009-01-02\" \"2009-01-03\" \"2009-01-04\" \"2009-01-05\"\n[6] \"2009-01-06\" \"2009-01-07\"\n\n\nA further complication comes from the fact that dates often come in different formats in which the order of year, month, and day are different. The preferred format is to show year (with all four digits), month (two digits), and then day, or what is called the ISO 8601. Specifically we use YYYY-MM-DD so that if we order the string, it will be ordered by date. You can see the function ymd returns them in this format.\nBut, what if you encounter dates such as “09/01/02”? This could be September 1, 2002 or January 2, 2009 or January 9, 2002. lubridate provides options:\n\nx &lt;- \"09/01/02\"\nymd(x)\n\n[1] \"2009-01-02\"\n\nmdy(x)\n\n[1] \"2002-09-01\"\n\ndmy(x)\n\n[1] \"2002-01-09\"\n\n\nThe lubridate package is also useful for dealing with times:\n\nnow()\n\n[1] \"2023-09-27 09:48:37 EDT\"\n\n\nYou can provide time zones too:\n\nnow(\"GMT\")\n\n[1] \"2023-09-27 13:48:37 GMT\"\n\n\nYou can see all the available time zones with OlsonNames() function.\nWe can extract hours, minutes, and seconds:\n\nnow() |&gt; hour()\n\n[1] 9\n\nnow() |&gt; minute()\n\n[1] 48\n\nnow() |&gt; second()\n\n[1] 37.46924\n\n\nThe package also includes a function to parse strings into times as well as parsers for time objects that include dates:\n\nx &lt;- c(\"12:34:56\")\nhms(x)\n\n[1] \"12H 34M 56S\"\n\nx &lt;- \"Nov/2/2012 12:34:56\"\nmdy_hms(x)\n\n[1] \"2012-11-02 12:34:56 UTC\""
  },
  {
    "objectID": "07-dates-and-times.html#sequences",
    "href": "07-dates-and-times.html#sequences",
    "title": "7  Dates and times",
    "section": "7.4 Sequences",
    "text": "7.4 Sequences\n\nx &lt;- seq(today(), today() + 7, by = \"days\")"
  },
  {
    "objectID": "07-dates-and-times.html#rounding",
    "href": "07-dates-and-times.html#rounding",
    "title": "7  Dates and times",
    "section": "7.5 Rounding",
    "text": "7.5 Rounding\n\nx &lt;- seq(today() - 365 + 1, today(), by = \"days\")\ntable(floor_date(x, unit = \"week\"))\n\n\n2022-09-25 2022-10-02 2022-10-09 2022-10-16 2022-10-23 2022-10-30 2022-11-06 \n         4          7          7          7          7          7          7 \n2022-11-13 2022-11-20 2022-11-27 2022-12-04 2022-12-11 2022-12-18 2022-12-25 \n         7          7          7          7          7          7          7 \n2023-01-01 2023-01-08 2023-01-15 2023-01-22 2023-01-29 2023-02-05 2023-02-12 \n         7          7          7          7          7          7          7 \n2023-02-19 2023-02-26 2023-03-05 2023-03-12 2023-03-19 2023-03-26 2023-04-02 \n         7          7          7          7          7          7          7 \n2023-04-09 2023-04-16 2023-04-23 2023-04-30 2023-05-07 2023-05-14 2023-05-21 \n         7          7          7          7          7          7          7 \n2023-05-28 2023-06-04 2023-06-11 2023-06-18 2023-06-25 2023-07-02 2023-07-09 \n         7          7          7          7          7          7          7 \n2023-07-16 2023-07-23 2023-07-30 2023-08-06 2023-08-13 2023-08-20 2023-08-27 \n         7          7          7          7          7          7          7 \n2023-09-03 2023-09-10 2023-09-17 2023-09-24 \n         7          7          7          4 \n\ntable(floor_date(x, unit = \"year\"))\n\n\n2022-01-01 2023-01-01 \n        95        270 \n\n\nWhat if I want to start counting on Mondays?\n\nx &lt;- seq(today() - weeks(1) + 1, today(), by = \"days\")\nwday(x)\n\n[1] 5 6 7 1 2 3 4\n\ndata.frame(day = x, week = floor_date(x, unit = \"week\", week_start = \"Sun\"))\n\n         day       week\n1 2023-09-21 2023-09-17\n2 2023-09-22 2023-09-17\n3 2023-09-23 2023-09-17\n4 2023-09-24 2023-09-24\n5 2023-09-25 2023-09-24\n6 2023-09-26 2023-09-24\n7 2023-09-27 2023-09-24"
  },
  {
    "objectID": "07-dates-and-times.html#day-of-the-year-or-month",
    "href": "07-dates-and-times.html#day-of-the-year-or-month",
    "title": "7  Dates and times",
    "section": "7.6 day of the year or month",
    "text": "7.6 day of the year or month\n\nyday(x)\n\n[1] 264 265 266 267 268 269 270\n\nmday(x)\n\n[1] 21 22 23 24 25 26 27"
  },
  {
    "objectID": "07-dates-and-times.html#exercises",
    "href": "07-dates-and-times.html#exercises",
    "title": "7  Dates and times",
    "section": "7.7 Exercises",
    "text": "7.7 Exercises\nIn the previous exercise section, we wrangled data from a PDF file containing vital statistics from Puerto Rico. We did this for the month of September. Below we include code that does it for all 12 months.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr   1.1.1     ✔ readr   2.1.4\n✔ forcats 1.0.0     ✔ stringr 1.5.0\n✔ ggplot2 3.4.2     ✔ tibble  3.2.1\n✔ purrr   1.0.1     ✔ tidyr   1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(purrr)\nlibrary(pdftools)\n\nUsing poppler version 22.02.0\n\nlibrary(dslabs)\n\nfn &lt;- system.file(\"extdata\", \"RD-Mortality-Report_2015-18-180531.pdf\",\n                  package=\"dslabs\")\ndat &lt;- map_df(str_split(pdf_text(fn), \"\\n\"), function(s){\n  s &lt;- str_trim(s)\n  header_index &lt;- str_which(s, \"2015\")[1]\n  tmp &lt;- str_split(s[header_index], \"\\\\s+\", simplify = TRUE)\n  month &lt;- tmp[1]\n  header &lt;- tmp[-1]\n  tail_index  &lt;- str_which(s, \"Total\")\n  n &lt;- str_count(s, \"\\\\d+\")\n  out &lt;- c(1:header_index, which(n == 1), \n           which(n &gt;= 28), tail_index:length(s))\n  res &lt;- s[-out] |&gt;  str_remove_all(\"[^\\\\d\\\\s]\") |&gt; str_trim() |&gt;\n    str_split_fixed(\"\\\\s+\", n = 6) \n  res &lt;- data.frame(res[,1:5]) |&gt; as_tibble() |&gt; \n    setNames(c(\"day\", header)) |&gt;\n    mutate(month = month, day = as.numeric(day)) |&gt;\n    pivot_longer(-c(day, month), names_to = \"year\", values_to = \"deaths\") |&gt;\n    mutate(deaths = as.numeric(deaths)) |&gt;\n    mutate(month = str_to_title(month)) |&gt;\n    mutate(month = if_else(month==\"Ago\", \"Aug\", month))\n}) \n\n\nMake sure that year is a number.\n\n\ndat &lt;- mutate(dat, year = as.numeric(year))\n\n\nWe want to make a plot of death counts versus date. A first step is to convert the month variable from characters to numbers. Hint: use month.abb.\n\n\ndat &lt;- dat |&gt; mutate(month = match(month, month.abb))\n\n\nCreate a new column date with the date for each observation. Hint: use the make_date function.\n\n\ndat &lt;- dat |&gt; mutate(date = make_date(year, month, day))\n\n\nPlot deaths versus date. Hint: the plot function can take dates for either axis.\n\n\nwith(dat, plot(date, deaths))\n\n\n\n\n\nNote that after May 31, 2018, the deaths are all 0. The data is probably not entered yet. We also see a drop off starting around May 1. Redefine dat to exclude observations taken on or after May 1, 2018. Then, remake the plot.\n\n\ndat &lt;- dat |&gt; filter(date &lt; make_date(2018, 5, 1))\nwith(dat, plot(date, deaths))\n\n\n\n\n\nRepeat the plot but use the day of the year on the x-axis instead of date and different colors for the different year. Hint: Use the col argument in plot.\n\n\nwith(dat, plot(yday(date), deaths, col = year - min(year) + 1))\n\n\n\n\n\nCompute the number deaths per day by month.\n\n\nres &lt;- dat |&gt; group_by(date = floor_date(date, unit = \"month\")) |&gt;\n  summarize(mean(deaths))\n\n\nShow the deaths per day for July and for September. What do you notice?\n\n\nres |&gt; filter(month(date) %in% c(7,9)) |&gt;\n  mutate(month = month(date), year = year(date)) |&gt;\n  arrange(month, year)\n\n# A tibble: 6 × 4\n  date       `mean(deaths)` month  year\n  &lt;date&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 2015-07-01           76.8     7  2015\n2 2016-07-01           79.2     7  2016\n3 2017-07-01           76.4     7  2017\n4 2015-09-01           75.1     9  2015\n5 2016-09-01           78.8     9  2016\n6 2017-09-01           98.4     9  2017\n\n\nSeptember 2017 is an outlier.\n\nCompute deaths per week and make a plot.\n\n\nres &lt;- dat |&gt; group_by(date = floor_date(date, unit = \"week\")) |&gt;\n  summarize(deaths = mean(deaths))\nwith(res, plot(date, deaths))"
  },
  {
    "objectID": "08-importing-data.html#r-base-functions",
    "href": "08-importing-data.html#r-base-functions",
    "title": "8  Importing data",
    "section": "8.1 R base functions",
    "text": "8.1 R base functions\nWe include example data files for practice in the dslabs package. They are stored here:\n\ndir &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nTake a look:\n\nlist.files(dir)\n\n [1] \"2010_bigfive_regents.xls\"                               \n [2] \"calificaciones.csv\"                                     \n [3] \"carbon_emissions.csv\"                                   \n [4] \"fertility-two-countries-example.csv\"                    \n [5] \"HRlist2.txt\"                                            \n [6] \"life-expectancy-and-fertility-two-countries-example.csv\"\n [7] \"murders.csv\"                                            \n [8] \"olive.csv\"                                              \n [9] \"RD-Mortality-Report_2015-18-180531.pdf\"                 \n[10] \"ssa-death-probability.csv\"                              \n\n\nCopy one of them to your working directory:\n\nfile_path &lt;- file.path(dir, \"murders.csv\")\nfile.copy(file_path, \"murders.csv\")\n\n[1] TRUE\n\n\nThe file.path function combines characters to form a complete path, ensuring compatibility with the respective operating system. Linux and Mac use forward slashes /, while Windows uses backslashes \\, to separate directories. This function is useful because often you want to define paths using a variable.\nThe file.copy function copies a file and returns TRUE if succesful. If the file exists it will not copy.\nWhat kind of file is it? Although the suffix usually tells us what type of file it is, there is no guarantee that these always match.\n\nreadLines(\"murders.csv\", n = 3)\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"         \n\n\nIt is comma delimited and has a header. You can import it like this:\n\ndat &lt;- read.csv(\"murders.csv\")\n\nThere are other importing function in base R: read.table, read.csv and read.delim, for example."
  },
  {
    "objectID": "08-importing-data.html#the-readr-and-readxl-packages",
    "href": "08-importing-data.html#the-readr-and-readxl-packages",
    "title": "8  Importing data",
    "section": "8.2 The readr and readxl packages",
    "text": "8.2 The readr and readxl packages\nTidyverse has improved versions of functions for importing data.\n\n8.2.1 readr\nThe readr package includes functions for reading data stored in text file spreadsheets into R. readr is part of the tidyverse package, but you can load it directly using:\n\nlibrary(readr)\n\nThe following functions are available to read-in spreadsheets:\n\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\n\nread_table\nwhite space separated values\ntxt\n\n\nread_csv\ncomma separated values\ncsv\n\n\nread_csv2\nsemicolon separated values\ncsv\n\n\nread_tsv\ntab delimited separated values\ntsv\n\n\nread_delim\ngeneral text file format, must define delimiter\ntxt\n\n\n\nthe readr equivalent of readLines is read_lines:\n\nread_lines(\"murders.csv\", n_max = 3)\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"         \n\n\nFrom the .csv suffix and the peek at the file, we know to use read_csv:\n\ndat &lt;- read_csv(\"murders.csv\")\n\nRows: 51 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): state, abb, region\ndbl (2): population, total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNote that we receive a message letting us know what data types were used for each column. Also note that dat is a tibble, not just a data frame. This is because read_csv is a tidyverse parser.\nA powerful added feature of read_csv is the col_type arguments that let’s you specify the data type of each column before reading. This can help with parsing dates or not letting an error like a letter in a column of numbers turn everything into a character.\n\n\n8.2.2 readxl\nMany spreadsheets are saved in Microsoft Excel format. For this we use parsers in the readxl package:\n\nlibrary(readxl)\n\nThe package provides functions to read-in Microsoft Excel formats:\n\n\n\n\n\n\n\n\nFunction\nFormat\nTypical suffix\n\n\n\n\nread_excel\nauto detect the format\nxls, xlsx\n\n\nread_xls\noriginal format\nxls\n\n\nread_xlsx\nnew format\nxlsx\n\n\n\nThe Microsoft Excel formats permit you to have more than one spreadsheet in one file. These are referred to as sheets. The functions listed above read the first sheet by default, but we can also read the others. The excel_sheets function gives us the names of all the sheets in an Excel file. These names can then be passed to the sheet argument in the three functions above to read sheets other than the first."
  },
  {
    "objectID": "08-importing-data.html#downloading-files",
    "href": "08-importing-data.html#downloading-files",
    "title": "8  Importing data",
    "section": "8.3 Downloading files",
    "text": "8.3 Downloading files\nA common place for data to reside is on the internet. When these data are in files, we can download them and then import them or even read them directly from the web.\n\nurl &lt;- \n  \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv\"\n\nThe read_csv file can read these files directly:\n\ndat &lt;- read_csv(url)\n\nYou can also download the file first using download.file or the Unix commands curl or wget.\n\ntmp_filename &lt;- tempfile()\ndownload.file(url, tmp_filename)\ndat &lt;- read_csv(tmp_filename)\n\nRows: 51 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): state, abb, region\ndbl (2): population, total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfile.remove(tmp_filename)\n\n[1] TRUE"
  },
  {
    "objectID": "08-importing-data.html#encoding",
    "href": "08-importing-data.html#encoding",
    "title": "8  Importing data",
    "section": "8.4 Encoding",
    "text": "8.4 Encoding\nRStudio assumes the Unicode encoding. A common pitfall in data analysis is assuming a file is Unicode when, in fact, it is something else.\nTo understand encoding, remember that everything on a computer needs to eventually be converted to 0s and 1s. ASCII is an encoding that maps characters to numbers. ASCII uses 7 bits (0s and 1s) which results in \\(2^7 = 128\\) unique items, enough to encode all the characters on an English language keyboard. However, other languages use characters not included in this encoding. For example, the é in México is not encoded by ASCII. For this reason, a new encoding, using more than 7 bits, was defined: Unicode. When using Unicode, one can chose between 8, 16, and 32 bits abbreviated UTF-8, UTF-16, and UTF-32 respectively. RStudio defaults to UTF-8 encoding. ASCII is a subset of UTF-8.\nTry reading in this file:\n\nurl &lt;- \"https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/calificaciones.csv\"\nreadLines(url, n = 2)\n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\"                       \n[2] \"\\\"Beyonc\\xe9\\\",\\\"04 de septiembre de 1981\\\",2023-09-22 02:11:02,\\\"87,5\\\"\"\n\n\nWhen you see these weird characters the problem is almost always that you are assuming the wrong encoding. You need to be a hacker to figure out, readr has a function that tries:\n\nguess_encoding(url)\n\n# A tibble: 3 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe first guess makes sense as Spanish is often saved using Latin-1 encoding, also known as ISO-8859 encoding because it was the first to include accents and other characters used in Spanish. Once we figure this out we can read in the file correctly:\n\nread_csv(url, locale = locale(encoding = \"ISO-8859-1\", decimal_mark = \",\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\ndbl  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05       99  \n3 João     10 de junio de 1931      2023-09-21 22:43:28       98.9\n4 López    24 de julio de 1969      2023-09-22 01:06:59       88.7\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37       93.1\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21       88.7\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02       83  \n\n\n\n\n[1] TRUE"
  },
  {
    "objectID": "08-importing-data.html#exercises",
    "href": "08-importing-data.html#exercises",
    "title": "8  Importing data",
    "section": "8.5 Exercises",
    "text": "8.5 Exercises\n\nUse the read_csv function to read each of the csv files that the following code saves in the files object. Hint: use the pattern in list.files to keep only the csv files.\n\n\nlibrary(readr)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfiles &lt;- list.files(path, pattern = \".csv\")\nres &lt;- lapply(files, function(fn) \n  read_csv(file.path(path, fn), show_col_types = FALSE))\n\nNew names:\n• `` -&gt; `...1`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n\nNote that you get a warning. To see which one you can run it one-by-one in a loop:\n\n\nfor (i in seq_along(files)) {\n  print(files[i])\n  read_csv(file.path(path, files[i]), show_col_types = FALSE)\n}\n\n[1] \"calificaciones.csv\"\n[1] \"carbon_emissions.csv\"\n[1] \"fertility-two-countries-example.csv\"\n[1] \"life-expectancy-and-fertility-two-countries-example.csv\"\n[1] \"murders.csv\"\n[1] \"olive.csv\"\n\n\nNew names:\n• `` -&gt; `...1`\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n[1] \"ssa-death-probability.csv\"\n\n\nolive.csv gives us a New names warning. This is because the first line of the file is missing the header for the first column.\n\nread_lines(file.path(path, \"olive.csv\"), n_max = 2)\n\n[1] \",Region,Area,palmitic,palmitoleic,stearic,oleic,linoleic,linolenic,arachidic,eicosenoic\"\n[2] \"1,North-Apulia,1,1,1075,75,226,7823,672,36,60,29\"                                       \n\n\nRead the help file for read_csv to figure out how to read in the file without reading this header. If you skip the header, you should not get this warning. Save the result to an object called dat.\n\nread_csv(file.path(path, \"olive.csv\"), col_names = FALSE, skip = 1)\n\nRows: 572 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): X2\ndbl (11): X1, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 572 × 12\n      X1 X2             X3    X4    X5    X6    X7    X8    X9   X10   X11   X12\n   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1 North-Apul…     1     1  1075    75   226  7823   672    36    60    29\n 2     2 North-Apul…     1     1  1088    73   224  7709   781    31    61    29\n 3     3 North-Apul…     1     1   911    54   246  8113   549    31    63    29\n 4     4 North-Apul…     1     1   966    57   240  7952   619    50    78    35\n 5     5 North-Apul…     1     1  1051    67   259  7771   672    50    80    46\n 6     6 North-Apul…     1     1   911    49   268  7924   678    51    70    44\n 7     7 North-Apul…     1     1   922    66   264  7990   618    49    56    29\n 8     8 North-Apul…     1     1  1100    61   235  7728   734    39    64    35\n 9     9 North-Apul…     1     1  1082    60   239  7745   709    46    83    33\n10    10 North-Apul…     1     1  1037    55   213  7944   633    26    52    30\n# ℹ 562 more rows\n\n\n\nA problem with the previous approach is that we don’t know what the columns represent. Type names(dat) to see that the names are not informative. Use the read_lines with argument n_max=1 to read just the first line.\n\n\nread_lines(file.path(path, \"olive.csv\"), n_max = 1)\n\n[1] \",Region,Area,palmitic,palmitoleic,stearic,oleic,linoleic,linolenic,arachidic,eicosenoic\"\n\n\nNotice that you can use this to assign names to the data frame.\n\ncolnames &lt;- read_lines(file.path(path, \"olive.csv\"), n_max = 1) \ncolnames &lt;- strsplit(colnames, \",\") |&gt; unlist()\ncolnames[1] &lt;- \"row_number\"\nnames(dat) &lt;- colnames\n\nWarning: The `value` argument of `names&lt;-` must have the same length as `x` as of tibble\n3.0.0."
  },
  {
    "objectID": "09-data-table.html#manipulating-data-tables",
    "href": "09-data-table.html#manipulating-data-tables",
    "title": "9  data.table",
    "section": "9.1 Manipulating data tables",
    "text": "9.1 Manipulating data tables\ndata.table is a separate package that needs to be installed. Once installed, we then need to load it:\n\nlibrary(data.table)\n\nWe will provide example code showing the data.table approaches to dplyr’s mutate, filter, select, group_by, and summarize shown in the tidyverse chapter. As in that chapter, we will use the murders dataset:\nThe first step when using data.table is to convert the data frame into a data.table object using the as.data.table function:\n\nmurders_dt &lt;- as.data.table(murders)\n\nWithout this initial step, most of the approaches shown below will not work.\n\n9.1.1 Selecting\nSelecting with data.table is done in a similar way to subsetting matrices. While with dplyr we write:\n\nselect(murders, state, region)\n\nIn data.table, we use notation similar to what is used with matrices:\n\nmurders_dt[, c(\"state\", \"region\")] |&gt; head()\n\n        state region\n1:    Alabama  South\n2:     Alaska   West\n3:    Arizona   West\n4:   Arkansas  South\n5: California   West\n6:   Colorado   West\n\n\nWe can also use the .() data.table notation to alert R that variables inside the parenthesis are column names, not objects in the R environment. So the above can also be written like this:\n\nmurders_dt[, .(state, region)] |&gt; head()\n\n        state region\n1:    Alabama  South\n2:     Alaska   West\n3:    Arizona   West\n4:   Arkansas  South\n5: California   West\n6:   Colorado   West\n\n\n\n\n9.1.2 Adding a column or changing columns\nWe learned to use the dplyr mutate function with this example:\n\nmurders &lt;- mutate(murders, rate = total / population * 100000)\n\ndata.table uses an approach that avoids a new assignment (update by reference). This can help with large datasets that take up most of your computer’s memory. The data.table :=` function permits us to do this:\n\nmurders_dt[, rate := total / population * 100000]\n\nThis adds a new column, rate, to the table. Notice that, as in dplyr, we used total and population without quotes.\nWe can see that the new column is added:\n\nhead(murders_dt)\n\n        state abb region population total     rate\n1:    Alabama  AL  South    4779736   135 2.824424\n2:     Alaska  AK   West     710231    19 2.675186\n3:    Arizona  AZ   West    6392017   232 3.629527\n4:   Arkansas  AR  South    2915918    93 3.189390\n5: California  CA   West   37253956  1257 3.374138\n6:   Colorado  CO   West    5029196    65 1.292453\n\n\nTo define new multiple columns, we can use the := function with multiple arguments:\n\nmurders_dt[, \":=\"(rate = total / population * 100000, rank = rank(population))]\n\n\n\n9.1.3 Technical detail: reference versus copy\nThe data.table package is designed to avoid wasting memory. So if you make a copy of a table, like this:\n\nx &lt;- data.table(a = 1)\ny &lt;- x\n\ny is actually referencing x, it is not an new opject: it’s just another name for x. Until you change y, a new object will not be made. However, the := function changes by reference so if you change x, a new object is not made and y continues to be just another name for x:\n\nx[,a := 2]\ny\n\n   a\n1: 2\n\n\nYou can also change x like this:\n\ny[,a := 1]\nx\n\n   a\n1: 1\n\n\nTo avoid this, you can use the copy function which forces the creation of an actual copy:\n\nx &lt;- data.table(a = 1)\ny &lt;- copy(x)\nx[,a := 2]\ny\n\n   a\n1: 1\n\n\nNote that the function as.data.table creates a copy of the data frame being converted. However, if working with a large data frames it is helpful to avoid this, and you can do this by using setDT.\n\nx &lt;- data.frame(a = 1)\nsetDT(x)\n\nHowever, note that because no copy is being made, be aware that the following code does not create a new object:\n\nx &lt;- data.frame(a = 1)\ny &lt;- setDT(x)\n\nThe objects x and y are referencing the same data table:\n\nx[,a := 2]\ny\n\n   a\n1: 2\n\n\n\n\n9.1.4 Subsetting\nWith dplyr, we filtered like this:\n\nfilter(murders, rate &lt;= 0.7)\n\nWith data.table, we again use an approach similar to subsetting matrices, except data.table knows that rate refers to a column name and not an object in the R environment:\n\nmurders_dt[rate &lt;= 0.7]\n\n           state abb        region population total      rate rank\n1:        Hawaii  HI          West    1360301     7 0.5145920   12\n2:          Iowa  IA North Central    3046355    21 0.6893484   22\n3: New Hampshire  NH     Northeast    1316470     5 0.3798036   10\n4:  North Dakota  ND North Central     672591     4 0.5947151    4\n5:       Vermont  VT     Northeast     625741     2 0.3196211    3\n\n\nNotice that we can combine the filter and select into one succint command. Here are the state names and rates for those with rates below 0.7.\n\nmurders_dt[rate &lt;= 0.7, .(state, rate)]\n\n           state      rate\n1:        Hawaii 0.5145920\n2:          Iowa 0.6893484\n3: New Hampshire 0.3798036\n4:  North Dakota 0.5947151\n5:       Vermont 0.3196211\n\n\nCompare to the dplyr approach:\n\nmurders |&gt; filter(rate &lt;= 0.7) |&gt; select(state, rate)"
  },
  {
    "objectID": "09-data-table.html#summarizing-data",
    "href": "09-data-table.html#summarizing-data",
    "title": "9  data.table",
    "section": "9.2 Summarizing data",
    "text": "9.2 Summarizing data\nAs an example, we will use the heights dataset:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(dslabs)\nheights_dt &lt;- as.data.table(heights)\n\nIn data.table, we can call functions inside .() and they will be applied to rows. So the equivalent of:\n\ns &lt;- heights |&gt; \n  summarize(average = mean(height), standard_deviation = sd(height))\n\nin dplyr is the following in data.table:\n\ns &lt;- heights_dt[, .(average = mean(height), standard_deviation = sd(height))]\n\nNote that this permits a compact way of subsetting and then summarizing. Instead of:\n\ns &lt;- heights |&gt; \n  filter(sex == \"Female\") |&gt;\n  summarize(average = mean(height), standard_deviation = sd(height))\n\nwe can write:\n\ns &lt;- heights_dt[sex == \"Female\", .(average = mean(height), standard_deviation = sd(height))]\n\n\n9.2.1 Multiple summaries\nWe previously defined the function:\n\nmedian_min_max &lt;- function(x){\n  qs &lt;- quantile(x, c(0.5, 0, 1))\n  data.frame(median = qs[1], minimum = qs[2], maximum = qs[3])\n}\n\nSimilar to dplyr, we can call this function within .() to obtain the three number summary:\n\nheights_dt[, .(median_min_max(height))]\n\n   median minimum  maximum\n1:   68.5      50 82.67717\n\n\n\n\n9.2.2 Group then summarize\nThe group_by followed by summarize in dplyr is performed in one line in data.table. We simply add the by argument to split the data into groups based on the values in categorical variable:\n\nheights_dt[, .(average = mean(height), standard_deviation = sd(height)), by = sex]\n\n      sex  average standard_deviation\n1:   Male 69.31475           3.611024\n2: Female 64.93942           3.760656"
  },
  {
    "objectID": "09-data-table.html#sorting-data-frames",
    "href": "09-data-table.html#sorting-data-frames",
    "title": "9  data.table",
    "section": "9.3 Sorting data frames",
    "text": "9.3 Sorting data frames\nWe can order rows using the same approach we use for filter. Here are the states ordered by murder rate:\n\nmurders_dt[order(population)] |&gt; head()\n\n                  state abb        region population total       rate rank\n1:              Wyoming  WY          West     563626     5  0.8871131    1\n2: District of Columbia  DC         South     601723    99 16.4527532    2\n3:              Vermont  VT     Northeast     625741     2  0.3196211    3\n4:         North Dakota  ND North Central     672591     4  0.5947151    4\n5:               Alaska  AK          West     710231    19  2.6751860    5\n6:         South Dakota  SD North Central     814180     8  0.9825837    6\n\n\nN To sort the table in descending order, we can order by the negative of population or use the decreasing argument:\n\nmurders_dt[order(population, decreasing = TRUE)] \n\n\n9.3.1 Nested sorting\nSimilarly, we can perform nested ordering by including more than one variable in order\n\nmurders_dt[order(region, rate)]"
  },
  {
    "objectID": "09-data-table.html#optional-exercises-will-not-be-included-in-the-midterms",
    "href": "09-data-table.html#optional-exercises-will-not-be-included-in-the-midterms",
    "title": "9  data.table",
    "section": "9.4 Optional exercises (will not be included in the midterms)",
    "text": "9.4 Optional exercises (will not be included in the midterms)\n\nLoad the data.table package and the murders dataset and convert it to data.table object:\n\n\nlibrary(data.table)\nlibrary(dslabs)\nmurders_dt &lt;- as.data.table(murders)\n\nRemember you can add columns like this:\n\nmurders_dt[, population_in_millions := population / 10^6]\n\nAdd a murders column named rate with the per 100,000 murder rate as in the example code above.\n\nAdd a column rank containing the rank, from highest to lowest murder rate.\nIf we want to only show the states and population sizes, we can use:\n\n\nmurders_dt[, .(state, population)] \n\nShow the state names and abbreviations in murders.\n\nYou can show just the New York row like this:\n\n\nmurders_dt[state == \"New York\"]\n\nYou can use other logical vectors to filter rows.\nShow the top 5 states with the highest murder rates. After we add murder rate and rank, do not change the murders dataset, just show the result. Remember that you can filter based on the rank column.\n\nWe can remove rows using the != operator. For example, to remove Florida, we would do this:\n\n\nno_florida &lt;- murders_dt[state != \"Florida\"]\n\nCreate a new data frame called no_south that removes states from the South region. How many states are in this category? You can use the function nrow for this.\n\nWe can also use %in% to filter. You can therefore see the data from New York and Texas as follows:\n\n\nmurders_dt[state %in% c(\"New York\", \"Texas\")]\n\nCreate a new data frame called murders_nw with only the states from the Northeast and the West. How many states are in this category?\n\nSuppose you want to live in the Northeast or West and want the murder rate to be less than 1. We want to see the data for the states satisfying these options. Note that you can use logical operators with filter. Here is an example in which we filter to keep only small states in the Northeast region.\n\n\nmurders_dt[population &lt; 5000000 & region == \"Northeast\"]\n\nMake sure murders has been defined with rate and rank and still has all states. Create a table called my_states that contains rows for states satisfying both the conditions: they are in the Northeast or West and the murder rate is less than 1. Show only the state name, the rate, and the rank."
  },
  {
    "objectID": "pset1.html#measles",
    "href": "pset1.html#measles",
    "title": "Problem Set 1",
    "section": "Measles",
    "text": "Measles\n\nLoad the dslabs package and figure out what is in the us_contagious_diseases dataset. Create a data frame, call it avg, that has a column for year, and a rate column containing the cases of Measles per 10,000 people per year in the US. Because we start in 1928, exclude Alaska and Hawaii. Make sure to take into account the number of weeks reporting each year. If a week was not report, it should not be included in the calculation of the rate.\n\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.1     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dslabs)\navg &lt;- us_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == \"Measles\" & weeks_reporting &gt; 0) |&gt;\n  group_by(year) |&gt;\n  summarize(rate = sum(count*52/weeks_reporting, na.rm = TRUE)/sum(population) * 10000)\n\n\nUse the data frame avg to make a trend plot showing the cases rate for Measles per year. Add a vertical line showing the year the Measles vaccines was introduced. Write a short paragraph describing the graph to someone you are urging to take the Measles vaccines.\n\n\nlibrary(ThemePark)\navg |&gt; ggplot(aes(year, rate)) + geom_line() + \n  geom_vline(xintercept = 1963, color = \"blue\") +\n  theme_barbie()\n\n\n\n\n\nIs the pattern observed above the same for each state? Add a grey trend line for each state to the plot above. Use a transformation that keeps the high rates from dominating the figure.\n\n\nus_contagious_diseases |&gt;\n  filter(disease == \"Measles\" & weeks_reporting &gt; 0) |&gt;\n  mutate(rate = count*52/weeks_reporting/population * 10000) |&gt;\n  ggplot(aes(x = year)) +\n  geom_line(aes(y = rate, group = state), color = \"grey\", alpha = 0.5) +\n  geom_line(data = avg, aes(x = year, y = rate)) + ## avg was defined in previous exercise\n  scale_y_continuous(trans = \"sqrt\") +\n  geom_vline(xintercept = 1963, color = \"blue\") \n\nWarning: Removed 14 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nIn the plot above we can’t tell which state is which curve. Using color would be challenging as it is hard if not impossible to find 48 colors we humans can distinguish. To make a plot where you can compare states knowing which is which, use one of the axis for state and the other for year, and then use hue or intensity as a visual cue for rates. Use a sqrt transformation to avoid the higher rates taking up all the color scale. Use grey to denote missing data. Order the states based on their highest peak.\n\n\nlibrary(RColorBrewer)\nmycolors &lt;- brewer.pal(9, \"Reds\")\nus_contagious_diseases |&gt;\n  filter(!state %in% c(\"Hawaii\",\"Alaska\") & disease == \"Measles\") |&gt;\n  mutate(rate = count/population*10000*52/weeks_reporting,\n         state = reorder(state, rate, max, na.rm = TRUE)) |&gt;\n  ggplot(aes(year, state, fill = rate)) +\n  geom_tile(color = \"grey\") +\n  scale_x_continuous(expand = c(0,0)) + ## to remove extra space on sides\n  scale_fill_gradientn(colors = mycolors, trans = \"sqrt\") +\n  geom_vline(xintercept = 1963, color = \"blue\") +\n  theme_minimal() +  \n  theme(panel.grid = element_blank(), \n        legend.position = \"bottom\", \n        text = element_text(size = 8)) +\n  labs(title = \"Measles cases per year in the US\", x = \"\", y = \"\")"
  },
  {
    "objectID": "pset1.html#covid-19",
    "href": "pset1.html#covid-19",
    "title": "Problem Set 1",
    "section": "COVID-19",
    "text": "COVID-19\n\nThe csv file shared here includes weekly data on SARS-CoV-2 reported cases, tests, COVID-19 hospitalizations and deaths, and vaccination rates by state.\n\n\nImport the file into R without making a copy on your computer.\nExamine the dataset.\nWrite a sentence describing each variable in the dataset.\n\n\nurl &lt;- \"https://raw.githubusercontent.com/datasciencelabs/2023/main/data/covid19-data.csv\"\ndat &lt;- read_csv(url) \n\nRows: 9853 Columns: 15\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): state, state_name\ndbl (13): population, region, mmwr_year, mmwr_week, cases, tests, hosp, deat...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nOne of these columns could benefit from being a factor. Identify the column and convert it to factor.\n\n\ndat &lt;- mutate(dat, region = factor(region))\n\n\nRather than providing a date, the dataset provides MMWR year and week. Read this document and write a function to convert these to the start of the MMWR week in ISO-8601.\n\n\nlibrary(lubridate)\nmmwr_to_date &lt;- function(mmwr_year, mmwr_week) {\n  first_day &lt;- floor_date(make_date(mmwr_year, 1, 4) , unit = \"week\")\n  date &lt;- first_day + weeks(mmwr_week - 1) \n  return(date)\n}\n\n\nAdd a columns start_date and end_date with the start and end of the MMWR week. Confirm that it worked by computing the MMWR week and year for both start and end date and comparing it to the MMWR week and year provided.\n\n\ndat &lt;- dat |&gt; mutate(start_date = mmwr_to_date(mmwr_year, mmwr_week),\n                     end_date = start_date + days(6))\n\n\ndat &lt;- dat |&gt; mutate(start_date = mmwr_to_date(mmwr_year, mmwr_week),\n                   end_date = start_date + days(6))\n## check : these should all be TRUE\ndat |&gt; summarize(w1 = all(epiweek(start_date) == mmwr_week),\n                 y1 = all(epiyear(start_date) == mmwr_year),\n                 w2 = all(epiweek(end_date) == mmwr_week),\n                 y2 = all(epiyear(end_date) == mmwr_year))\n\n# A tibble: 1 × 4\n  w1    y1    w2    y2   \n  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;\n1 TRUE  TRUE  TRUE  TRUE \n\n\n\nMake a trend plot similar to the one we made for Measles:\n\n\nInclude a trend line for the US cases rate. Use per 100,000 person per week as the unit.\nAdd a trend for each state, with color representing region.\nUse the end of the week date for your x-axis.\nAdd a vertical dashed line on the day COVID-19 vaccination started being delivered in the US.\n\nWrite a paragraph describing the COVID-19 pandemic by describing the plot.\n\nThe number of cases depends on testing capacity. Note that during the start of the pandemic, when we know many people died, there are not that many cases reported. Also notice somewhat large variability across states that might not be consistent with actual prevalence. The tests columns provides the cumulative number of tests performed by the data represented by the week. This data is not official CDC data. It was provided by Johns Hopkins Coronavirus Resource Center. Before using the data, explore the data for each state carefully looking for potential problems.\n\nFor each state compute and plot the number of tests perforemd each week. Look at the plot for each state and look for possible problems. No need to make this plot pretty since we are just using it for data exploration. Report any inconsistencies if any.\n\ndat |&gt; \n  filter(!is.na(tests)) |&gt;\n  arrange(end_date) |&gt;\n  group_by(state) |&gt;\n  mutate(tests = diff(c(0,tests))) |&gt;\n  ggplot(aes(end_date, tests/10^5)) + geom_line() +\n  facet_wrap(~state, scales = \"free_y\")\n\n\n\n\n\nTo see if the inconsistencies seen in the previous plot are a problem if we are only going to look at the total number of tests at the end, plot the cumulative tests for each of the states with inconsistencies and see if the results are sensible. Explain your answer in 1-2 sentences.\nJHU stopped reporting some time in 2021. What was that date? Show the day in the format September 18, 2022.\nCompute the number of tests per capita for the last day JHU reported these statistics. Make a boxplot of these values for each region and include the state level data with the state abbreviation as a label. Write a sentences describing these differences you see and how this could affect our interpretation of differences in cases rates across states.\n\n\nlast_day &lt;- as_date(\"2021-1-2\") ## this should come from the previous exercise... \ndat |&gt; filter(end_date == last_day) |&gt;\n  ggplot(aes(region, tests/population)) +\n  geom_boxplot() +\n  geom_text(aes(label = state)) +\n  labs(x = \"Region\", y = \"Test per capita\", title = \"SARS-COV2 tests per person\")\n\n\n\n\n\nAlthough JHU stopped collecting testing data from the states, the CDC collected data from a few laboratories. We provide these date in this url.\n\n\nImport the data into R without downloading the file.\nMake sure that you create a data frame with a column with dates in Dates format and tests as numbers.\n\n\nurl &lt;- \"https://raw.githubusercontent.com/datasciencelabs/2023/main/data/covid19-tests.txt\"\ntests &lt;- read_delim(url, delim = \" \") \n\nRows: 183 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \" \"\nchr (2): date, tests\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ntests &lt;- tests |&gt; mutate(date = mdy(date), tests = parse_number(tests))\n## alternatively we can re-read the file\ntests &lt;- read_delim(url, delim = \" \", \n                    col_types = cols(date = col_date(format = \"%m-%d-%y\"),\n                                     tests = col_number()))\n\n\nPlot the tests per week to see the trend in time. Write a sentence of the pattern you see.\n\nAt the start of the pandemic there were few tests conducted and now we are also seeing a drop.\n\nThe analysis on tests points to cases as not being a good measure of the state of the pandemic. Remake the trend plot but using death rates instead of cases rates. Write a sentence on how this plot better shows about the start of the pandemic that the cases plot did not.\n\nWe see that the highest death rates were actually at the start of the pandemic and that is was particularly bad in the northeast.\n\nWe want to examine the percent of the population that completed the first series of vaccines, received the booster, and received the bivalent booster, respectively. First run this line of code and examine what it does.\n\n\ntmp &lt;- dat |&gt; \n  pivot_longer(c(series_complete, booster, bivalent), names_to = \"series\", values_to = \"percent\") |&gt;\n  select(state, region, population, end_date, series, percent) |&gt;\n  filter(!is.na(percent)) |&gt;\n  mutate(percent = percent/population,\n         series = factor(series, c(\"series_complete\", \"booster\", \"bivalent\"))) \n\nThen make a plot showing the percent of population vaccination for each state. Use color to represent region.\n\nShow the dates on the x axis with the month abbreviation and year.\nPlace the three plots vertically, on top of each other.\nShow percentages on the y axis. Hint: use scales::percent.\n\n\nFor each period used to make the trend plot for the three series, make a boxplot showing the maximum percentage reached by every state stratified by region. Let the range of the y axis adapt to the range of each group.\nUse the plot above to define four periods: No vaccine, First vaccine, Booster, and Bivalent. Get the dates when each vaccine series starts (these don’t have to be exact). Create a version of your data frame with a column called period that keeps the period associated with the week contained in each row.\nDefine a new variable that stores the maximum vaccination percentage reached during each period. But for the first vaccine period use series_complete, for the booster period use the booster column, and for the bivalent period use the bivalent percentages. Remove data from the no vaccine period. The make a plot comparing the COVID-19 death rate to the percent vaccinated. Use color to denote region. You should produce three plots, each with it’s own axes range. Put the three plots in three entries of a 2x2 layout. Comment on what you observe.\nDoes population density have an effect on infections? Use the state.area predefined variable to add compute population density. Make a histogram and density plot of state densities. Note that you will have to add the geographical area for Puerto Rico and DC as it is not included in state.area\n\n\nmy.state.abb &lt;- c(state.abb, \"PR\", \"DC\")\nmy.state.area &lt;- c(state.area, 5325, 69)\npopdens &lt;- dat |&gt; filter(end_date == min(end_date)) |&gt; \n  select(state, population) |&gt;\n  mutate(area = my.state.area[match(state, my.state.abb)]) |&gt; \n  mutate(popdens = population / area) \n  \npopdens |&gt; ggplot(aes(popdens)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25, color = \"black\") +\n  geom_density() +\n  scale_x_log10() +\n  labs(title = \"Distribution of poplation density across states\", x = \"Population density\", y = \"Density\")\n\n\n\n\n\nPlot death rates versus density for the four periods defined above."
  },
  {
    "objectID": "10-distributions.html#case-study-describing-student-heights",
    "href": "10-distributions.html#case-study-describing-student-heights",
    "title": "10  Distributions",
    "section": "10.1 Case study: describing student heights",
    "text": "10.1 Case study: describing student heights\nWe will study self reported heights from studnets from past classes:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nhead(heights)\n\n     sex height\n1   Male     75\n2   Male     70\n3   Male     68\n4   Male     74\n5   Male     61\n6 Female     65"
  },
  {
    "objectID": "10-distributions.html#distributions",
    "href": "10-distributions.html#distributions",
    "title": "10  Distributions",
    "section": "10.2 Distributions",
    "text": "10.2 Distributions\nThe most basic statistical summary of a list of objects or numbers is its distribution.\n\nprop.table(table(heights$sex))\n\n\n   Female      Male \n0.2266667 0.7733333 \n\n\nHere is the distribution for the regions in the murders dataset:\n\n\n\n\n\n\n10.2.1 Histograms\nCumulative distributions function shows everything you need to know the distribution.\n\n\n\n\n\nHistograms lose a bit fo information but are easier to read:\n\n\n\n\n\n\n\n10.2.2 Smoothed density\nSmooth density plots relay the same information as a histogram but are aesthetically more appealing. Here is what a smooth density plot looks like for our heights data:\n\n\n\n\n\nAn advantage is that it is easy to show more than one:\n\n\n\n\n\nWith the right argument, ggplot automatically shades the intersecting region with a different color.\n\n\n10.2.3 The normal distribution\nThe normal distribution, also known as the bell curve and as the Gaussian distribution. Here is what the normal distribution looks like:\n\n\n\n\n\nA useful characteristic of the normal distribution is that it is defined by just two numbers: the average (also called mean) and the standard deviation.\nSo for the male height data we can define the average of standard deviation like this:\n\nindex &lt;- heights$sex == \"Male\"\nx &lt;- heights$height[index]\nm &lt;- sum(x) / length(x)\ns &lt;- sqrt(sum((x - mu)^2)/length(x))\n\nThe pre-built functions mean and sd can be used here: :::\n\nm &lt;- mean(x)\ns &lt;- sd(x)\nc(average = m, sd = s)\n\n  average        sd \n69.314755  3.611024 \n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pre-built functions mean and sd (note that, for reasons explained in statistics textbooks,sd divides by length(x)-1 rather than length(x)) can be used here:\n\n\nHere is a plot of the smooth density and the normal distribution with mean = 69.3 and SD = 3.6 plotted as a black line with our student height smooth density in blue:"
  },
  {
    "objectID": "10-distributions.html#boxplots",
    "href": "10-distributions.html#boxplots",
    "title": "10  Distributions",
    "section": "10.3 Boxplots",
    "text": "10.3 Boxplots\nBoxplots provide a five number summary (and shows outliers):\n\n\n\n\n\nIn this case, the histogram above or a smooth density plot would serve as a relatively succinct summary."
  },
  {
    "objectID": "10-distributions.html#sec-dataviz-stratification",
    "href": "10-distributions.html#sec-dataviz-stratification",
    "title": "10  Distributions",
    "section": "10.4 Stratification",
    "text": "10.4 Stratification\nShowing conditional distributions is often very informative\n\n\n\n\n\nWe also see the normal approximation might not be useful for females:\n\n\n\n\n\nRegarding the five smallest values, note that these values are:\n\nheights |&gt; filter(sex == \"Female\") |&gt; \n  top_n(5, desc(height)) |&gt;\n  pull(height)\n\n[1] 51 53 55 52 52\n\n\nBecause these are reported heights, a possibility is that the student meant to enter 5'1\", 5'2\", 5'3\" or 5'5\"."
  },
  {
    "objectID": "10-distributions.html#exercises",
    "href": "10-distributions.html#exercises",
    "title": "10  Distributions",
    "section": "10.5 Exercises",
    "text": "10.5 Exercises\n\nSuppose we can’t make a plot and want to compare the distributions side by side. We can’t just list all the numbers. Instead, we will look at the percentiles. Create a five row table showing female_percentiles and male_percentiles with the 10th, 30th, 50th, 70th, & 90th percentiles for each sex. Then create a data frame with these two as columns.\nStudy the following boxplots showing population sizes by country:\n\n\n\n\n\n\nWhich continent has the country with the biggest population size?\n\nWhat continent has the largest median population size?\nWhat is median population size for Africa to the nearest million?\nWhat proportion of countries in Europe have populations below 14 million?\n\n\n0.99\n0.75\n0.50\n0.25\n\n\nIf we use a log transformation, which continent shown above has the largest interquartile range?\nLoad the height data set and create a vector x with just the male heights:\n\n\nlibrary(dslabs)\nx &lt;- heights$height[heights$sex==\"Male\"]\n\nWhat proportion of the data is between 69 and 72 inches (taller than 69, but shorter or equal to 72)? Hint: use a logical operator and mean."
  },
  {
    "objectID": "11-ggplot2.html#the-components-of-a-graph",
    "href": "11-ggplot2.html#the-components-of-a-graph",
    "title": "11  ggplot2",
    "section": "11.1 The components of a graph",
    "text": "11.1 The components of a graph\nWe will construct a graph that summarizes the US murders dataset that looks like this:\n\n\n\n\n\nThe first step in learning ggplot2 is to be able to break a graph apart into components. Let’s break down the plot above and introduce some of the ggplot2 terminology. The main three components to note are:\n\nData: The US murders data table is being summarized. We refer to this as the data component.\nGeometry: The plot above is a scatterplot. This is referred to as the geometry component. Other possible geometries are barplot, histogram, smooth densities, qqplot, and boxplot. We will learn more about these in the Data Visualization part of the book.\nAesthetic mapping: The plot uses several visual cues to represent the information provided by the dataset. The two most important cues in this plot are the point positions on the x-axis and y-axis, which represent population size and the total number of murders, respectively. Each point represents a different observation, and we map data about these observations to visual cues like x- and y-scale. Color is another visual cue that we map to region. We refer to this as the aesthetic mapping component. How we define the mapping depends on what geometry we are using.\n\nWe also note that:\n\nThe points are labeled with the state abbreviations.\nThe range of the x-axis and y-axis appears to be defined by the range of the data. They are both on log-scales.\nThere are labels, a title, a legend, and we use the style of The Economist magazine.\n\nWe will now construct the plot piece by piece."
  },
  {
    "objectID": "11-ggplot2.html#ggplot-objects",
    "href": "11-ggplot2.html#ggplot-objects",
    "title": "11  ggplot2",
    "section": "11.2 ggplot objects",
    "text": "11.2 ggplot objects\nStart by defining the dataset:\n\nggplot(data = murders)\n\nWe can also use the pipe:\n\nmurders |&gt; ggplot()\n\n\n\n\nWe call aslo assign the output to a variabel\n\np &lt;- ggplot(data = murders)\nclass(p)\n\n[1] \"gg\"     \"ggplot\"\n\n\nTo see the plot we can print it:\n\nprint(p)\np"
  },
  {
    "objectID": "11-ggplot2.html#geometries",
    "href": "11-ggplot2.html#geometries",
    "title": "11  ggplot2",
    "section": "11.3 Geometries",
    "text": "11.3 Geometries\nIn ggplot2 we create graphs by adding layers. Layers can define geometries, compute summary statistics, define what scales to use, or even change styles. To add layers, we use the symbol +. In general, a line of code will look like this:\n\n\nDATA |&gt; ggplot() + LAYER 1 + LAYER 2 + … + LAYER N\n\n\nUsually, the first added layer defines the geometry. We want to make a scatterplot. What geometry do we use?\nLet’s look at the cheat sheet: https://rstudio.github.io/cheatsheets/data-visualization.pdf"
  },
  {
    "objectID": "11-ggplot2.html#aesthetic-mappings",
    "href": "11-ggplot2.html#aesthetic-mappings",
    "title": "11  ggplot2",
    "section": "11.4 Aesthetic mappings",
    "text": "11.4 Aesthetic mappings\nTo make a scatter plot we use geom_points. Take a look at the help file and learn that this is how we use it:\n\nmurders |&gt; ggplot() + \n  geom_point(aes(x = population/10^6, y = total))\n\nSince we defined p above, we can add a layer like this:\n\np + geom_point(aes(population/10^6, total))"
  },
  {
    "objectID": "11-ggplot2.html#layers",
    "href": "11-ggplot2.html#layers",
    "title": "11  ggplot2",
    "section": "11.5 Layers",
    "text": "11.5 Layers\nTo add text we use geom_text:\n\np + geom_point(aes(population/10^6, total)) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\nAs an example of the unique behavior of aes mentioned above, note that this call:\n\np_test &lt;- p + geom_text(aes(population/10^6, total, label = abb))\n\nis fine, whereas this call:\n\np_test &lt;- p + geom_text(aes(population/10^6, total), label = abb) \n\nwill give you an error since abb is not found because it is outside of the aes function. The layer geom_text does not know where to find abb since it is a column name and not a global variable.\n\n11.5.1 Tinkering with arguments\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb))\n\n\n\n\n\np + geom_point(aes(population/10^6, total), size = 3) +\n  geom_text(aes(population/10^6, total, label = abb), nudge_x = 1.5)"
  },
  {
    "objectID": "11-ggplot2.html#global-versus-local-aesthetic-mappings",
    "href": "11-ggplot2.html#global-versus-local-aesthetic-mappings",
    "title": "11  ggplot2",
    "section": "11.6 Global versus local aesthetic mappings",
    "text": "11.6 Global versus local aesthetic mappings\n\nargs(ggplot)\n\nfunction (data = NULL, mapping = aes(), ..., environment = parent.frame()) \nNULL\n\n\nWe can define a global aes in the ggplot function. All the layers will assume this mapping unless we explicitly define another one:\n\np &lt;- murders |&gt; ggplot(aes(population/10^6, total, label = abb))\np + geom_point(size = 3) + \n  geom_text(nudge_x = 1.5)\n\n\n\n\nWe can overide the global aes by defining one in the geometry functions:\n\np + geom_point(size = 3) +  \n  geom_text(aes(x = 10, y = 800, label = \"Hello there!\"))"
  },
  {
    "objectID": "11-ggplot2.html#scales",
    "href": "11-ggplot2.html#scales",
    "title": "11  ggplot2",
    "section": "11.7 Scales",
    "text": "11.7 Scales\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_continuous(trans = \"log10\") +\n  scale_y_continuous(trans = \"log10\") \n\n\n\n\nThis particular transformation is so common that ggplot2 provides the specialized functions scale_x_log10and scale_y_log10, which we can use to rewrite the code like this:\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "11-ggplot2.html#labels-and-titles",
    "href": "11-ggplot2.html#labels-and-titles",
    "title": "11  ggplot2",
    "section": "11.8 Labels and titles",
    "text": "11.8 Labels and titles\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\")\n\n\n\n\nWe can also use the labs function:\n\np + geom_point(size = 3) +  \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  labs(x = \"Populations in millions (log scale)\", \n       y = \"Total number of murders (log scale)\", \n       title = \"US Gun Murders in 2010\")\n\nWe are almost there! All we have left to do is add color, a legend, and optional changes to the style."
  },
  {
    "objectID": "11-ggplot2.html#categories-as-colors",
    "href": "11-ggplot2.html#categories-as-colors",
    "title": "11  ggplot2",
    "section": "11.9 Categories as colors",
    "text": "11.9 Categories as colors\nLet’s redefine p so we can test layers easilty:\n\np &lt;-  murders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_text(nudge_x = 0.05) + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\")\n\nHere is an exmaple of adding color:\n\np + geom_point(size = 3, color = \"blue\")\n\n\n\n\nBut if we want the color to relate to a variable, we need to include it in the map:\n\np + geom_point(aes(col = region), size = 3)"
  },
  {
    "objectID": "11-ggplot2.html#annotation-shapes-and-adjustments",
    "href": "11-ggplot2.html#annotation-shapes-and-adjustments",
    "title": "11  ggplot2",
    "section": "11.10 Annotation, shapes, and adjustments",
    "text": "11.10 Annotation, shapes, and adjustments\nWe want to add a line with intercept the us rate. So lets comput that\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt; \n  pull(rate)\n\nNow we can use the geom_abline function.\n\np + geom_point(aes(col = region), size = 3) + \n  geom_abline(intercept = log10(r))\n\n\n\n\nWe are very close to the goal. Let’s redefine p so we can easily add the finishing touches:\n\np &lt;- p + geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col=region), size = 3)  \n\nFor example, this is how we change the name of the legend:\n\np &lt;- p + scale_color_discrete(name = \"Region\") \np"
  },
  {
    "objectID": "11-ggplot2.html#sec-add-on-packages",
    "href": "11-ggplot2.html#sec-add-on-packages",
    "title": "11  ggplot2",
    "section": "11.11 Add-on packages",
    "text": "11.11 Add-on packages\nThe dslabs package can define the look used in the textbook:\n\nds_theme_set()\n\nMany other themes are added by the package ggthemes. Among those are the theme_economist theme that we used. After installing the package, you can change the style by adding a layer like this:\n\nlibrary(ggthemes)\np + theme_economist()\n\n\n\n\nYou can see how some of the other themes look by simply changing the function. For instance, you might try the theme_fivethirtyeight() theme instead.\n\nlibrary(ggthemes)\np + theme_fivethirtyeight()\n\n\n\n\nAnd if you want to ruin the plot, give it the excel theme:\n\np + theme_excel()\n\n\n\n\nFor more fun themes:\n\nlibrary(ThemePark)\np + theme_starwars()\n\n\n\n\n\np + theme_zelda()"
  },
  {
    "objectID": "11-ggplot2.html#putting-it-all-together",
    "href": "11-ggplot2.html#putting-it-all-together",
    "title": "11  ggplot2",
    "section": "11.12 Putting it all together",
    "text": "11.12 Putting it all together\nNow that we are done testing, we can write one piece of code that produces our desired plot from scratch.\n\nlibrary(ggthemes)\nlibrary(ggrepel)\n\nr &lt;- murders |&gt; \n  summarize(rate = sum(total) /  sum(population) * 10^6) |&gt;\n  pull(rate)\n\nmurders |&gt; ggplot(aes(population/10^6, total, label = abb)) +   \n  geom_abline(intercept = log10(r), lty = 2, color = \"darkgrey\") +\n  geom_point(aes(col = region), size = 3) +\n  geom_text_repel() + \n  scale_x_log10() +\n  scale_y_log10() +\n  xlab(\"Populations in millions (log scale)\") + \n  ylab(\"Total number of murders (log scale)\") +\n  ggtitle(\"US Gun Murders in 2010\") + \n  scale_color_discrete(name = \"Region\") +\n  theme_economist()"
  },
  {
    "objectID": "11-ggplot2.html#grids-of-plots",
    "href": "11-ggplot2.html#grids-of-plots",
    "title": "11  ggplot2",
    "section": "11.13 Grids of plots",
    "text": "11.13 Grids of plots\nThere are often reasons to graph plots next to each other. The gridExtra package permits us to do that:\n\nlibrary(gridExtra)\np1 &lt;- murders |&gt; ggplot(aes(log10(population))) + geom_histogram()\np2 &lt;- murders |&gt; ggplot(aes(log10(population), log10(total))) + geom_point()\ngrid.arrange(p1, p2, ncol = 2)"
  },
  {
    "objectID": "11-ggplot2.html#sec-other-geometries",
    "href": "11-ggplot2.html#sec-other-geometries",
    "title": "11  ggplot2",
    "section": "11.14 ggplot2 geometries",
    "text": "11.14 ggplot2 geometries\nWe previously introduced the ggplot2 package for data visualization. Here we demonstrate how to generate plots related to distributions, specifically the plots shown earlier in this chapter.\n\n11.14.1 Barplots\nTo generate a barplot we can use the geom_bar geometry. The default is to count the number of each category and draw a bar. Here is the plot for the regions of the US.\n\nmurders |&gt; ggplot(aes(region)) + geom_bar()\n\n\n\n\nWe often already have a table with a distribution that we want to present as a barplot. Here is an example of such a table:\n\ntab &lt;- murders |&gt; \n  count(region) |&gt; \n  mutate(proportion = n/sum(n))\ntab\n\n         region  n proportion\n1     Northeast  9  0.1764706\n2         South 17  0.3333333\n3 North Central 12  0.2352941\n4          West 13  0.2549020\n\n\nWe no longer want geom_bar to count, but rather just plot a bar to the height provided by the proportion variable. For this we need to provide x (the categories) and y (the values) and use the stat=\"identity\" option.\n\ntab |&gt; ggplot(aes(region, proportion)) + geom_bar(stat = \"identity\")\n\n\n\n\n\n\n11.14.2 Histograms\nTo generate histograms we use geom_histogram. By looking at the help file for this function, we learn that the only required argument is x, the variable for which we will construct a histogram. We dropped the x because we know it is the first argument. The code looks like this:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram()\n\nIf we run the code above, it gives us a message:\n\nstat_bin() using bins = 30. Pick better value with binwidth.\n\nWe previously used a bin size of 1 inch, so the code looks like this:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) + \n  geom_histogram(binwidth = 1)\n\nFinally, if for aesthetic reasons we want to add color, we use the arguments described in the help file. We also add labels and a title:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt; \n  ggplot(aes(height)) +\n  geom_histogram(binwidth = 1, fill = \"blue\", col = \"black\") +\n  xlab(\"Female heights in inches\") + \n  ggtitle(\"Histogram\")\n\n\n\n\n\n\n11.14.3 Density plots\nTo create a smooth density, we use the geom_density. To make a smooth density plot with the data previously shown as a histogram we can use this code:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density()\n\nTo fill in with color, we can use the fill argument.\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill = \"blue\")\n\n\n\n\nTo change the smoothness of the density, we use the adjust argument to multiply the default value by that adjust. For example, if we want the bandwidth to be twice as big we use:\n\nheights |&gt; \n  filter(sex == \"Female\") |&gt;\n  ggplot(aes(height)) +\n  geom_density(fill = \"blue\", adjust = 2)\n\n\n\n11.14.4 Boxplots\nThe geometry for boxplot is geom_boxplot. As discussed, boxplots are useful for comparing distributions. For example, below are the previously shown heights for women, but compared to men. For this geometry, we need arguments x as the categories, and y as the values.\n\n\n\n\n\n\n\n11.14.5 QQ-plots\nFor qq-plots we use the geom_qq geometry. From the help file, we learn that we need to specify the sample (we will learn about samples in a later chapter). Here is the qqplot for men heights.\n\nheights |&gt; filter(sex == \"Male\") |&gt;\n  ggplot(aes(sample = height)) +\n  geom_qq()\n\n\n\n\nBy default, the sample variable is compared to a normal distribution with average 0 and standard deviation 1. To change this, we use the dparams arguments based on the help file. Adding an identity line is as simple as assigning another layer. For straight lines, we use the geom_abline function. The default line is the identity line (slope = 1, intercept = 0).\n\nparams &lt;- heights |&gt; filter(sex==\"Male\") |&gt;\n  summarize(mean = mean(height), sd = sd(height))\n\nheights |&gt; filter(sex==\"Male\") |&gt;\n  ggplot(aes(sample = height)) +\n  geom_qq(dparams = params) +\n  geom_abline()\n\nAnother option here is to scale the data first and then make a qqplot against the standard normal.\n\nheights |&gt; \n  filter(sex==\"Male\") |&gt;\n  ggplot(aes(sample = scale(height))) + \n  geom_qq() +\n  geom_abline()\n\n\n\n11.14.6 Images\nWe introduce the two geometries used to create images: geom_tile and geom_raster. They behave similarly; to see how they differ, please consult the help file. To create an image in ggplot2 we need a data frame with the x and y coordinates as well as the values associated with each of these. Here is a data frame.\n\nx &lt;- expand.grid(x = 1:12, y = 1:10) |&gt; \n  mutate(z = 1:120) \n\nNote that this is the tidy version of a matrix, matrix(1:120, 12, 10). To plot the image we use the following code:\n\nx |&gt; ggplot(aes(x, y, fill = z)) + \n  geom_raster()\n\nWith these images you will often want to change the color scale. This can be done through the scale_fill_gradientn layer.\n\nx |&gt; ggplot(aes(x, y, fill = z)) + \n  geom_raster() + \n  scale_fill_gradientn(colors =  terrain.colors(10, alpha = 1))"
  },
  {
    "objectID": "11-ggplot2.html#exercises",
    "href": "11-ggplot2.html#exercises",
    "title": "11  ggplot2",
    "section": "11.15 Exercises",
    "text": "11.15 Exercises\n\nCreate a ggplot object using the pipe to assign the heights data to a ggplot object. Assign height to the x values through the aes function.\nAdd a layer to actually make the histogram. Use the object created in the previous exercise and the geom_histogram function to make the histogram.\nNote that when we run the code in the previous exercise we get the warning: stat_bin() using bins = 30. Pick better value with binwidth. Use the binwidth argument to change the histogram made in the previous exercise to use bins of size 1 inch.\nInstead of a histogram, we are going to make a smooth density plot. In this case we will not make an object, but instead render the plot with one line of code. Change the geometry in the code previously used to make a smooth density instead of a histogram.\nNow we are going to make a density plot for males and females separately. We can do this using the group argument. We assign groups via the aesthetic mapping as each point needs to a group before making the calculations needed to estimate a density.\nWe can also assign groups through the color argument. This has the added benefit that it uses color to distinguish the groups. Change the code above to use color.\nWe can also assign groups through the fill argument. This has the added benefit that it uses colors to distinguish the groups, like this:\n\n\nheights |&gt; \n  ggplot(aes(height, fill = sex)) + \n  geom_density() \n\nHowever, here the second density is drawn over the other. We can make the curves more visible by using alpha blending to add transparency. Set the alpha parameter to 0.2 in the geom_density function to make this change.\n\nUsing the pipe |&gt;, create an object p with the heights dataset as the data.\nNow we are going to add a layer and the corresponding aesthetic mappings. For the murders data we plotted total murders versus population sizes. Explore the murders data frame to remind yourself what are the names for these two variables and select the correct answer.\n\n\nstate and abb.\ntotal_murders and population_size.\ntotal and population.\nmurders and size.\n\n\nTo create a scatterplot we add a layer with geom_point. The aesthetic mappings require us to define the x-axis and y-axis variables, respectively. So the code looks like this:\n\n\nmurders |&gt; ggplot(aes(x = , y = )) +\n  geom_point()\n\nexcept we have to define the two variables x and y. Fill this out with the correct variable names.\n\nNote that if we don’t use argument names, we can obtain the same plot by making sure we enter the variable names in the right order like this:\n\n\nmurders |&gt; ggplot(aes(population, total)) +\n  geom_point()\n\nRemake the plot but now with total in the x-axis and population in the y-axis.\n\nIf instead of points we want to add text, we can use the geom_text() or geom_label() geometries. The following code\n\n\nmurders |&gt; ggplot(aes(population, total)) + geom_label()\n\nwill give us the error message: Error: geom_label requires the following missing aesthetics: label\nWhy is this?\n\nWe need to map a character to each point through the label argument in aes.\nWe need to let geom_label know what character to use in the plot.\nThe geom_label geometry does not require x-axis and y-axis values.\ngeom_label is not a ggplot2 command.\n\n\nRewrite the code above to use abbreviation as the label through aes\nChange the color of the labels to blue. How will we do this?\n\n\nAdding a column called blue to murders.\nBecause each label needs a different color we map the colors through aes.\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels blue.\nNow suppose we want to use color to represent the different regions. In this case which of the following is most appropriate:\n\n\nAdding a column called color to murders with the color we want to use.\nBecause each label needs a different color we map the colors through the color argument of aes .\nUse the color argument in ggplot.\nBecause we want all colors to be blue, we do not need to map colors, just use the color argument in geom_label.\n\n\nRewrite the code above to make the labels’ color be determined by the state’s region.\nNow we are going to change the x-axis to a log scale to account for the fact the distribution of population is skewed. Let’s start by defining an object p holding the plot we have made up to now\n\n\np &lt;- murders |&gt; \n  ggplot(aes(population, total, label = abb, color = region)) +\n  geom_label() \n\nTo change the y-axis to a log scale we learned about the scale_x_log10() function. Add this layer to the object p to change the scale and render the plot.\n\nRepeat the previous exercise but now change both axes to be in the log scale.\nNow edit the code above to add the title “Gun murder data” to the plot. Hint: use the ggtitle function."
  },
  {
    "objectID": "12-dataviz-principles.html#encoding-data-using-visual-cues",
    "href": "12-dataviz-principles.html#encoding-data-using-visual-cues",
    "title": "12  Data visualization principles",
    "section": "12.1 Encoding data using visual cues",
    "text": "12.1 Encoding data using visual cues\nWe start by describing some principles for encoding data. There are several approaches at our disposal including position, aligned lengths, angles, area, brightness, and color hue.\nTo illustrate how some of these strategies compare, let’s suppose we want to report the results from two hypothetical polls regarding browser preference taken in 2000 and then 2015. For each year, we are simply comparing five quantities – the five percentages. A widely used graphical representation of percentages, popularized by Microsoft Excel, is the pie chart:\n\n\n\n\n\nHere we are representing quantities with both areas and angles, since both the angle and area of each pie slice are proportional to the quantity the slice represents. This turns out to be a sub-optimal choice since, as demonstrated by perception studies, humans are not good at precisely quantifying angles and are even worse when area is the only available visual cue. The donut chart is an example of a plot that uses only area:\n\n\n\n\n\nTo see how hard it is to quantify angles and area, note that the rankings and all the percentages in the plots above changed from 2000 to 2015. Can you determine the actual percentages and rank the browsers’ popularity? Can you see how the percentages changed from 2000 to 2015? It is not easy to tell from the plot. In this case, simply showing the numbers is not only clearer, but would also save on printing costs if printing a paper copy:\n\n\n\n\n\nBrowser\n2000\n2015\n\n\n\n\nOpera\n3\n2\n\n\nSafari\n21\n22\n\n\nFirefox\n23\n21\n\n\nChrome\n26\n29\n\n\nIE\n28\n27\n\n\n\n\n\n\n\nThe preferred way to plot these quantities is to use length and position as visual cues, since humans are much better at judging linear measures. The barplot uses this approach by using bars of length proportional to the quantities of interest. By adding horizontal lines at strategically chosen values, in this case at every multiple of 10, we ease the visual burden of quantifying through the position of the top of the bars. Compare and contrast the information we can extract from the two figures.\n\n\n\n\n\nNotice how much easier it is to see the differences in the barplot. In fact, we can now determine the actual percentages by following a horizontal line to the x-axis.\nIf for some reason you need to make a pie chart, label each pie slice with its respective percentage so viewers do not have to infer them from the angles or area:\n\n\n\n\n\nIn general, when displaying quantities, position and length are preferred over angles and/or area. Brightness and color are even harder to quantify than angles. But, as we will see later, they are sometimes useful when more than two dimensions must be displayed at once."
  },
  {
    "objectID": "12-dataviz-principles.html#know-when-to-include-0",
    "href": "12-dataviz-principles.html#know-when-to-include-0",
    "title": "12  Data visualization principles",
    "section": "12.2 Know when to include 0",
    "text": "12.2 Know when to include 0\nWhen using barplots, it is misinformative not to start the bars at 0. This is because, by using a barplot, we are implying the length is proportional to the quantities being displayed. By avoiding 0, relatively small differences can be made to look much bigger than they actually are. This approach is often used by politicians or media organizations trying to exaggerate a difference.\n\n\n\n\n\n(Source: Fox News, via Media Matters1.)\nFrom the plot above, it appears that apprehensions have almost tripled when, in fact, they have only increased by about 16%. Starting the graph at 0 illustrates this clearly:\n\n\n\n\n\nHere is another example:\n\n\n\n\n\n(Source: Fox News, via Flowing Data2.)\nThis plot makes a 13% increase look like a five fold change. Here is the appropriate plot:\n\n\n\n\n\nFinally, here is an extreme example that makes a very small difference of under 2% look like a 10-100 fold change:\n\n\n\n\n\n(Source: Venezolana de Televisión via Pakistan Today3 and Diego Mariano.)\nHere is the appropriate plot:\n\n\n\n\n\nWhen using position rather than length, it is then not necessary to include 0. This is particularly the case when we want to compare differences between groups relative to the within-group variability. Here is an illustrative example showing country average life expectancy stratified across continents in 2012:\n\n\n\n\n\nNote that in the plot on the left, which includes 0, the space between 0 and 43 adds no information and makes it harder to compare the between and within group variability."
  },
  {
    "objectID": "12-dataviz-principles.html#do-not-distort-quantities",
    "href": "12-dataviz-principles.html#do-not-distort-quantities",
    "title": "12  Data visualization principles",
    "section": "12.3 Do not distort quantities",
    "text": "12.3 Do not distort quantities\nDuring President Barack Obama’s 2011 State of the Union Address, the following chart was used to compare the US GDP to the GDP of four competing nations:\n\n\n\n\n\n(Source: The 2011 State of the Union Address4)\nJudging by the area of the circles, the US appears to have an economy over five times larger than China’s and over 30 times larger than France’s. However, if we look at the actual numbers, we see that this is not the case. The actual ratios are 2.6 and 5.8 times bigger than China and France, respectively. The reason for this distortion is that the radius, rather than the area, was made to be proportional to the quantity, which implies that the proportion between the areas is squared: 2.6 turns into 6.5 and 5.8 turns into 34.1. Here is a comparison of the circles we get if we make the value proportional to the radius and to the area:\n\n\n\n\n\nNot surprisingly, ggplot2 defaults to using area rather than radius. Of course, in this case, we really should not be using area at all since we can use position and length:"
  },
  {
    "objectID": "12-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "href": "12-dataviz-principles.html#order-categories-by-a-meaningful-value",
    "title": "12  Data visualization principles",
    "section": "12.4 Order categories by a meaningful value",
    "text": "12.4 Order categories by a meaningful value\nWhen one of the axes is used to show categories, as is done in barplots, the default ggplot2 behavior is to order the categories alphabetically when they are defined by character strings. If they are defined by factors, they are ordered by the factor levels. We rarely want to use alphabetical order. Instead, we should order by a meaningful quantity. In all the cases above, the barplots were ordered by the values being displayed. The exception was the graph showing barplots comparing browsers. In this case, we kept the order the same across the barplots to ease the comparison. Specifically, instead of ordering the browsers separately in the two years, we ordered both years by the average value of 2000 and 2015.\n\n\n\n\n\nWe can make the second plot like this:\n\nmurders |&gt; mutate(murder_rate = total / population * 100000) |&gt;\n  mutate(state = reorder(state, murder_rate)) |&gt;\n  ggplot(aes(state, murder_rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() +\n  theme(axis.text.y = element_text(size = 6)) +\n  xlab(\"\")\n\nThe reorder function lets us reorder groups as well. Earlier we saw an example related to income distributions across regions. Here are the two versions plotted against each other:\n\n\n\n\n\nThe first orders the regions alphabetically, while the second orders them by the group’s median."
  },
  {
    "objectID": "12-dataviz-principles.html#show-the-data",
    "href": "12-dataviz-principles.html#show-the-data",
    "title": "12  Data visualization principles",
    "section": "12.5 Show the data",
    "text": "12.5 Show the data\nTo motivate our first principle, “show the data”, we go back to our artificial example of describing heights to ET, an extraterrestrial. This time let’s assume ET is interested in the difference in heights between males and females. A commonly seen plot used for comparisons between groups, popularized by software such as Microsoft Excel, is the dynamite plot, which shows the average and standard errors (standard errors are defined in a later chapter, but do not confuse them with the standard deviation of the data). The plot looks like this:\n\n\n\n\n\nThe average of each group is represented by the top of each bar and the antennae extend out from the average to the average plus two standard errors. If all ET receives is this plot, he will have little information on what to expect if he meets a group of human males and females. The bars go to 0: does this mean there are tiny humans measuring less than one foot? Are all males taller than the tallest females? Is there a range of heights? ET can’t answer these questions since we have provided almost no information on the height distribution.\nThis brings us to our first principle: show the data. This simple ggplot2 code already generates a more informative plot than the barplot by simply showing all the data points:\n\nheights |&gt; \n  ggplot(aes(sex, height)) + \n  geom_point() \n\n\n\n\nFor example, this plot gives us an idea of the range of the data. However, this plot has limitations as well, since we can’t really see all the 238 and 812 points plotted for females and males, respectively, and many points are plotted on top of each other. As we have previously described, visualizing the distribution is much more informative. But before doing this, we point out two ways we can improve a plot showing all the points.\nThe first is to add jitter, which adds a small random shift to each point. In this case, adding horizontal jitter does not alter the interpretation, since the point heights do not change, but we minimize the number of points that fall on top of each other and, therefore, get a better visual sense of how the data is distributed. A second improvement comes from using alpha blending: making the points somewhat transparent. The more points fall on top of each other, the darker the plot, which also helps us get a sense of how the points are distributed. Here is the same plot with jitter and alpha blending:\n\nheights |&gt; \n  ggplot(aes(sex, height)) +\n  geom_jitter(width = 0.1, alpha = 0.2) \n\n\n\n\nNow we start getting a sense that, on average, males are taller than females. We also note dark horizontal bands of points, demonstrating that many report values that are rounded to the nearest integer."
  },
  {
    "objectID": "12-dataviz-principles.html#ease-comparisons",
    "href": "12-dataviz-principles.html#ease-comparisons",
    "title": "12  Data visualization principles",
    "section": "12.6 Ease comparisons",
    "text": "12.6 Ease comparisons\n\n12.6.1 Use common axes\nSince there are so many points, it is more effective to show distributions rather than individual points. We therefore show histograms for each group:\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nHowever, from this plot it is not immediately obvious that males are, on average, taller than females. We have to look carefully to notice that the x-axis has a higher range of values in the male histogram. An important principle here is to keep the axes the same when comparing data across two plots. Below we see how the comparison becomes easier:\n\n\n\n\n\n\n\n12.6.2 Align plots vertically to see horizontal changes and horizontally to see vertical changes\nIn these histograms, the visual cue related to decreases or increases in height are shifts to the left or right, respectively: horizontal changes. Aligning the plots vertically helps us see this change when the axes are fixed:\n\n\n\n\n\n\nheights |&gt; \n  ggplot(aes(height, ..density..)) +\n  geom_histogram(binwidth = 1, color=\"black\") +\n  facet_grid(sex~.)\n\nThis plot makes it much easier to notice that men are, on average, taller.\nIf , we want the more compact summary provided by boxplots, we then align them horizontally since, by default, boxplots move up and down with changes in height. Following our show the data principle, we then overlay all the data points:\n\n\n\n\n\n\n heights |&gt; \n  ggplot(aes(sex, height)) + \n  geom_boxplot(coef=3) + \n  geom_jitter(width = 0.1, alpha = 0.2) +\n  ylab(\"Height in inches\")\n\nNow contrast and compare these three plots, based on exactly the same data:\n\n\n\n\n\nNotice how much more we learn from the two plots on the right. Barplots are useful for showing one number, but not very useful when we want to describe distributions.\n\n\n12.6.3 Consider transformations\nWe have motivated the use of the log transformation in cases where the changes are multiplicative. Population size was an example in which we found a log transformation to yield a more informative transformation.\nThe combination of an incorrectly chosen barplot and a failure to use a log transformation when one is merited can be particularly distorting. As an example, consider this barplot showing the average population sizes for each continent in 2015:\n\n\n\n\n\nFrom this plot, one would conclude that countries in Asia are much more populous than in other continents. Following the show the data principle, we quickly notice that this is due to two very large countries, which we assume are India and China:\n\n\n\n\n\nUsing a log transformation here provides a much more informative plot. We compare the original barplot to a boxplot using the log scale transformation for the y-axis:\n\n\n\n\n\nWith the new plot, we realize that countries in Africa actually have a larger median population size than those in Asia.\nOther transformations you should consider are the logistic transformation (logit), useful to better see fold changes in odds, and the square root transformation (sqrt), useful for count data.\n\n\n12.6.4 Visual cues to be compared should be adjacent\nFor each continent, let’s compare income in 1970 versus 2010. When comparing income data across regions between 1970 and 2010, we made a figure similar to the one below, but this time we investigate continents rather than regions.\n\n\n\n\n\nThe default in ggplot2 is to order labels alphabetically so the labels with 1970 come before the labels with 2010, making the comparisons challenging because a continent’s distribution in 1970 is visually far from its distribution in 2010. It is much easier to make the comparison between 1970 and 2010 for each continent when the boxplots for that continent are next to each other:\n\n\n\n\n\n\n\n12.6.5 Use color\nThe comparison becomes even easier to make if we use color to denote the two things we want to compare:"
  },
  {
    "objectID": "12-dataviz-principles.html#think-of-the-color-blind",
    "href": "12-dataviz-principles.html#think-of-the-color-blind",
    "title": "12  Data visualization principles",
    "section": "12.7 Think of the color blind",
    "text": "12.7 Think of the color blind\nAbout 10% of the population is color blind. Unfortunately, the default colors used in ggplot2 are not optimal for this group. However, ggplot2 does make it easy to change the color palette used in the plots. An example of how we can use a color blind friendly palette is described here: http://www.cookbook-r.com/Graphs/Colors_(ggplot2)/#a-colorblind-friendly-palette:\n\ncolor_blind_friendly_cols &lt;- \n  c(\"#999999\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \n    \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\nHere are the colors\n\n\n\n\n\nThere are several resources that can help you select colors, for example this one: http://bconnelly.net/2013/10/creating-colorblind-friendly-figures/."
  },
  {
    "objectID": "12-dataviz-principles.html#plots-for-two-variables",
    "href": "12-dataviz-principles.html#plots-for-two-variables",
    "title": "12  Data visualization principles",
    "section": "12.8 Plots for two variables",
    "text": "12.8 Plots for two variables\nIn general, you should use scatterplots to visualize the relationship between two variables. In every single instance in which we have examined the relationship between two variables, including total murders versus population size, life expectancy versus fertility rates, and infant mortality versus income, we have used scatterplots. This is the plot we generally recommend. However, there are some exceptions and we describe two alternative plots here: the slope chart and the Bland-Altman plot.\n\n12.8.1 Slope charts\nOne exception where another type of plot may be more informative is when you are comparing variables of the same type, but at different time points and for a relatively small number of comparisons. For example, comparing life expectancy between 2010 and 2015. In this case, we might recommend a slope chart.\nThere is no geometry for slope charts in ggplot2, but we can construct one using geom_line. We need to do some tinkering to add labels. Below is an example comparing 2010 to 2015 for large western countries:\n\nwest &lt;- c(\"Western Europe\",\"Northern Europe\",\"Southern Europe\",\n          \"Northern America\",\"Australia and New Zealand\")\n\ndat &lt;- gapminder |&gt; \n  filter(year %in% c(2010, 2015) & region %in% west & \n           !is.na(life_expectancy) & population &gt; 10^7) \n\ndat |&gt;\n  mutate(location = ifelse(year == 2010, 1, 2), \n         location = ifelse(year == 2015 & \n                             country %in% c(\"United Kingdom\", \"Portugal\"),\n                           location+0.22, location),\n         hjust = ifelse(year == 2010, 1, 0)) |&gt;\n  mutate(year = as.factor(year)) |&gt;\n  ggplot(aes(year, life_expectancy, group = country)) +\n  geom_line(aes(color = country), show.legend = FALSE) +\n  geom_text(aes(x = location, label = country, hjust = hjust), \n            show.legend = FALSE) +\n  xlab(\"\") + ylab(\"Life Expectancy\")\n\n\n\n\nAn advantage of the slope chart is that it permits us to quickly get an idea of changes based on the slope of the lines. Although we are using angle as the visual cue, we also have position to determine the exact values. Comparing the improvements is a bit harder with a scatterplot:\n\n\n\n\n\nIn the scatterplot, we have followed the principle use common axes since we are comparing these before and after. However, if we have many points, slope charts stop being useful as it becomes hard to see all the lines.\n\n\n12.8.2 Bland-Altman plot\nSince we are primarily interested in the difference, it makes sense to dedicate one of our axes to it. The Bland-Altman plot, also known as the Tukey mean-difference plot and the MA-plot, shows the difference versus the average:\n\nlibrary(ggrepel)\ndat |&gt; \n  mutate(year = paste0(\"life_expectancy_\", year)) |&gt;\n  select(country, year, life_expectancy) |&gt; \n  pivot_wider(names_from = \"year\", values_from=\"life_expectancy\") |&gt; \n  mutate(average = (life_expectancy_2015 + life_expectancy_2010)/2,\n         difference = life_expectancy_2015 - life_expectancy_2010) |&gt;\n  ggplot(aes(average, difference, label = country)) + \n  geom_point() +\n  geom_text_repel() +\n  geom_abline(lty = 2) +\n  xlab(\"Average of 2010 and 2015\") + \n  ylab(\"Difference between 2015 and 2010\")\n\n\n\n\nHere, by simply looking at the y-axis, we quickly see which countries have shown the most improvement. We also get an idea of the overall value from the x-axis."
  },
  {
    "objectID": "12-dataviz-principles.html#encoding-a-third-variable",
    "href": "12-dataviz-principles.html#encoding-a-third-variable",
    "title": "12  Data visualization principles",
    "section": "12.9 Encoding a third variable",
    "text": "12.9 Encoding a third variable\nAn earlier scatterplot showed the relationship between infant survival and average income. Below is a version of this plot that encodes three variables: OPEC membership, region, and population.\n\n\n\n\n\nWe encode categorical variables with color and shape. These shapes can be controlled with shape argument. Below are the shapes available for use in R. For the last five, the color goes inside.\n\n\n\n\n\nFor continuous variables, we can use color, intensity, or size. We now show an example of how we do this with a case study.\nWhen selecting colors to quantify a numeric variable, we choose between two options: sequential and diverging. Sequential colors are suited for data that goes from high to low. High values are clearly distinguished from low values. Here are some examples offered by the package RColorBrewer:\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(type=\"seq\")\n\n\n\n\n\n\nDiverging colors are used to represent values that diverge from a center. We put equal emphasis on both ends of the data range: higher than the center and lower than the center. An example of when we would use a divergent pattern would be if we were to show height in standard deviations away from the average. Here are some examples of divergent patterns:\n\nlibrary(RColorBrewer)\ndisplay.brewer.all(type=\"div\")"
  },
  {
    "objectID": "12-dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "href": "12-dataviz-principles.html#avoid-pseudo-three-dimensional-plots",
    "title": "12  Data visualization principles",
    "section": "12.10 Avoid pseudo-three-dimensional plots",
    "text": "12.10 Avoid pseudo-three-dimensional plots\nThe figure below, taken from the scientific literature5, shows three variables: dose, drug type and survival. Although your screen/book page is flat and two-dimensional, the plot tries to imitate three dimensions and assigned a dimension to each variable.\n\n\n\n\n\n(Image courtesy of Karl Broman)\nHumans are not good at seeing in three dimensions (which explains why it is hard to parallel park) and our limitation is even worse with regard to pseudo-three-dimensions. To see this, try to determine the values of the survival variable in the plot above. Can you tell when the purple ribbon intersects the red one? This is an example in which we can easily use color to represent the categorical variable instead of using a pseudo-3D:\n\n\n\n\n\nNotice how much easier it is to determine the survival values.\nPseudo-3D is sometimes used completely gratuitously: plots are made to look 3D even when the 3rd dimension does not represent a quantity. This only adds confusion and makes it harder to relay your message. Here are two examples:\n\n\n\n\n\n\n\n\n(Images courtesy of Karl Broman)"
  },
  {
    "objectID": "12-dataviz-principles.html#avoid-too-many-significant-digits",
    "href": "12-dataviz-principles.html#avoid-too-many-significant-digits",
    "title": "12  Data visualization principles",
    "section": "12.11 Avoid too many significant digits",
    "text": "12.11 Avoid too many significant digits\nBy default, statistical software like R returns many significant digits. The default behavior in R is to show 7 significant digits. That many digits often adds no information and the added visual clutter can make it hard for the viewer to understand the message. As an example, here are the per 10,000 disease rates, computed from totals and population in R, for California across the five decades:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.8826320\n18.3397861\n0.8266512\n\n\nCalifornia\n1950\n13.9124205\n4.7467350\n1.9742639\n\n\nCalifornia\n1960\n14.1386471\nNA\n0.2640419\n\n\nCalifornia\n1970\n0.9767889\nNA\nNA\n\n\nCalifornia\n1980\n0.3743467\n0.0515466\nNA\n\n\n\n\n\n\n\nWe are reporting precision up to 0.00001 cases per 10,000, a very small value in the context of the changes that are occurring across the dates. In this case, two significant figures is more than enough and clearly makes the point that rates are decreasing:\n\n\n\n\n\nstate\nyear\nMeasles\nPertussis\nPolio\n\n\n\n\nCalifornia\n1940\n37.9\n18.3\n0.8\n\n\nCalifornia\n1950\n13.9\n4.7\n2.0\n\n\nCalifornia\n1960\n14.1\nNA\n0.3\n\n\nCalifornia\n1970\n1.0\nNA\nNA\n\n\nCalifornia\n1980\n0.4\n0.1\nNA\n\n\n\n\n\n\n\nUseful ways to change the number of significant digits or to round numbers are signif and round. You can define the number of significant digits globally by setting options like this: options(digits = 3).\nAnother principle related to displaying tables is to place values being compared on columns rather than rows. Note that our table above is easier to read than this one:\n\n\n\n\n\nstate\ndisease\n1940\n1950\n1960\n1970\n1980\n\n\n\n\nCalifornia\nMeasles\n37.9\n13.9\n14.1\n1\n0.4\n\n\nCalifornia\nPertussis\n18.3\n4.7\nNA\nNA\n0.1\n\n\nCalifornia\nPolio\n0.8\n2.0\n0.3\nNA\nNA"
  },
  {
    "objectID": "12-dataviz-principles.html#know-your-audience",
    "href": "12-dataviz-principles.html#know-your-audience",
    "title": "12  Data visualization principles",
    "section": "12.12 Know your audience",
    "text": "12.12 Know your audience\nGraphs can be used for 1) our own exploratory data analysis, 2) to convey a message to experts, or 3) to help tell a story to a general audience. Make sure that the intended audience understands each element of the plot.\nAs a simple example, consider that for your own exploration it may be more useful to log-transform data and then plot it. However, for a general audience that is unfamiliar with converting logged values back to the original measurements, using a log-scale for the axis instead of log-transformed values will be much easier to digest."
  },
  {
    "objectID": "12-dataviz-principles.html#exercises",
    "href": "12-dataviz-principles.html#exercises",
    "title": "12  Data visualization principles",
    "section": "12.13 Exercises",
    "text": "12.13 Exercises\nFor these exercises, we will be using the vaccines data in the dslabs package:\n\nlibrary(dslabs)\n\n1. Pie charts are appropriate:\n\nWhen we want to display percentages.\nWhen ggplot2 is not available.\nWhen I am in a bakery.\nNever. Barplots and tables are always better.\n\n2. What is the problem with the plot below:\n\n\n\n\n\n\nThe values are wrong. The final vote was 306 to 232.\nThe axis does not start at 0. Judging by the length, it appears Trump received 3 times as many votes when, in fact, it was about 30% more.\nThe colors should be the same.\nPercentages should be shown as a pie chart.\n\n3. Take a look at the following two plots. They show the same information: 1928 rates of measles across the 50 states.\n\n\n\n\n\nWhich plot is easier to read if you are interested in determining which are the best and worst states in terms of rates, and why?\n\nThey provide the same information, so they are both equally as good.\nThe plot on the right is better because it orders the states alphabetically.\nThe plot on the right is better because alphabetical order has nothing to do with the disease and by ordering according to actual rate, we quickly see the states with most and least rates.\nBoth plots should be a pie chart.\n\n4. To make the plot on the left, we have to reorder the levels of the states’ variables.\n\ndat &lt;- us_contagious_diseases |&gt;  \n  filter(year == 1967 & disease==\"Measles\" & !is.na(population)) |&gt;\n  mutate(rate = count / population * 10000 * 52 / weeks_reporting)\n\nNote what happens when we make a barplot:\n\ndat |&gt; ggplot(aes(state, rate)) +\n  geom_bar(stat=\"identity\") +\n  coord_flip() \n\n\n\n\nDefine these objects:\n\nstate &lt;- dat$state\nrate &lt;- dat$count/dat$population*10000*52/dat$weeks_reporting\n\nRedefine the state object so that the levels are re-ordered. Print the new object state and its levels so you can see that the vector is not re-ordered by the levels.\n5. Now with one line of code, define the dat table as done above, but change the use mutate to create a rate variable and re-order the state variable so that the levels are re-ordered by this variable. Then make a barplot using the code above, but for this new dat.\n6. Say we are interested in comparing gun homicide rates across regions of the US. We see this plot:\n\nlibrary(dslabs)\nmurders |&gt; mutate(rate = total/population*100000) |&gt;\ngroup_by(region) |&gt;\nsummarize(avg = mean(rate)) |&gt;\nmutate(region = factor(region)) |&gt;\nggplot(aes(region, avg)) +\ngeom_bar(stat=\"identity\") +\nylab(\"Murder Rate Average\")\n\n\n\n\nand decide to move to a state in the western region. What is the main problem with this interpretation?\n\nThe categories are ordered alphabetically.\nThe graph does not show standarad errors.\nIt does not show all the data. We do not see the variability within a region and it’s possible that the safest states are not in the West.\nThe Northeast has the lowest average.\n\n7. Make a boxplot of the murder rates defined as\n\nmurders |&gt; mutate(rate = total/population*100000)\n\nby region, showing all the points and ordering the regions by their median rate.\n8. The plots below show three continuous variables.\n\n\n\n\n\nThe line \\(x=2\\) appears to separate the points. But it is actually not the case, which we can see by plotting the data in a couple of two-dimensional points.\n\n\n\n\n\nWhy is this happening?\n\nHumans are not good at reading pseudo-3D plots.\nThere must be an error in the code.\nThe colors confuse us.\nScatterplots should not be used to compare two variables when we have access to 3."
  },
  {
    "objectID": "12-dataviz-principles.html#footnotes",
    "href": "12-dataviz-principles.html#footnotes",
    "title": "12  Data visualization principles",
    "section": "",
    "text": "http://mediamatters.org/blog/2013/04/05/fox-news-newest-dishonest-chart-immigration-enf/193507↩︎\nhttp://flowingdata.com/2012/08/06/fox-news-continues-charting-excellence/↩︎\nhttps://www.pakistantoday.com.pk/2018/05/18/whats-at-stake-in-venezuelan-presidential-vote↩︎\nhttps://www.youtube.com/watch?v=kl2g40GoRxg↩︎\nhttps://projecteuclid.org/download/pdf_1/euclid.ss/1177010488↩︎"
  },
  {
    "objectID": "13-wrangling.html#reshaping-data",
    "href": "13-wrangling.html#reshaping-data",
    "title": "13  Wrangling",
    "section": "13.1 Reshaping data",
    "text": "13.1 Reshaping data\n\nlibrary(tidyverse) \nlibrary(dslabs)\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\nfilename &lt;- file.path(path, \"fertility-two-countries-example.csv\")\nwide_data &lt;- read_csv(filename)\n\n\n13.1.1 pivot_longer\n\nwide_data |&gt; pivot_longer(`1960`:`2015`)\n\n# A tibble: 112 × 3\n   country name  value\n   &lt;chr&gt;   &lt;chr&gt; &lt;dbl&gt;\n 1 Germany 1960   2.41\n 2 Germany 1961   2.44\n 3 Germany 1962   2.47\n 4 Germany 1963   2.49\n 5 Germany 1964   2.49\n 6 Germany 1965   2.48\n 7 Germany 1966   2.44\n 8 Germany 1967   2.37\n 9 Germany 1968   2.28\n10 Germany 1969   2.17\n# ℹ 102 more rows\n\n\nWe can also use the pipe like this:\n\nnew_tidy_data &lt;- wide_data |&gt; \n  pivot_longer(`1960`:`2015`, names_to = \"year\", values_to = \"fertility\")\nhead(new_tidy_data)\n\n# A tibble: 6 × 3\n  country year  fertility\n  &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;\n1 Germany 1960       2.41\n2 Germany 1961       2.44\n3 Germany 1962       2.47\n4 Germany 1963       2.49\n5 Germany 1964       2.49\n6 Germany 1965       2.48\n\n\nUsually its easier to name the columns not to be pivoted.\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\")\n\nThe new_tidy_data object looks like the original tidy_data we defined this way\n\ntidy_data &lt;- gapminder |&gt; \n  filter(country %in% c(\"South Korea\", \"Germany\") & !is.na(fertility)) |&gt;\n  select(country, year, fertility)\n\nwith just one minor difference. Can you spot it? Look at the data type of the year column:\n\nclass(tidy_data$year)\n\n[1] \"integer\"\n\nclass(new_tidy_data$year)\n\n[1] \"character\"\n\n\nThe pivot_longer function assumes that column names are characters. So we need a bit more wrangling before we are ready to make a plot. We need to convert the year column to be numbers:\n\nnew_tidy_data &lt;- wide_data |&gt;\n  pivot_longer(-country, names_to = \"year\", values_to = \"fertility\") |&gt;\n  mutate(year = as.integer(year))\n\nNow that the data is tidy, we can use this relatively simple ggplot code:\n\nnew_tidy_data |&gt; ggplot(aes(year, fertility, color = country)) + \n  geom_point()\n\n\n\n13.1.2 pivot_wider\n\nnew_wide_data &lt;- new_tidy_data |&gt; \n  pivot_wider(names_from = year, values_from = fertility)\nselect(new_wide_data, country, `1960`:`1967`)\n\n# A tibble: 2 × 9\n  country     `1960` `1961` `1962` `1963` `1964` `1965` `1966` `1967`\n  &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 Germany       2.41   2.44   2.47   2.49   2.49   2.48   2.44   2.37\n2 South Korea   6.16   5.99   5.79   5.57   5.36   5.16   4.99   4.85\n\n\nSimilar to pivot_wider, names_from and values_from default to name and value.\n\n\n13.1.3 separate\nThe data wrangling shown above was simple compared to what is usually required. In our example spreadsheet files, we include an illustration that is slightly more complicated. It contains two variables: life expectancy and fertility. However, the way it is stored is not tidy and, as we will explain, not optimal.\n\npath &lt;- system.file(\"extdata\", package = \"dslabs\")\n\nfilename &lt;- \"life-expectancy-and-fertility-two-countries-example.csv\"\nfilename &lt;-  file.path(path, filename)\n\nraw_dat &lt;- read_csv(filename)\nselect(raw_dat, 1:5)\n\n# A tibble: 2 × 5\n  country     `1960_fertility` `1960_life_expectancy` `1961_fertility`\n  &lt;chr&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;            &lt;dbl&gt;\n1 Germany                 2.41                   69.3             2.44\n2 South Korea             6.16                   53.0             5.99\n# ℹ 1 more variable: `1961_life_expectancy` &lt;dbl&gt;\n\n\n\ndat &lt;- raw_dat |&gt; pivot_longer(-country)\nhead(dat)\n\n# A tibble: 6 × 3\n  country name                 value\n  &lt;chr&gt;   &lt;chr&gt;                &lt;dbl&gt;\n1 Germany 1960_fertility        2.41\n2 Germany 1960_life_expectancy 69.3 \n3 Germany 1961_fertility        2.44\n4 Germany 1961_life_expectancy 69.8 \n5 Germany 1962_fertility        2.47\n6 Germany 1962_life_expectancy 70.0 \n\n\nThe result is not exactly what we refer to as tidy since each observation is associated with two, not one, rows. We want to have the values from the two variables, fertility and life expectancy, in two separate columns. The first challenge to achieve this is to separate the name column into the year and the variable type. Notice that the entries in this column separate the year from the variable name with an underscore:\n\ndat$name[1:5]\n\n[1] \"1960_fertility\"       \"1960_life_expectancy\" \"1961_fertility\"      \n[4] \"1961_life_expectancy\" \"1962_fertility\"      \n\n\nEncoding multiple variables in a column name is such a common problem that the readr package includes a function to separate these columns into two or more. Apart from the data, the separate function takes three arguments: the name of the column to be separated, the names to be used for the new columns, and the character that separates the variables. So, a first attempt at this is:\n\ndat |&gt; separate(name, c(\"year\", \"name\"), \"_\")\n\nBecause _ is the default separator assumed by separate, we do not have to include it in the code:\n\ndat |&gt; separate(name, c(\"year\", \"name\"))\n\nWarning: Expected 2 pieces. Additional pieces discarded in 112 rows [2, 4, 6, 8, 10, 12,\n14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, ...].\n\n\n# A tibble: 224 × 4\n   country year  name      value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;\n 1 Germany 1960  fertility  2.41\n 2 Germany 1960  life      69.3 \n 3 Germany 1961  fertility  2.44\n 4 Germany 1961  life      69.8 \n 5 Germany 1962  fertility  2.47\n 6 Germany 1962  life      70.0 \n 7 Germany 1963  fertility  2.49\n 8 Germany 1963  life      70.1 \n 9 Germany 1964  fertility  2.49\n10 Germany 1964  life      70.7 \n# ℹ 214 more rows\n\n\nWe get a warning. Here we tell it to fill the column on the right:\n\nvar_names &lt;- c(\"year\", \"first_variable_name\", \"second_variable_name\")\ndat |&gt; separate(name, var_names, fill = \"right\")\n\n# A tibble: 224 × 5\n   country year  first_variable_name second_variable_name value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany 1960  fertility           &lt;NA&gt;                  2.41\n 2 Germany 1960  life                expectancy           69.3 \n 3 Germany 1961  fertility           &lt;NA&gt;                  2.44\n 4 Germany 1961  life                expectancy           69.8 \n 5 Germany 1962  fertility           &lt;NA&gt;                  2.47\n 6 Germany 1962  life                expectancy           70.0 \n 7 Germany 1963  fertility           &lt;NA&gt;                  2.49\n 8 Germany 1963  life                expectancy           70.1 \n 9 Germany 1964  fertility           &lt;NA&gt;                  2.49\n10 Germany 1964  life                expectancy           70.7 \n# ℹ 214 more rows\n\n\nHowever, if we read the separate help file, we find that a better approach is to merge the last two variables when there is an extra separation:\n\ndat |&gt; separate(name, c(\"year\", \"name\"), extra = \"merge\")\n\n# A tibble: 224 × 4\n   country year  name            value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1 Germany 1960  fertility        2.41\n 2 Germany 1960  life_expectancy 69.3 \n 3 Germany 1961  fertility        2.44\n 4 Germany 1961  life_expectancy 69.8 \n 5 Germany 1962  fertility        2.47\n 6 Germany 1962  life_expectancy 70.0 \n 7 Germany 1963  fertility        2.49\n 8 Germany 1963  life_expectancy 70.1 \n 9 Germany 1964  fertility        2.49\n10 Germany 1964  life_expectancy 70.7 \n# ℹ 214 more rows\n\n\nThis achieves the separation we wanted. However, we are not done yet. We need to create a column for each variable. As we learned, the pivot_wider function can do this:\n\ndat |&gt; \n  separate(name, c(\"year\", \"name\"), extra = \"merge\") |&gt;\n  pivot_wider() |&gt;\n  mutate(year = as.integer(year)) |&gt;\n  ggplot(aes(fertility, life_expectancy, color = country)) + geom_point()\n\n\n\n\nThe data is now in tidy format with one row for each observation with three variables: year, fertility, and life expectancy.\n\n\n13.1.4 unite\nIt is sometimes useful to do the inverse of separate, unite two columns into one. To demonstrate how to use unite, we show code that, although not the optimal approach, serves as an illustration. Suppose that we did not know about extra and used this command to separate:\n\nvar_names &lt;- c(\"year\", \"first_variable_name\", \"second_variable_name\")\ndat |&gt; \n  separate(name, var_names, fill = \"right\")\n\n# A tibble: 224 × 5\n   country year  first_variable_name second_variable_name value\n   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;                &lt;dbl&gt;\n 1 Germany 1960  fertility           &lt;NA&gt;                  2.41\n 2 Germany 1960  life                expectancy           69.3 \n 3 Germany 1961  fertility           &lt;NA&gt;                  2.44\n 4 Germany 1961  life                expectancy           69.8 \n 5 Germany 1962  fertility           &lt;NA&gt;                  2.47\n 6 Germany 1962  life                expectancy           70.0 \n 7 Germany 1963  fertility           &lt;NA&gt;                  2.49\n 8 Germany 1963  life                expectancy           70.1 \n 9 Germany 1964  fertility           &lt;NA&gt;                  2.49\n10 Germany 1964  life                expectancy           70.7 \n# ℹ 214 more rows\n\n\nWe can achieve the same final result by uniting the second and third columns, then pivoting the columns and renaming fertility_NA to fertility:\n\ndat |&gt; \n  separate(name, var_names, fill = \"right\") |&gt;\n  unite(name, first_variable_name, second_variable_name) |&gt;\n  pivot_wider() |&gt;\n  rename(fertility = fertility_NA)\n\n# A tibble: 112 × 4\n   country year  fertility life_expectancy\n   &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt;           &lt;dbl&gt;\n 1 Germany 1960       2.41            69.3\n 2 Germany 1961       2.44            69.8\n 3 Germany 1962       2.47            70.0\n 4 Germany 1963       2.49            70.1\n 5 Germany 1964       2.49            70.7\n 6 Germany 1965       2.48            70.6\n 7 Germany 1966       2.44            70.8\n 8 Germany 1967       2.37            71.0\n 9 Germany 1968       2.28            70.6\n10 Germany 1969       2.17            70.5\n# ℹ 102 more rows"
  },
  {
    "objectID": "13-wrangling.html#joining-tables",
    "href": "13-wrangling.html#joining-tables",
    "title": "13  Wrangling",
    "section": "13.2 Joining tables",
    "text": "13.2 Joining tables\nThe information we need for a given analysis may not be just in one table. For example, when forecasting elections we used the function left_join to combine the information from two tables. Here we use a simpler example to illustrate the general challenge of combining tables.\nSuppose we want to explore the relationship between population size for US states and electoral votes. We have the population size in this table:\n\nlibrary(tidyverse)\nlibrary(dslabs)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\n\nand electoral votes in this one:\n\nhead(results_us_election_2016)\n\n         state electoral_votes clinton trump others\n1   California              55    61.7  31.6    6.7\n2        Texas              38    43.2  52.2    4.5\n3      Florida              29    47.8  49.0    3.2\n4     New York              29    59.0  36.5    4.5\n5     Illinois              20    55.8  38.8    5.4\n6 Pennsylvania              20    47.9  48.6    3.6\n\n\nJust concatenating these two tables together will not work since the order of the states is not the same.\n\nidentical(results_us_election_2016$state, murders$state)\n\n[1] FALSE\n\n\nThe join functions, described below, are designed to handle this challenge.\n\n13.2.1 Joins\nThe join functions in the dplyr package make sure that the tables are combined so that matching rows are together. If you know SQL, you will see that the approach and syntax is very similar. The general idea is that one needs to identify one or more columns that will serve to match the two tables. Then a new table with the combined information is returned. Notice what happens if we join the two tables above by state using left_join (we will remove the others column and rename electoral_votes so that the tables fit on the page):\n\ntab &lt;- left_join(murders, results_us_election_2016, by = \"state\") |&gt;\n  select(-others) |&gt; rename(ev = electoral_votes)\nhead(tab)\n\n       state abb region population total ev clinton trump\n1    Alabama  AL  South    4779736   135  9    34.4  62.1\n2     Alaska  AK   West     710231    19  3    36.6  51.3\n3    Arizona  AZ   West    6392017   232 11    45.1  48.7\n4   Arkansas  AR  South    2915918    93  6    33.7  60.6\n5 California  CA   West   37253956  1257 55    61.7  31.6\n6   Colorado  CO   West    5029196    65  9    48.2  43.3\n\n\nThe data has been successfully joined and we can now, for example, make a plot to explore the relationship:\n\nlibrary(ggrepel)\ntab |&gt; ggplot(aes(population/10^6, ev)) +\n  geom_point() +\n  geom_text_repel(aes(label = abb), max.overlaps = 20) + \n  scale_x_continuous(trans = \"log2\") +\n  scale_y_continuous(trans = \"log2\") +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\nWe see the relationship is close to linear with about 2 electoral votes for every million persons, but with very small states getting higher ratios.\nIn practice, it is not always the case that each row in one table has a matching row in the other. For this reason, we have several versions of join. To illustrate this challenge, we will take subsets of the tables above. We create the tables tab1 and tab2 so that they have some states in common but not all:\n\ntab_1 &lt;- slice(murders, 1:6) |&gt; select(state, population)\ntab_1\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4   Arkansas    2915918\n5 California   37253956\n6   Colorado    5029196\n\ntab_2 &lt;- results_us_election_2016 |&gt; \n  filter(state %in% c(\"Alabama\", \"Alaska\", \"Arizona\", \n                    \"California\", \"Connecticut\", \"Delaware\")) |&gt; \n  select(state, electoral_votes) |&gt; rename(ev = electoral_votes)\ntab_2\n\n        state ev\n1  California 55\n2     Arizona 11\n3     Alabama  9\n4 Connecticut  7\n5      Alaska  3\n6    Delaware  3\n\n\nWe will use these two tables as examples in the next sections.\n\n13.2.1.1 Left join\nSuppose we want a table like tab_1, but adding electoral votes to whatever states we have available. For this, we use left_join with tab_1 as the first argument. We specify which column to use to match with the by argument.\n\nleft_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4   Arkansas    2915918 NA\n5 California   37253956 55\n6   Colorado    5029196 NA\n\n\nNote that NAs are added to the two states not appearing in tab_2. Also, notice that this function, as well as all the other joins, can receive the first arguments through the pipe:\n\ntab_1 |&gt; left_join(tab_2, by = \"state\")\n\n\n\n13.2.1.2 Right join\nIf instead of a table with the same rows as first table, we want one with the same rows as second table, we can use right_join:\n\ntab_1 |&gt; right_join(tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4  California   37253956 55\n5 Connecticut         NA  7\n6    Delaware         NA  3\n\n\nNow the NAs are in the column coming from tab_1.\n\n\n13.2.1.3 Inner join\nIf we want to keep only the rows that have information in both tables, we use inner_join. You can think of this as an intersection:\n\ninner_join(tab_1, tab_2, by = \"state\")\n\n       state population ev\n1    Alabama    4779736  9\n2     Alaska     710231  3\n3    Arizona    6392017 11\n4 California   37253956 55\n\n\n\n\n13.2.1.4 Full join\nIf we want to keep all the rows and fill the missing parts with NAs, we can use full_join. You can think of this as a union:\n\nfull_join(tab_1, tab_2, by = \"state\")\n\n        state population ev\n1     Alabama    4779736  9\n2      Alaska     710231  3\n3     Arizona    6392017 11\n4    Arkansas    2915918 NA\n5  California   37253956 55\n6    Colorado    5029196 NA\n7 Connecticut         NA  7\n8    Delaware         NA  3\n\n\n\n\n13.2.1.5 Semi join\nThe semi_join function lets us keep the part of first table for which we have information in the second. It does not add the columns of the second:\n\nsemi_join(tab_1, tab_2, by = \"state\")\n\n       state population\n1    Alabama    4779736\n2     Alaska     710231\n3    Arizona    6392017\n4 California   37253956\n\n\n\n\n13.2.1.6 Anti join\nThe function anti_join is the opposite of semi_join. It keeps the elements of the first table for which there is no information in the second:\n\nanti_join(tab_1, tab_2, by = \"state\")\n\n     state population\n1 Arkansas    2915918\n2 Colorado    5029196\n\n\n\n\n\n13.2.2 Set operators\nYou can use set operators on data frames:\n\n13.2.2.1 Intersect\nYou can take intersections of vectors of any type, such as numeric:\n\nintersect(1:10, 6:15)\n\n[1]  6  7  8  9 10\n\n\nor characters:\n\nintersect(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"b\" \"c\"\n\n\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::intersect(tab_1, tab_2)\n\n       state abb region population total ev clinton trump\n1    Arizona  AZ   West    6392017   232 11    45.1  48.7\n2   Arkansas  AR  South    2915918    93  6    33.7  60.6\n3 California  CA   West   37253956  1257 55    61.7  31.6\n\n\n\n\n13.2.2.2 Union\nSimilarly union takes the union of vectors. For example:\n\nunion(1:10, 6:15)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15\n\nunion(c(\"a\",\"b\",\"c\"), c(\"b\",\"c\",\"d\"))\n\n[1] \"a\" \"b\" \"c\" \"d\"\n\n\nThe dplyr package includes a version of union that combines all the rows of two tables with the same column names.\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::union(tab_1, tab_2) \n\n        state abb    region population total ev clinton trump\n1     Alabama  AL     South    4779736   135  9    34.4  62.1\n2      Alaska  AK      West     710231    19  3    36.6  51.3\n3     Arizona  AZ      West    6392017   232 11    45.1  48.7\n4    Arkansas  AR     South    2915918    93  6    33.7  60.6\n5  California  CA      West   37253956  1257 55    61.7  31.6\n6    Colorado  CO      West    5029196    65  9    48.2  43.3\n7 Connecticut  CT Northeast    3574097    97  7    54.6  40.9\n\n\n\n\n13.2.2.3 setdiff\nThe set difference between a first and second argument can be obtained with setdiff. Unlike intersect and union, this function is not symmetric:\n\nsetdiff(1:10, 6:15)\n\n[1] 1 2 3 4 5\n\nsetdiff(6:15, 1:10)\n\n[1] 11 12 13 14 15\n\n\nAs with the functions shown above, dplyr has a version for data frames:\n\ntab_1 &lt;- tab[1:5,]\ntab_2 &lt;- tab[3:7,]\ndplyr::setdiff(tab_1, tab_2)\n\n    state abb region population total ev clinton trump\n1 Alabama  AL  South    4779736   135  9    34.4  62.1\n2  Alaska  AK   West     710231    19  3    36.6  51.3\n\n\n\n\n13.2.2.4 setequal\nFinally, the function setequal tells us if two sets are the same, regardless of order. So notice that:\n\nsetequal(1:5, 1:6)\n\n[1] FALSE\n\n\nbut:\n\nsetequal(1:5, 5:1)\n\n[1] TRUE\n\n\nWhen applied to data frames that are not equal, regardless of order, the dplyr version provides a useful message letting us know how the sets are different:\n\ndplyr::setequal(tab_1, tab_2)\n\n[1] FALSE"
  },
  {
    "objectID": "13-wrangling.html#string-processing",
    "href": "13-wrangling.html#string-processing",
    "title": "13  Wrangling",
    "section": "13.3 String processing",
    "text": "13.3 String processing\n\n13.3.1 The stringr package\n\nlibrary(tidyverse)\nlibrary(stringr)\n\n\n\n13.3.2 Case study: self-reported heights\nThe dslabs package includes the raw data from which the heights dataset was obtained. You can load it like this:\n\nlibrary(dslabs)\nhead(reported_heights)\n\n           time_stamp    sex height\n1 2014-09-02 13:40:36   Male     75\n2 2014-09-02 13:46:59   Male     70\n3 2014-09-02 13:59:20   Male     68\n4 2014-09-02 14:51:53   Male     74\n5 2014-09-02 15:16:15   Male     61\n6 2014-09-02 15:16:16 Female     65\n\n\n\nclass(reported_heights$height)\n\n[1] \"character\"\n\n\nIf we try to parse it into numbers, we get a warning:\n\nx &lt;- as.numeric(reported_heights$height)\n\nWarning: NAs introduced by coercion\n\n\nAlthough most values appear to be height in inches as requested:\n\nhead(x)\n\n[1] 75 70 68 74 61 65\n\n\nwe do end up with many NAs:\n\nsum(is.na(x))\n\n[1] 81\n\n\nWe can see some of the entries that are not successfully converted by using filter to keep only the entries resulting in NAs:\n\nreported_heights |&gt; \n  mutate(new_height = as.numeric(height)) |&gt;\n  filter(is.na(new_height)) |&gt; \n  head(n = 10)\n\n            time_stamp    sex                 height new_height\n1  2014-09-02 15:16:28   Male                  5' 4\"         NA\n2  2014-09-02 15:16:37 Female                  165cm         NA\n3  2014-09-02 15:16:52   Male                    5'7         NA\n4  2014-09-02 15:16:56   Male                  &gt;9000         NA\n5  2014-09-02 15:16:56   Male                   5'7\"         NA\n6  2014-09-02 15:17:09 Female                   5'3\"         NA\n7  2014-09-02 15:18:00   Male 5 feet and 8.11 inches         NA\n8  2014-09-02 15:19:48   Male                   5'11         NA\n9  2014-09-04 00:46:45   Male                  5'9''         NA\n10 2014-09-04 10:29:44   Male                 5'10''         NA\n\n\nWe permit a range that covers about 99.9999% of the adult population. We also use suppressWarnings to avoid the warning message we know as.numeric will gives us.\n\nnot_inches &lt;- function(x, smallest = 50, tallest = 84){\n  inches &lt;- suppressWarnings(as.numeric(x))\n  ind &lt;- is.na(inches) | inches &lt; smallest | inches &gt; tallest\n  ind\n}\n\nWe apply this function and find the number of problematic entries:\n\nproblems &lt;- reported_heights |&gt; \n  filter(not_inches(height)) |&gt;\n  pull(height)\nproblems\n\n  [1] \"6\"                      \"5' 4\\\"\"                 \"5.3\"                   \n  [4] \"165cm\"                  \"511\"                    \"6\"                     \n  [7] \"2\"                      \"5'7\"                    \"&gt;9000\"                 \n [10] \"5'7\\\"\"                  \"5'3\\\"\"                  \"5 feet and 8.11 inches\"\n [13] \"5.25\"                   \"5'11\"                   \"5.5\"                   \n [16] \"11111\"                  \"5'9''\"                  \"6\"                     \n [19] \"6.5\"                    \"150\"                    \"5'10''\"                \n [22] \"103.2\"                  \"5.8\"                    \"19\"                    \n [25] \"5\"                      \"5.6\"                    \"175\"                   \n [28] \"177\"                    \"300\"                    \"5,3\"                   \n [31] \"6'\"                     \"6\"                      \"5.9\"                   \n [34] \"6,8\"                    \"5' 10\"                  \"5.5\"                   \n [37] \"178\"                    \"163\"                    \"6.2\"                   \n [40] \"175\"                    \"Five foot eight inches\" \"6.2\"                   \n [43] \"5.8\"                    \"5.1\"                    \"178\"                   \n [46] \"165\"                    \"5.11\"                   \"5'5\\\"\"                 \n [49] \"165\"                    \"180\"                    \"5'2\\\"\"                 \n [52] \"5.75\"                   \"169\"                    \"5,4\"                   \n [55] \"7\"                      \"5.4\"                    \"157\"                   \n [58] \"6.1\"                    \"169\"                    \"5'3\"                   \n [61] \"5.6\"                    \"214\"                    \"183\"                   \n [64] \"5.6\"                    \"6\"                      \"162\"                   \n [67] \"178\"                    \"180\"                    \"5'10''\"                \n [70] \"170\"                    \"5'3''\"                  \"178\"                   \n [73] \"0.7\"                    \"190\"                    \"5.4\"                   \n [76] \"184\"                    \"5'7''\"                  \"5.9\"                   \n [79] \"5'12\"                   \"5.6\"                    \"5.6\"                   \n [82] \"184\"                    \"6\"                      \"167\"                   \n [85] \"2'33\"                   \"5'11\"                   \"5'3\\\"\"                 \n [88] \"5.5\"                    \"5.2\"                    \"180\"                   \n [91] \"5.5\"                    \"5.5\"                    \"6.5\"                   \n [94] \"5,8\"                    \"180\"                    \"183\"                   \n [97] \"170\"                    \"5'6''\"                  \"172\"                   \n[100] \"612\"                    \"5.11\"                   \"168\"                   \n[103] \"5'4\"                    \"1,70\"                   \"172\"                   \n[106] \"87\"                     \"5.5\"                    \"176\"                   \n[109] \"5'7.5''\"                \"5'7.5''\"                \"111\"                   \n[112] \"5'2\\\"\"                  \"173\"                    \"174\"                   \n[115] \"176\"                    \"175\"                    \"5' 7.78\\\"\"             \n[118] \"6.7\"                    \"12\"                     \"6\"                     \n[121] \"5.1\"                    \"5.6\"                    \"5.5\"                   \n[124] \"yyy\"                    \"5.2\"                    \"5'5\"                   \n[127] \"5'8\"                    \"5'6\"                    \"5 feet 7inches\"        \n[130] \"89\"                     \"5.6\"                    \"5.7\"                   \n[133] \"183\"                    \"172\"                    \"34\"                    \n[136] \"25\"                     \"6\"                      \"5.9\"                   \n[139] \"168\"                    \"6.5\"                    \"170\"                   \n[142] \"175\"                    \"6\"                      \"22\"                    \n[145] \"5.11\"                   \"684\"                    \"6\"                     \n[148] \"1\"                      \"1\"                      \"6*12\"                  \n[151] \"5 .11\"                  \"87\"                     \"162\"                   \n[154] \"165\"                    \"184\"                    \"6\"                     \n[157] \"173\"                    \"1.6\"                    \"172\"                   \n[160] \"170\"                    \"5.7\"                    \"5.5\"                   \n[163] \"174\"                    \"170\"                    \"160\"                   \n[166] \"120\"                    \"120\"                    \"23\"                    \n[169] \"192\"                    \"5 11\"                   \"167\"                   \n[172] \"150\"                    \"1.7\"                    \"174\"                   \n[175] \"5.8\"                    \"6\"                      \"5'4\"                   \n[178] \"5'8\\\"\"                  \"5'5\"                    \"5.8\"                   \n[181] \"5.1\"                    \"5.11\"                   \"5.7\"                   \n[184] \"5'7\"                    \"5'6\"                    \"5'11\\\"\"                \n[187] \"5'7\\\"\"                  \"5'7\"                    \"172\"                   \n[190] \"5'8\"                    \"180\"                    \"5' 11\\\"\"               \n[193] \"5\"                      \"180\"                    \"180\"                   \n[196] \"6'1\\\"\"                  \"5.9\"                    \"5.2\"                   \n[199] \"5.5\"                    \"69\\\"\"                   \"5' 7\\\"\"                \n[202] \"5'10''\"                 \"5.51\"                   \"5'10\"                  \n[205] \"5'10\"                   \"5ft 9 inches\"           \"5 ft 9 inches\"         \n[208] \"5'2\"                    \"5'11\"                   \"5.8\"                   \n[211] \"5.7\"                    \"167\"                    \"168\"                   \n[214] \"6\"                      \"6.1\"                    \"5'11''\"                \n[217] \"5.69\"                   \"178\"                    \"182\"                   \n[220] \"164\"                    \"5'8\\\"\"                  \"185\"                   \n[223] \"6\"                      \"86\"                     \"5.7\"                   \n[226] \"708,661\"                \"5.25\"                   \"5.5\"                   \n[229] \"5 feet 6 inches\"        \"5'10''\"                 \"172\"                   \n[232] \"6\"                      \"5'8\"                    \"160\"                   \n[235] \"6'3\\\"\"                  \"649,606\"                \"10000\"                 \n[238] \"5.1\"                    \"152\"                    \"1\"                     \n[241] \"180\"                    \"728,346\"                \"175\"                   \n[244] \"158\"                    \"173\"                    \"164\"                   \n[247] \"6 04\"                   \"169\"                    \"0\"                     \n[250] \"185\"                    \"168\"                    \"5'9\"                   \n[253] \"169\"                    \"5'5''\"                  \"174\"                   \n[256] \"6.3\"                    \"179\"                    \"5'7\\\"\"                 \n[259] \"5.5\"                    \"6\"                      \"6\"                     \n[262] \"170\"                    \"6\"                      \"172\"                   \n[265] \"158\"                    \"100\"                    \"159\"                   \n[268] \"190\"                    \"5.7\"                    \"170\"                   \n[271] \"158\"                    \"6'4\\\"\"                  \"180\"                   \n[274] \"5.57\"                   \"5'4\"                    \"210\"                   \n[277] \"88\"                     \"6\"                      \"162\"                   \n[280] \"170 cm\"                 \"5.7\"                    \"170\"                   \n[283] \"157\"                    \"186\"                    \"170\"                   \n[286] \"7,283,465\"              \"5\"                      \"5\"                     \n[289] \"34\"                     \"161\"                    \"5'6\"                   \n[292] \"5'6\"                   \n\n\n\n\n13.3.3 Regular expressions\n\n13.3.3.1 Special characters\nNow let’s consider a slightly more complicated example. Which of the following strings contain the pattern cm or inches?\n\nyes &lt;- c(\"180 cm\", \"70 inches\")\nno &lt;- c(\"180\", \"70''\")\ns &lt;- c(yes, no)\n\n\nstr_detect(s, \"cm\") | str_detect(s, \"inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nHowever, we don’t need to do this. The main feature that distinguishes the regex language from plain strings is that we can use special characters. These are characters with a meaning. We start by introducing | which means or. So if we want to know if either cm or inches appears in the strings, we can use the regex cm|inches:\n\nstr_detect(s, \"cm|inches\")\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nand obtain the correct answer.\nAnother special character that will be useful for identifying feet and inches values is \\d which means any digit: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9. The backslash is used to distinguish it from the character d. In R, we have to escape the backslash \\ so we actually have to use \\\\d to represent digits. Here is an example:\n\nyes &lt;- c(\"5\", \"6\", \"5'10\", \"5 feet\", \"4'11\")\nno &lt;- c(\"\", \".\", \"Five\", \"six\")\ns &lt;- c(yes, no)\npattern &lt;- \"\\\\d\"\nstr_detect(s, pattern)\n\n[1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nWe take this opportunity to introduce the str_view_all function, which is helpful for troubleshooting as it shows us the first match for each string:\n\nstr_view_all(s, pattern)\n\nWarning: `str_view()` was deprecated in stringr 1.5.0.\nℹ Please use `str_view_all()` instead.\n\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'&lt;1&gt;&lt;0&gt;\n[4] │ &lt;5&gt; feet\n[5] │ &lt;4&gt;'&lt;1&gt;&lt;1&gt;\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nand str_view_all shows us all the matches, so 3'2 has two matches and 5'10 has three.\n\nstr_view_all(s, pattern)\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'&lt;1&gt;&lt;0&gt;\n[4] │ &lt;5&gt; feet\n[5] │ &lt;4&gt;'&lt;1&gt;&lt;1&gt;\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nThere are many other special characters. We will learn some others below, but you can see most or all of them in the cheat sheet1 mentioned earlier.\nFinally, a useful special character is \\w which stands for word character and it matches any letter, number, or underscore. It is equivalent to [a-zA-Z0-9_].\n\n\n13.3.3.2 Character classes\nCharacter classes are used to define a series of characters that can be matched. We define character classes with square brackets []. So, for example, if we want the pattern to match only if we have a 5 or a 6, we use the regex [56]:\n\nstr_view_all(s, \"[56]\")\n\n[1] │ &lt;5&gt;\n[2] │ &lt;6&gt;\n[3] │ &lt;5&gt;'10\n[4] │ &lt;5&gt; feet\n[5] │ 4'11\n[6] │ \n[7] │ .\n[8] │ Five\n[9] │ six\n\n\nSuppose we want to match values between 4 and 7. A common way to define character classes is with ranges. So, for example, [0-9] is equivalent to \\\\d. The pattern we want is therefore [4-7].\n\nyes &lt;- as.character(4:7)\nno &lt;- as.character(1:3)\ns &lt;- c(yes, no)\nstr_detect(s, \"[4-7]\")\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE\n\n\nHowever, it is important to know that in regex everything is a character; there are no numbers. So 4 is the character 4 not the number four. Notice, for example, that [1-20] does not mean 1 through 20, it means the characters 1 through 2 or the character 0. So [1-20] simply means the character class composed of 0, 1, and 2.\nKeep in mind that characters do have an order and the digits do follow the numeric order. So 0 comes before 1 which comes before 2 and so on. For the same reason, we can define lower case letters as [a-z], upper case letters as [A-Z], and [a-zA-z] as both.\n\n\n13.3.3.3 Anchors\nWhat if we want a match when we have exactly 1 digit? This will be useful in our case study since feet are never more than 1 digit so a restriction will help us. One way to do this with regex is by using anchors, which let us define patterns that must start or end at a specific place. The two most common anchors are ^ and $ which represent the beginning and end of a string, respectively. So the pattern ^\\\\d$ is read as “start of the string followed by one digit followed by end of string”.\nThis pattern now only detects the strings with exactly one digit:\n\npattern &lt;- \"^\\\\d$\"\nyes &lt;- c(\"1\", \"5\", \"9\")\nno &lt;- c(\"12\", \"123\", \" 1\", \"a4\", \"b\")\ns &lt;- c(yes, no)\nstr_view_all(s, pattern)\n\n[1] │ &lt;1&gt;\n[2] │ &lt;5&gt;\n[3] │ &lt;9&gt;\n[4] │ 12\n[5] │ 123\n[6] │  1\n[7] │ a4\n[8] │ b\n\n\nThe 1 does not match because it does not start with the digit but rather with a space, which is actually not easy to see.\n\n\n13.3.3.4 Quantifiers\nFor the inches part, we can have one or two digits. This can be specified in regex with quantifiers. This is done by following the pattern with curly brackets containing the number of times the previous entry can be repeated. We use an example to illustrate. The pattern for one or two digits is:\n\npattern &lt;- \"^\\\\d{1,2}$\"\nyes &lt;- c(\"1\", \"5\", \"9\", \"12\")\nno &lt;- c(\"123\", \"a4\", \"b\")\nstr_view_all(c(yes, no), pattern)\n\n[1] │ &lt;1&gt;\n[2] │ &lt;5&gt;\n[3] │ &lt;9&gt;\n[4] │ &lt;12&gt;\n[5] │ 123\n[6] │ a4\n[7] │ b\n\n\n\n\n13.3.3.5 White space \\s\nAnother problem we have are spaces. For example, our pattern does not match 5' 4\" because there is a space between ' and 4 which our pattern does not permit. Spaces are characters and R does not ignore them:\n\nidentical(\"Hi\", \"Hi \")\n\n[1] FALSE\n\n\nIn regex, \\s represents white space. To find patterns like 5' 4, we can change our pattern to:\n\npattern_2 &lt;- \"^[4-7]'\\\\s\\\\d{1,2}\\\"$\"\nstr_subset(problems, pattern_2)\n\n[1] \"5' 4\\\"\"  \"5' 11\\\"\" \"5' 7\\\"\" \n\n\nHowever, this will not match the patterns with no space. So do we need more than one regex pattern? It turns out we can use a quantifier for this as well.\n\n\n13.3.3.6 Quantifiers: *, ?, +\nWe want the pattern to permit spaces but not require them. Even if there are several spaces, like in this example 5'   4, we still want it to match. There is a quantifier for exactly this purpose. In regex, the character * means zero or more instances of the previous character. Here is an example:\n\nyes &lt;- c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\")\nno &lt;- c(\"A2B\", \"A21B\")\nstr_detect(yes, \"A1*B\")\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\nstr_detect(no, \"A1*B\")\n\n[1] FALSE FALSE\n\n\nThe above matches the first string which has zero 1s and all the strings with one or more 1. We can then improve our pattern by adding the * after the space character \\s.\nThere are two other similar quantifiers. For none or once, we can use ?, and for one or more, we can use +. You can see how they differ with this example:\n\ndata.frame(string = c(\"AB\", \"A1B\", \"A11B\", \"A111B\", \"A1111B\"),\n           none_or_more = str_detect(yes, \"A1*B\"),\n           nore_or_once = str_detect(yes, \"A1?B\"),\n           once_or_more = str_detect(yes, \"A1+B\"))\n\n  string none_or_more nore_or_once once_or_more\n1     AB         TRUE         TRUE        FALSE\n2    A1B         TRUE         TRUE         TRUE\n3   A11B         TRUE        FALSE         TRUE\n4  A111B         TRUE        FALSE         TRUE\n5 A1111B         TRUE        FALSE         TRUE\n\n\nWe will actually use all three in our reported heights example, but we will see these in a later section.\n\n\n13.3.3.7 Not\nTo specify patterns that we do not want to detect, we can use the ^ symbol but only inside square brackets. Remember that outside the square bracket ^ means the start of the string. So, for example, if we want to detect digits that are preceded by anything except a letter we can do the following:\n\npattern &lt;- \"[^a-zA-Z]\\\\d\"\nyes &lt;- c(\".3\", \"+2\", \"-0\",\"*4\")\nno &lt;- c(\"A3\", \"B2\", \"C0\", \"E4\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE FALSE\n\n\nAnother way to generate a pattern that searches for everything except is to use the upper case of the special character. For example \\\\D means anything other than a digit, \\\\S means anything except a space, and so on.\n\n\n13.3.3.8 Lookarounds\nLookarounds provide a way to ask for one or more conditions to be satisfied without moving the search forward or matching it. For example, you might want to check for multiple conditions and if they are matched, then return the pattern or part of the pattern that matched. An example: check if a string satisfies the conditions for a password and if it does return the password. Suppose the conditions are 1) 8-16 word characters, 2) starts with a letter, and 3) has at least one digit.\nThere are four types of lookarounds: lookahead (?=pattern), lookbehind (?&lt;=pattern), negative lookahead (?!pattern), and negative lookbehind (?&lt;!pattern). You can concatenate them to check for multiple conditions so for our example we can write it like this:\n\npattern &lt;- \"(?=\\\\w{8,16})(?=^[a-z|A-Z].*)(?=.*\\\\d+.*).*\"\nyes &lt;- c(\"Ihatepasswords1\", \"password1234\")\nno &lt;- c(\"sh0rt\", \"Ihaterpasswords\", \"7X%9,N`yrYG92b7\")\nstr_detect(yes, pattern)\n\n[1] TRUE TRUE\n\nstr_detect(no, pattern)\n\n[1] FALSE FALSE FALSE\n\nstr_extract(yes, pattern)\n\n[1] \"Ihatepasswords1\" \"password1234\"   \n\n\n\n\n13.3.3.9 Groups\nGroups are a powerful aspect of regex that permits the extraction of values. Groups are defined using parentheses. They don’t affect the pattern matching per se. Instead, it permits tools to identify specific parts of the pattern so we can extract them.\nWe want to change heights written like 5.6 to 5'6.\nTo avoid changing patterns such as 70.2, we will require that the first digit be between 4 and 7 [4-7] and that the second be none or more digits \\\\d*. Let’s start by defining a simple pattern that matches this:\n\npattern_without_groups &lt;- \"^[4-7],\\\\d*$\"\n\nWe want to extract the digits so we can then form the new version using a period. These are our two groups, so we encapsulate them with parentheses:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\n\nWe encapsulate the part of the pattern that matches the parts we want to keep for later use. Adding groups does not affect the detection, since it only signals that we want to save what is captured by the groups. Note that both patterns return the same result when using str_detect:\n\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_detect(s, pattern_without_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\nstr_detect(s, pattern_with_groups)\n\n[1]  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nOnce we define groups, we can use the function str_match to extract the values these groups define:\n\nstr_match(s, pattern_with_groups)\n\n     [,1]   [,2] [,3]\n[1,] \"5,9\"  \"5\"  \"9\" \n[2,] \"5,11\" \"5\"  \"11\"\n[3,] \"6,\"   \"6\"  \"\"  \n[4,] \"6,1\"  \"6\"  \"1\" \n[5,] NA     NA   NA  \n[6,] NA     NA   NA  \n[7,] NA     NA   NA  \n[8,] NA     NA   NA  \n\n\nNotice that the second and third columns contain feet and inches, respectively. The first column is the part of the string matching the pattern. If no match occurred, we see an NA.\nNow we can understand the difference between the functions str_extract and str_match: str_extract extracts only strings that match a pattern, not the values defined by groups:\n\nstr_extract(s, pattern_with_groups)\n\n[1] \"5,9\"  \"5,11\" \"6,\"   \"6,1\"  NA     NA     NA     NA    \n\n\n\n\n\n13.3.4 Search and replace with regex\nEarlier we defined the object problems containing the strings that do not appear to be in inches. We can see that not too many of our problematic strings match the pattern:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}\\\"$\"\nsum(str_detect(problems, pattern))\n\n[1] 14\n\n\nTo see why this is, we show some examples that expose why we don’t have more matches:\n\nproblems[  c(2, 10, 11, 12, 15)] |&gt; str_view_all(pattern)\n\n[1] │ 5' 4\"\n[2] │ &lt;5'7\"&gt;\n[3] │ &lt;5'3\"&gt;\n[4] │ 5 feet and 8.11 inches\n[5] │ 5.5\n\n\nAn initial problem we see immediately is that some students wrote out the words “feet” and “inches”. We can see the entries that did this with the str_subset function:\n\nstr_subset(problems, \"inches\")\n\n[1] \"5 feet and 8.11 inches\" \"Five foot eight inches\" \"5 feet 7inches\"        \n[4] \"5ft 9 inches\"           \"5 ft 9 inches\"          \"5 feet 6 inches\"       \n\n\nWe also see that some entries used two single quotes '' instead of a double quote \".\n\nstr_subset(problems, \"''\")\n\n [1] \"5'9''\"   \"5'10''\"  \"5'10''\"  \"5'3''\"   \"5'7''\"   \"5'6''\"   \"5'7.5''\"\n [8] \"5'7.5''\" \"5'10''\"  \"5'11''\"  \"5'10''\"  \"5'5''\"  \n\n\nTo correct this, we can replace the different ways of representing inches and feet with a uniform symbol. We will use ' for feet, whereas for inches we will simply not use a symbol since some entries were of the form x'y. Now, if we no longer use the inches symbol, we have to change our pattern accordingly:\n\npattern &lt;- \"^[4-7]'\\\\d{1,2}$\"\n\nIf we do this replacement before the matching, we get many more matches:\n\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n\n[1] 48\n\n\nHowever, we still have many cases to go.\nNote that in the code above, we leveraged the stringr consistency and used the pipe.\nFor now, we improve our pattern by adding \\\\s* in front of and after the feet symbol ' to permit space between the feet symbol and the numbers. Now we match a few more entries:\n\npattern &lt;- \"^[4-7]\\\\s*'\\\\s*\\\\d{1,2}$\"\nproblems |&gt; \n  str_replace(\"feet|ft|foot\", \"'\") |&gt; # replace feet, ft, foot with ' \n  str_replace(\"inches|in|''|\\\"\", \"\") |&gt; # remove all inches symbols\n  str_detect(pattern) |&gt; \n  sum()\n\n[1] 53\n\n\n\n13.3.4.1 Search and replace using groups\nAnother powerful aspect of groups is that you can refer to the extracted values in a regex when searching and replacing.\nThe regex special character for the i-th group is \\\\i. So \\\\1 is the value extracted from the first group, \\\\2 the value from the second and so on. As a simple example, note that the following code will replace a comma with period, but only if it is between two digits:\n\npattern_with_groups &lt;-  \"^([4-7]),(\\\\d*)$\"\nyes &lt;- c(\"5,9\", \"5,11\", \"6,\", \"6,1\")\nno &lt;- c(\"5'9\", \",\", \"2,8\", \"6.1.1\")\ns &lt;- c(yes, no)\nstr_replace(s, pattern_with_groups, \"\\\\1'\\\\2\")\n\n[1] \"5'9\"   \"5'11\"  \"6'\"    \"6'1\"   \"5'9\"   \",\"     \"2,8\"   \"6.1.1\"\n\n\n\n\n\n13.3.5 Trimming\nIn general, spaces at the start or end of the string are uninformative. These can be particularly deceptive because sometimes they can be hard to see:\n\ns &lt;- \"Hi \"\ncat(s)\n\nHi \n\nidentical(s, \"Hi\")\n\n[1] FALSE\n\n\nThis is a general enough problem that there is a function dedicated to removing them: str_trim.\n\nstr_trim(\" 5 ' 9 \")\n\n[1] \"5 ' 9\"\n\n\n\n\n13.3.6 Changing lettercase\n\ns &lt;- c(\"Five feet eight inches\")\nstr_to_lower(s)\n\n[1] \"five feet eight inches\"\n\n\nOther related functions are str_to_upper and str_to_title. We are now ready to define a procedure that converts all the problematic cases to inches.\n\n\n13.3.7 The extract function\nThe extract function is a useful tidyverse function for string processing that we will use in our final solution, so we introduce it here. In a previous section, we constructed a regex that lets us identify which elements of a character vector match the feet and inches pattern. However, we want to do more. We want to extract and save the feet and number values so that we can convert them to inches when appropriate.\nIf we have a simpler case like this:\n\ns &lt;- c(\"5'10\", \"6'1\")\ntab &lt;- data.frame(x = s)\n\nIn Section Section 13.1.3 we learned about the separate function, which can be used to achieve our current goal:\n\ntab |&gt; separate(x, c(\"feet\", \"inches\"), sep = \"'\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nThe extract function from the tidyr package lets us use regex groups to extract the desired values. Here is the equivalent to the code above using separate but using extract:\n\nlibrary(tidyr)\ntab |&gt; extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n\n\nSo why do we even need the new function extract? We have seen how small changes can throw off exact pattern matching. Groups in regex give us more flexibility. For example, if we define:\n\ns &lt;- c(\"5'10\", \"6'1\\\"\",\"5'8inches\")\ntab &lt;- data.frame(x = s)\n\nand we only want the numbers, separate fails:\n\ntab |&gt; separate(x, c(\"feet\",\"inches\"), sep = \"'\", fill = \"right\")\n\n  feet  inches\n1    5      10\n2    6      1\"\n3    5 8inches\n\n\nHowever, we can use extract. The regex here is a bit more complicated since we have to permit ' with spaces and feet. We also do not want the \" included in the value, so we do not include that in the group:\n\ntab |&gt; extract(x, c(\"feet\", \"inches\"), regex = \"(\\\\d)'(\\\\d{1,2})\")\n\n  feet inches\n1    5     10\n2    6      1\n3    5      8\n\n\n\n\n13.3.8 Putting it all together\nWe are now ready to put it all together and wrangle our reported heights data to try to recover as many heights as possible. The code is complex, but we will break it down into parts.\nWe start by cleaning up the height column so that the heights are closer to a feet’inches format. We added an original heights column so we can compare before and after.\nWe now put all of what we have learned together into a function that takes a string vector and tries to convert as many strings as possible to one format. We write a function that puts together what we have done above.\n\nconvert_format &lt;- function(s){\n  s |&gt;\n    str_replace(\"feet|foot|ft\", \"'\") |&gt; \n    str_replace_all(\"inches|in|''|\\\"|cm|and\", \"\") |&gt;  \n    str_replace(\"^([4-7])\\\\s*[,\\\\.\\\\s+]\\\\s*(\\\\d*)$\", \"\\\\1'\\\\2\") |&gt; \n    str_replace(\"^([56])'?$\", \"\\\\1'0\") |&gt; \n    str_replace(\"^([12])\\\\s*,\\\\s*(\\\\d*)$\", \"\\\\1\\\\.\\\\2\") |&gt;  \n    str_trim() \n}\nlibrary(english)\nwords_to_numbers &lt;- function(s){\n  s &lt;- str_to_lower(s)\n  for (i in 0:11)\n    s &lt;- str_replace_all(s, words(i), as.character(i))\n  s\n}\n\n\npattern &lt;- \"^([4-7])\\\\s*'\\\\s*(\\\\d+\\\\.?\\\\d*)$\"\n\nsmallest &lt;- 50\ntallest &lt;- 84\nnew_heights &lt;- reported_heights |&gt; \n  mutate(original = height, \n         height = words_to_numbers(height) |&gt; convert_format()) |&gt;\n  extract(height, c(\"feet\", \"inches\"), regex = pattern, remove = FALSE) |&gt; \n  mutate_at(c(\"height\", \"feet\", \"inches\"), as.numeric) |&gt;\n  mutate(guess = 12 * feet + inches) |&gt;\n  mutate(height = case_when(\n    is.na(height) ~ as.numeric(NA),\n    between(height, smallest, tallest) ~ height,  #inches\n    between(height/2.54, smallest, tallest) ~ height/2.54, #cm\n    between(height*100/2.54, smallest, tallest) ~ height*100/2.54, #meters\n    TRUE ~ as.numeric(NA))) |&gt;\n  mutate(height = ifelse(is.na(height) & \n                           inches &lt; 12 & between(guess, smallest, tallest),\n                         guess, height)) |&gt;\n  select(-guess)\n\nWe can check all the entries we converted by typing:\n\nnew_heights |&gt;\n  filter(not_inches(original)) |&gt;\n  select(original, height) |&gt; \n  arrange(height) |&gt;\n  View()\n\nA final observation is that if we look at the shortest students in our course:\n\nnew_heights |&gt; arrange(height) |&gt; head(n = 7)\n\n           time_stamp    sex height feet inches original\n1 2017-07-04 01:30:25   Male  50.00   NA     NA       50\n2 2017-09-07 10:40:35   Male  50.00   NA     NA       50\n3 2014-09-02 15:18:30 Female  51.00   NA     NA       51\n4 2016-06-05 14:07:20 Female  52.00   NA     NA       52\n5 2016-06-05 14:07:38 Female  52.00   NA     NA       52\n6 2014-09-23 03:39:56 Female  53.00   NA     NA       53\n7 2015-01-07 08:57:29   Male  53.77   NA     NA    53.77\n\n\nWe see heights of 53, 54, and 55. In the originals, we also have 51 and 52. These short heights are rare and it is likely that the students actually meant 5'1, 5'2, 5'3, 5'4, and 5'5. Because we are not completely sure, we will leave them as reported. The object new_heights contains our final solution for this case study.\n\n\n13.3.9 String splitting\nAnother very common data wrangling operation is string splitting. To illustrate how this comes up, we start with an illustrative example. Suppose we did not have the function read_csv or read.csv available to us. We instead have to read a csv file using the base R function readLines like this:\n\nfilename &lt;- system.file(\"extdata/murders.csv\", package = \"dslabs\")\nlines &lt;- readLines(filename)\n\nThis function reads-in the data line-by-line to create a vector of strings. In this case, one string for each row in the spreadsheet. The first six lines are:\n\nlines |&gt; head()\n\n[1] \"state,abb,region,population,total\" \"Alabama,AL,South,4779736,135\"     \n[3] \"Alaska,AK,West,710231,19\"          \"Arizona,AZ,West,6392017,232\"      \n[5] \"Arkansas,AR,South,2915918,93\"      \"California,CA,West,37253956,1257\" \n\n\nWe want to extract the values that are separated by a comma for each string in the vector. The command str_split does exactly this:\n\nx &lt;- str_split(lines, \",\") \nx |&gt; head(2)\n\n[[1]]\n[1] \"state\"      \"abb\"        \"region\"     \"population\" \"total\"     \n\n[[2]]\n[1] \"Alabama\" \"AL\"      \"South\"   \"4779736\" \"135\"    \n\n\n\n\n13.3.10 Recoding\nAnother common operation involving strings is recoding the names of categorical variables. Let’s say you have really long names for your levels and you will be displaying them in plots, you might want to use shorter versions of these names. For example, in character vectors with country names, you might want to change “United States of America” to “USA” and “United Kingdom” to UK, and so on. We can do this with case_when, although the tidyverse offers an option that is specifically designed for this task: the case_match function.\nHere is an example that shows how to rename countries with long names:\n\nlibrary(dslabs)\n\nSuppose we want to show life expectancy time series by country for the Caribbean:\n\ngapminder |&gt; \n  filter(region == \"Caribbean\") |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\n\ngapminder |&gt; filter(region == \"Caribbean\") |&gt;\n  mutate(country = case_match(country, \n                          \"Antigua and Barbuda\" ~ \"Barbuda\",\n                          \"Dominican Republic\" ~ \"DR\",\n                          \"St. Vincent and the Grenadines\" ~ \"St. Vincent\",\n                          \"Trinidad and Tobago\" ~ \"Trinidad\",\n                          .default = country)) |&gt;\n  ggplot(aes(year, life_expectancy, color = country)) +\n  geom_line()\n\n\n\n\nThere are other similar functions in other R packages, such as recode_factor and fct_recoder in the forcats package."
  },
  {
    "objectID": "13-wrangling.html#exercises",
    "href": "13-wrangling.html#exercises",
    "title": "13  Wrangling",
    "section": "13.4 Exercises",
    "text": "13.4 Exercises\n\nRun the following command to define the co2_wide object:\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = as.character(1959:1997))\n\nUse the pivot_longer function to wrangle this into a tidy dataset. Call the column with the CO2 measurements co2 and call the month column month. Call the resulting object co2_tidy.\n\nPlot CO2 versus month with a different curve for each year using this code:\n\n\nco2_tidy |&gt; ggplot(aes(month, co2, color = year)) + geom_line()\n\nIf the expected plot is not made, it is probably because co2_tidy$month is not numeric:\n\nclass(co2_tidy$month)\n\nRewrite your code to make sure the month column is numeric. Then make the plot.\n\nWhat do we learn from this plot?\n\n\nCO2 measures increase monotonically from 1959 to 1997.\nCO2 measures are higher in the summer and the yearly average increased from 1959 to 1997.\nCO2 measures appear constant and random variability explains the differences.\nCO2 measures do not have a seasonal trend.\n\n\nNow load the admissions data set, which contains admission information for men and women across six majors and keep only the admitted percentage column:\n\n\nload(admissions)\ndat &lt;- admissions |&gt; select(-applicants)\n\nIf we think of an observation as a major, and that each observation has two variables (men admitted percentage and women admitted percentage) then this is not tidy. Use the pivot_wider function to wrangle into tidy shape: one row for each major.\n\nNow we will try a more advanced wrangling challenge. We want to wrangle the admissions data so that for each major we have 4 observations: admitted_men, admitted_women, applicants_men and applicants_women. The trick we perform here is actually quite common: first use pivot_longer to generate an intermediate data frame and then pivot_wider to obtain the tidy data we want. We will go step by step in this and the next two exercises.\n\nUse the pivot_longer function to create a tmp data.frame with a column containing the type of observation admitted or applicants. Call the new columns name and value.\n\nNow you have an object tmp with columns major, gender, name and value. Note that if you combine the name and gender, we get the column names we want: admitted_men, admitted_women, applicants_men and applicants_women. Use the function unite to create a new column called column_name.\nNow use the pivot_wider function to generate the tidy data with four variables for each major.\nNow use the pipe to write a line of code that turns admissions to the table produced in the previous exercise.\nInstall and load the Lahman library. This database includes data related to baseball teams. It includes summary statistics about how the players performed on offense and defense for several years. It also includes personal information about the players.\n\nThe Batting data frame contains the offensive statistics for all players for many years. You can see, for example, the top 10 hitters by running this code:\n\nlibrary(Lahman)\n\ntop &lt;- Batting |&gt; \n  filter(yearID == 2016) |&gt;\n  arrange(desc(HR)) |&gt;\n  slice(1:10)\n\ntop |&gt; as_tibble()\n\nBut who are these players? We see an ID, but not the names. The player names are in this table\n\nPeople |&gt; as_tibble()\n\nWe can see column names nameFirst and nameLast. Use the left_join function to create a table of the top home run hitters. The table should have playerID, first name, last name, and number of home runs (HR). Rewrite the object top with this new table.\n\nNow use the Salaries data frame to add each player’s salary to the table you created in exercise 1. Note that salaries are different every year so make sure to filter for the year 2016, then use right_join. This time show first name, last name, team, HR, and salary.\nIn a previous exercise, we created a tidy version of the co2 dataset:\n\n\nco2_wide &lt;- data.frame(matrix(co2, ncol = 12, byrow = TRUE)) |&gt; \n  setNames(1:12) |&gt;\n  mutate(year = 1959:1997) |&gt;\n  pivot_longer(-year, names_to = \"month\", values_to = \"co2\") |&gt;\n  mutate(month = as.numeric(month))\n\nWe want to see if the monthly trend is changing so we are going to remove the year effects and then plot the results. We will first compute the year averages. Use the group_by and summarize to compute the average co2 for each year. Save in an object called yearly_avg.\n\nNow use the left_join function to add the yearly average to the co2_wide dataset. Then compute the residuals: observed co2 measure - yearly average.\nMake a plot of the seasonal trends by year but only after removing the year effect."
  },
  {
    "objectID": "13-wrangling.html#footnotes",
    "href": "13-wrangling.html#footnotes",
    "title": "13  Wrangling",
    "section": "",
    "text": "https://www.rstudio.com/wp-content/uploads/2016/09/RegExCheatsheet.pdf↩︎"
  },
  {
    "objectID": "14-web-scraping.html#html",
    "href": "14-web-scraping.html#html",
    "title": "14  Web scraping",
    "section": "14.1 HTML",
    "text": "14.1 HTML\nHere is the key piece of code:\n&lt;table class=\"wikitable sortable\"&gt;\n&lt;tr&gt;\n&lt;th&gt;State&lt;/th&gt;\n&lt;th&gt;&lt;a href=\"/wiki/List_of_U.S._states_and_territories_by_population\" \ntitle=\"List of U.S. states and territories by population\"&gt;Population&lt;/a&gt;&lt;br /&gt;\n&lt;small&gt;(total inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-1\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-1\"&gt;[1]&lt;/a&gt;&lt;/sup&gt;&lt;/th&gt;\n&lt;th&gt;Murders and Nonnegligent\n&lt;p&gt;Manslaughter&lt;br /&gt;\n&lt;small&gt;(total deaths)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt; &lt;sup id=\"cite_ref-2\" class=\"reference\"&gt;\n&lt;a href=\"#cite_note-2\"&gt;[2]&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;th&gt;Murder and Nonnegligent\n&lt;p&gt;Manslaughter Rate&lt;br /&gt;\n&lt;small&gt;(per 100,000 inhabitants)&lt;/small&gt;&lt;br /&gt;\n&lt;small&gt;(2015)&lt;/small&gt;&lt;/p&gt;\n&lt;/th&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alabama\" title=\"Alabama\"&gt;Alabama&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;4,853,875&lt;/td&gt;\n&lt;td&gt;348&lt;/td&gt;\n&lt;td&gt;7.2&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;\n&lt;td&gt;&lt;a href=\"/wiki/Alaska\" title=\"Alaska\"&gt;Alaska&lt;/a&gt;&lt;/td&gt;\n&lt;td&gt;737,709&lt;/td&gt;\n&lt;td&gt;59&lt;/td&gt;\n&lt;td&gt;8.0&lt;/td&gt;\n&lt;/tr&gt;\n&lt;tr&gt;"
  },
  {
    "objectID": "14-web-scraping.html#the-rvest-package",
    "href": "14-web-scraping.html#the-rvest-package",
    "title": "14  Web scraping",
    "section": "14.2 The rvest package",
    "text": "14.2 The rvest package\nThe tidyverse provides a web harvesting package called rvest. The first step using this package is to import the webpage into R. The package makes this quite simple:\n\nlibrary(tidyverse)\nlibrary(rvest)\nh &lt;- read_html(url)\n\nNote that the entire Murders in the US Wikipedia webpage is now contained in h. The class of this object is:\n\nclass(h)\n\n[1] \"xml_document\" \"xml_node\"    \n\n\nThe rvest package is actually more general; it handles XML documents. XML is a general markup language (that’s what the ML stands for) that can be used to represent any kind of data. HTML is a specific type of XML specifically developed for representing webpages. Here we focus on HTML documents.\nNow, how do we extract the table from the object h? If we print h, we don’t really see much:\n\nh\n\n{html_document}\n&lt;html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-sticky-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-zebra-design-disabled vector-feature-custom-font-size-clientpref-disabled vector-feature-client-preferences-disabled\" lang=\"en\" dir=\"ltr\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body class=\"skin-vector skin-vector-search-vue mediawiki ltr sitedir-ltr ...\n\n\nWe can see all the code that defines the downloaded webpage using the html_text function like this:\n\nhtml_text(h)\n\nIf we look at it, we can see the data we are after are stored in an HTML table: you can see this in this line of the HTML code above &lt;table class=\"wikitable sortable\"&gt;. The different parts of an HTML document, often defined with a message in between &lt; and &gt; are referred to as nodes. The rvest package includes functions to extract nodes of an HTML document: html_nodes extracts all nodes of different types and html_node extracts the first one. To extract the tables from the html code we use:\n\ntab &lt;- h |&gt; html_nodes(\"table\")\n\nNow, instead of the entire webpage, we just have the html code for the tables in the page:\n\ntab\n\n{xml_nodeset (2)}\n[1] &lt;table class=\"wikitable sortable\"&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n ...\n[2] &lt;table class=\"nowraplinks hlist mw-collapsible mw-collapsed navbox-inner\" ...\n\n\nThe table we are interested is the first one:\n\ntab[[1]]\n\n{html_node}\n&lt;table class=\"wikitable sortable\"&gt;\n[1] &lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;State\\n&lt;/th&gt;\\n&lt;th&gt;\\n&lt;a href=\"/wiki/List_of_U.S._states ...\n\n\nThis is clearly not a tidy dataset, not even a data frame. In the code above, you can definitely see a pattern and writing code to extract just the data is very doable. In fact, rvest includes a function just for converting HTML tables into data frames:\n\ntab &lt;- tab[[1]] |&gt; html_table()\nclass(tab)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe are now much closer to having a usable data table:\n\ntab &lt;- tab |&gt; setNames(c(\"state\", \"population\", \"total\", \"murder_rate\")) \nhead(tab)\n\n# A tibble: 6 × 4\n  state      population total murder_rate\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt;\n1 Alabama    4,853,875  348           7.2\n2 Alaska     737,709    59            8  \n3 Arizona    6,817,565  309           4.5\n4 Arkansas   2,977,853  181           6.1\n5 California 38,993,940 1,861         4.8\n6 Colorado   5,448,819  176           3.2\n\n\nWe still have some wrangling to do. For example, we need to remove the commas and turn characters into numbers. Before continuing with this, we will learn a more general approach to extracting information from web sites."
  },
  {
    "objectID": "14-web-scraping.html#sec-css-selectors",
    "href": "14-web-scraping.html#sec-css-selectors",
    "title": "14  Web scraping",
    "section": "14.3 CSS selectors",
    "text": "14.3 CSS selectors\nThe default look of a webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS to define the look and style of webpages. The fact that all pages for a company have the same style usually results from their use of the same CSS file to define the style. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin. CSS does this by leveraging patterns used to define these elements, referred to as selectors. An example of such a pattern, which we used above, is table, but there are many, many more.\nIf we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page containing this data, we can use the html_nodes function. However, knowing which selector can be quite complicated. In fact, the complexity of webpages has been increasing as they become more sophisticated. For some of the more advanced ones, it seems almost impossible to find the nodes that define a particular piece of data. However, selector gadgets actually make this possible.\nSelectorGadget1 is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables from html pages, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget and then, as you click through the page, it highlights parts and shows you the selector you need to extract these parts. There are various demos of how to do this including rvest author Hadley Wickham’s vignette2 and other tutorials based on the vignette3 4."
  },
  {
    "objectID": "14-web-scraping.html#json",
    "href": "14-web-scraping.html#json",
    "title": "14  Web scraping",
    "section": "14.4 JSON",
    "text": "14.4 JSON\nSharing data on the internet has become more and more common. Unfortunately, providers use different formats, which makes it harder for data scientists to wrangle data into R. Yet there are some standards that are also becoming more common. Currently, a format that is widely being adopted is the JavaScript Object Notation or JSON. Because this format is very general, it is nothing like a spreadsheet. This JSON file looks more like the code you use to define a list. Here is an example of information stored in a JSON format:\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n[\n  {\n    \"name\": \"Miguel\",\n    \"student_id\": 1,\n    \"exam_1\": 85,\n    \"exam_2\": 86\n  },\n  {\n    \"name\": \"Sofia\",\n    \"student_id\": 2,\n    \"exam_1\": 94,\n    \"exam_2\": 93\n  },\n  {\n    \"name\": \"Aya\",\n    \"student_id\": 3,\n    \"exam_1\": 87,\n    \"exam_2\": 88\n  },\n  {\n    \"name\": \"Cheng\",\n    \"student_id\": 4,\n    \"exam_1\": 90,\n    \"exam_2\": 91\n  }\n] \n\n\nThe file above actually represents a data frame. To read it, we can use the function fromJSON from the jsonlite package. Note that JSON files are often made available via the internet. Several organizations provide a JSON API or a web service that you can connect directly to and obtain data. Here is an example providing information Nobel prize winners:\n\nnobel &lt;- fromJSON(\"http://api.nobelprize.org/v1/prize.json\")\n\nThis downloads a list. The first argument is a table with information about Nobel prize winners:\n\nfilter(nobel$prize, category == \"literature\" & year == \"1971\") |&gt; pull(laureates)\n\n[[1]]\n   id firstname surname\n1 645     Pablo  Neruda\n                                                                                               motivation\n1 \"for a poetry that with the action of an elemental force brings alive a continent's destiny and dreams\"\n  share\n1     1\n\n\nYou can learn much more by examining tutorials and help files from the jsonlite package. This package is intended for relatively simple tasks such as converting data into tables. For more flexibility, we recommend rjson."
  },
  {
    "objectID": "14-web-scraping.html#apis",
    "href": "14-web-scraping.html#apis",
    "title": "14  Web scraping",
    "section": "14.5 APIs",
    "text": "14.5 APIs\nAn API, or Application Programming Interface, is a set of rules and protocols that allows different software entities to communicate with each other. It defines methods and data formats that software components should use when requesting and exchanging information.\nAPIs can be understood as middlemen between different software systems, facilitating their interactions. They play a crucial role in enabling integration that make today’s software so interconnected and versatile.\nThere are several types of APIs. The main ones related to retrieving data are:\n\nWeb Services:\n\nOften built using protocols like HTTP/HTTPS.\nCommonly used to enable applications to communicate with each other over the web. For instance, a weather application for a smartphone may use a web API to request weather data from a remote server.\n\nDatabases:\n\nEnable communication between an application and a database.\nExamples include SQL-based calls or more abstracted ORM (Object-Relational Mapping) frameworks.\n\n\nOther APIs include: * Hardware APIs: Used for applications to interact with hardware, such as printers, cameras, or graphics cards.\n\nRemote Procedure Calls: Allows a protocol to execute a program or procedure on a remote server rather than on a local system.\nLibrary or Framework APIs: Define how to interact with a particular programming library or framework.\nOperating Systems:Define how software applications request and perform lower-level services performed by an operating system.\n\nKey concepts associated with APIs:\n\nEndpoints: Specific functions available through the API. For web APIs, an endpoint is usually a specific URL where the API can be accessed.\nMethods: Actions that can be performed. In web APIs, these often correspond to HTTP methods like GET, POST, PUT, DELETE.\nRequests and Responses: The act of asking the API to perform its function is a request, and the data it returns to you is the response.\nRate Limits: Restrictions on how often you can call the API, often used to prevent abuse or overloading of the service.\nAuthentication and Authorization: Mechanisms to ensure that only approved users or applications can use the API. Common methods include API keys, OAuth, or JWT.\nData Formats: Many web APIs exchange data in a specific format, often JSON or XML.\n\nIn today’s digital age, APIs are foundational. They enable the creation of complex, feature-rich applications by allowing different software components to leverage each other’s capabilities seamlessly."
  },
  {
    "objectID": "14-web-scraping.html#httr-package",
    "href": "14-web-scraping.html#httr-package",
    "title": "14  Web scraping",
    "section": "14.6 httr package",
    "text": "14.6 httr package\nThe httr package provides functions to work with HTTP requests. One of the core functions in this package is GET(), which is used to send GET requests to web servers.\nThe GET() function sends an HTTP GET request to the specified URL. Typically, HTTP GET requests are used to retrieve information from a server based on the provided URL.\nThe function returns an object of class response. This object contains all the details of the server’s response, including status code, headers, and content. You can then use other httr functions to extract or interpret information from this response.\nLet’s say you want to retrieve a webpage or API endpoint:\n\nlibrary(httr)\n\nurl &lt;- \"https://data.cdc.gov/resource/gvsb-yw6g.json\"\nresponse &lt;- GET(url)\nprint(status_code(response))\n\n[1] 200\n\ncontent_data &lt;- content(response, \"parsed\")\ntab &lt;- fromJSON(content(response, as = \"text\"), flatten = TRUE)\ndim(tab)\n\n[1] 1000   10\n\n\nNote it is only 1000 entries. API often limit how much you can download. Additional Notes:\n\nresponse &lt;- GET(paste0(url, \"?$limit=10000000\"))\ntab &lt;- fromJSON(content(response, as = \"text\"), flatten = TRUE)\ndim(tab)\n\n[1] 19873    10\n\n\nThe httr package provides several utility functions to work with the response object, such as content(), status_code(), and headers().\nFor making requests other than GET, httr offers functions like POST(), PUT(), DELETE(), etc.\nWhen working with APIs, it’s essential to check the API’s documentation for rate limits, required headers, or authentication methods. The httr package provides tools to handle these requirements, such as setting headers or using OAuth authentication."
  },
  {
    "objectID": "14-web-scraping.html#exercises-will-not-be-included-in-midterm",
    "href": "14-web-scraping.html#exercises-will-not-be-included-in-midterm",
    "title": "14  Web scraping",
    "section": "14.7 Exercises (will not be included in midterm)",
    "text": "14.7 Exercises (will not be included in midterm)\n\nVisit the following web page: https://web.archive.org/web/20181024132313/http://www.stevetheump.com/Payrolls.htm\n\nNotice there are several tables. Say we are interested in comparing the payrolls of teams across the years. The next few exercises take us through the steps needed to do this.\nStart by applying what you learned to read in the website into an object called h.\n\nNote that, although not very useful, we can actually see the content of the page by typing:\n\n\nhtml_text(h)\n\nThe next step is to extract the tables. For this, we can use the html_nodes function. We learned that tables in html are associated with the table node. Use the html_nodes function and the table node to extract the first table. Store it in an object nodes.\n\nThe html_nodes function returns a list of objects of class xml_node. We can see the content of each one using, for example, the html_text function. You can see the content for an arbitrarily picked component like this:\n\n\nhtml_text(nodes[[8]])\n\nIf the content of this object is an html table, we can use the html_table function to convert it to a data frame. Use the html_table function to convert the 8th entry of nodes into a table.\n\nRepeat the above for the first 4 components of nodes. Which of the following are payroll tables:\n\n\nAll of them.\n1\n2\n2-4\n\n\nRepeat the above for the first last 3 components of nodes. Which of the following is true:\n\n\nThe last entry in nodes shows the average across all teams through time, not payroll per team.\nAll three are payroll per team tables.\nAll three are like the first entry, not a payroll table.\nAll of the above.\n\n\nWe have learned that the first and last entries of nodes are not payroll tables. Redefine nodes so that these two are removed.\nWe saw in the previous analysis that the first table node is not actually a table. This happens sometimes in html because tables are used to make text look a certain way, as opposed to storing numeric values. Remove the first component and then use sapply and html_table to convert each node in nodes into a table. Note that in this case, sapply will return a list of tables. You can also use lapply to assure that a list is applied.\nLook through the resulting tables. Are they all the same? Could we just join them with bind_rows?\nCreate two tables, call them tab_1 and tab_2 using entries 10 and 19.\nUse a full_join function to combine these two tables. Before you do this you will have to fix the missing header problem. You will also need to make the names match.\nAfter joining the tables, you see several NAs. This is because some teams are in one table and not the other. Use the anti_join function to get a better idea of why this is happening.\nWe see see that one of the problems is that Yankees are listed as both N.Y. Yankees and NY Yankees. In the next section, we will learn efficient approaches to fixing problems like this. Here we can do it “by hand” as follows:\n\n\ntab_1 &lt;- tab_1 |&gt;\n  mutate(Team = ifelse(Team == \"N.Y. Yankees\", \"NY Yankees\", Team))\n\nNow join the tables and show only Oakland and the Yankees and the payroll columns."
  },
  {
    "objectID": "14-web-scraping.html#footnotes",
    "href": "14-web-scraping.html#footnotes",
    "title": "14  Web scraping",
    "section": "",
    "text": "http://selectorgadget.com/↩︎\nhttps://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html↩︎\nhttps://stat4701.github.io/edav/2015/04/02/rvest_tutorial/↩︎\nhttps://www.analyticsvidhya.com/blog/2017/03/beginners-guide-on-web-scraping-in-r-using-rvest-with-hands-on-knowledge/↩︎"
  },
  {
    "objectID": "15-locales.html",
    "href": "15-locales.html",
    "title": "15  Locales",
    "section": "",
    "text": "Notice the character on this file.\n\nfn &lt;- file.path(system.file(\"extdata\", package = \"dslabs\"), \"calificaciones.csv\")\nreadLines(fn)\n\n[1] \"\\\"nombre\\\",\\\"f.n.\\\",\\\"estampa\\\",\\\"puntuaci\\xf3n\\\"\"                       \n[2] \"\\\"Beyonc\\xe9\\\",\\\"04 de septiembre de 1981\\\",2023-09-22 02:11:02,\\\"87,5\\\"\"\n[3] \"\\\"Bl\\xfcmchen\\\",\\\"20 de abril de 1980\\\",2023-09-22 03:23:05,\\\"99,0\\\"\"    \n[4] \"\\\"Jo\\xe3o\\\",\\\"10 de junio de 1931\\\",2023-09-21 22:43:28,\\\"98,9\\\"\"        \n[5] \"\\\"L\\xf3pez\\\",\\\"24 de julio de 1969\\\",2023-09-22 01:06:59,\\\"88,7\\\"\"       \n[6] \"\\\"\\xd1engo\\\",\\\"15 de diciembre de 1981\\\",2023-09-21 23:35:37,\\\"93,1\\\"\"   \n[7] \"\\\"Pl\\xe1cido\\\",\\\"24 de enero de 1941\\\",2023-09-21 23:17:21,\\\"88,7\\\"\"     \n[8] \"\\\"Thal\\xeda\\\",\\\"26 de agosto de 1971\\\",2023-09-21 23:08:02,\\\"83,0\\\"\"     \n\ntry({x &lt;- read_csv(\"inst/extdata/calificaciones.csv\")})\n\nError in read_csv(\"inst/extdata/calificaciones.csv\") : \n  could not find function \"read_csv\"\n\n\nThis is because it is not UTF encoding, which is the default:\n\nSys.getlocale()\n\n[1] \"en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\"\n\n\nThe locale is a group of information about the system. This includes the encoding, the language, and the time zone.\nYou can use this funciton to guess the encoding of a character\n\nx &lt;- readLines(fn, n = 1)\nstringi::stri_enc_detect(x)\n\n[[1]]\n    Encoding Language Confidence\n1 ISO-8859-1       es       0.75\n2 ISO-8859-2       cs       0.18\n3   UTF-16BE                0.10\n4   UTF-16LE                0.10\n5  Shift_JIS       ja       0.10\n6    GB18030       zh       0.10\n7       Big5       zh       0.10\n\n\nWe can also use this readr function to detect encoding of files:\n\nlibrary(readr)\nguess_encoding(fn)\n\n# A tibble: 3 × 2\n  encoding   confidence\n  &lt;chr&gt;           &lt;dbl&gt;\n1 ISO-8859-1       0.92\n2 ISO-8859-2       0.72\n3 ISO-8859-9       0.53\n\n\nThe read_csv permits us the define an econding:\n\nx &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\nnum  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nx\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02        875\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05        990\n3 João     10 de junio de 1931      2023-09-21 22:43:28        989\n4 López    24 de julio de 1969      2023-09-22 01:06:59        887\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37        931\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21        887\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02        830\n\n\nHowever, notice the last column. Compare it to what we saw with read lines. They were numbers that used the European decimal point. We can also change the encoding for that:\n\nx &lt;- read_csv(fn, locale = locale(encoding = \"ISO-8859-1\", decimal_mark = \",\"))\n\nRows: 7 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): nombre, f.n.\ndbl  (1): puntuación\ndttm (1): estampa\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nx\n\n# A tibble: 7 × 4\n  nombre   f.n.                     estampa             puntuación\n  &lt;chr&gt;    &lt;chr&gt;                    &lt;dttm&gt;                   &lt;dbl&gt;\n1 Beyoncé  04 de septiembre de 1981 2023-09-22 02:11:02       87.5\n2 Blümchen 20 de abril de 1980      2023-09-22 03:23:05       99  \n3 João     10 de junio de 1931      2023-09-21 22:43:28       98.9\n4 López    24 de julio de 1969      2023-09-22 01:06:59       88.7\n5 Ñengo    15 de diciembre de 1981  2023-09-21 23:35:37       93.1\n6 Plácido  24 de enero de 1941      2023-09-21 23:17:21       88.7\n7 Thalía   26 de agosto de 1971     2023-09-21 23:08:02       83  \n\n\nNow let’s try to change the dates to date format:\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\ndmy(x$f.n.)\n\nWarning: All formats failed to parse. No formats found.\n\n\n[1] NA NA NA NA NA NA NA\n\n\nThis is because it is in spanish. You can change the locale to use spanish as the language:\n\nparse_date(x$f.n., format = \"%d de %B de %Y\", locale = locale(date_names = \"es\"))\n\n[1] \"1981-09-04\" \"1980-04-20\" \"1931-06-10\" \"1969-07-24\" \"1981-12-15\"\n[6] \"1941-01-24\" \"1971-08-26\"\n\n\nFinally notice that two students turned in the homework past the deadline of september 21\n\nx$estampa &gt;= make_date(2023, 9, 22)\n\n[1]  TRUE  TRUE FALSE  TRUE FALSE FALSE FALSE\n\n\nBut these times are in GMT. If we change to out timezone:\n\nwith_tz(x$estampa, tz =  Sys.timezone()) &gt;= make_date(2023, 9, 22)\n\n[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\nwe see everybody turned it in on time."
  },
  {
    "objectID": "16-text-mining.html#case-study-trump-tweets",
    "href": "16-text-mining.html#case-study-trump-tweets",
    "title": "16  Text mining",
    "section": "16.1 Case study: Trump tweets",
    "text": "16.1 Case study: Trump tweets\nDuring the 2016 US presidential election, then candidate Donald J. Trump used his twitter account as a way to communicate with potential voters. On August 6, 2016, Todd Vaziri tweeted1 about Trump that “Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him).” Data scientist David Robinson conducted an analysis2 to determine if data supported this assertion. Here, we go through David’s analysis to learn some of the basics of text mining. To learn more about text mining in R, we recommend the Text Mining with R book3 by Julia Silge and David Robinson.\nWe will use the following libraries:\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(scales)\n\nIn general, we can extract data directly from Twitter using the rtweet package. However, in this case, a group has already compiled data for us and made it available at http://www.trumptwitterarchive.com. We can get the data from their JSON API using a script like this:\n\nurl &lt;- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'\ntrump_tweets &lt;- map(2009:2017, ~sprintf(url, .x)) |&gt;\n  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) |&gt;\n  filter(!is_retweet & !str_detect(text, '^\"')) |&gt;\n  mutate(created_at = parse_date_time(created_at, \n                                      orders = \"a b! d! H!:M!:S! z!* Y!\",\n                                      tz=\"EST\")) \n\nFor convenience, we include the result of the code above in the dslabs package:\n\nlibrary(dslabs)\n\nYou can see the data frame with information about the tweets by typing\n\nhead(trump_tweets)\n\nwith the following variables included:\n\nnames(trump_tweets)\n\n[1] \"source\"                  \"id_str\"                 \n[3] \"text\"                    \"created_at\"             \n[5] \"retweet_count\"           \"in_reply_to_user_id_str\"\n[7] \"favorite_count\"          \"is_retweet\"             \n\n\nThe help file ?trump_tweets provides details on what each variable represents. The tweets are represented by the text variable:\n\ntrump_tweets$text[16413] |&gt; str_wrap(width = options()$width) |&gt; cat()\n\nGreat to be back in Iowa! #TBT with @JerryJrFalwell joining me in Davenport-\nthis past winter. #MAGA https://t.co/A5IF0QHnic\n\n\nand the source variable tells us which device was used to compose and upload each tweet:\n\ntrump_tweets |&gt; count(source) |&gt; arrange(desc(n)) |&gt; head(5)\n\n               source     n\n1  Twitter Web Client 10718\n2 Twitter for Android  4652\n3  Twitter for iPhone  3962\n4           TweetDeck   468\n5     TwitLonger Beta   288\n\n\nWe are interested in what happened during the campaign, so for this analysis we will focus on what was tweeted between the day Trump announced his campaign and election day. We define the following table containing just the tweets from that time period. Note that we use extract to remove the Twitter for part of the source and filter out retweets.\n\ncampaign_tweets &lt;- trump_tweets |&gt; \n  extract(source, \"source\", \"Twitter for (.*)\") |&gt;\n  filter(source %in% c(\"Android\", \"iPhone\") &\n           created_at &gt;= ymd(\"2015-06-17\") & \n           created_at &lt; ymd(\"2016-11-08\")) |&gt;\n  filter(!is_retweet) |&gt;\n  arrange(created_at) |&gt; \n  as_tibble()\n\nWe can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, East Coast time (EST), it was tweeted and then compute the proportion of tweets tweeted at each hour for each device:\n\ncampaign_tweets |&gt;\n  mutate(hour = hour(with_tz(created_at, \"EST\"))) |&gt;\n  count(source, hour) |&gt;\n  group_by(source) |&gt;\n  mutate(percent = n / sum(n)) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(hour, percent, color = source)) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = percent_format()) +\n  labs(x = \"Hour of day (EST)\", y = \"% of tweets\", color = \"\")\n\n\n\n\nWe notice a big peak for the Android in the early hours of the morning, between 6 and 8 AM. There seems to be a clear difference in these patterns. We will therefore assume that two different entities are using these two devices.\nWe will now study how the tweets differ when we compare Android to iPhone. To do this, we introduce the tidytext package."
  },
  {
    "objectID": "16-text-mining.html#text-as-data",
    "href": "16-text-mining.html#text-as-data",
    "title": "16  Text mining",
    "section": "16.2 Text as data",
    "text": "16.2 Text as data\nThe tidytext package helps us convert free form text into a tidy table. Having the data in this format greatly facilitates data visualization and the use of statistical techniques.\n\nlibrary(tidytext)\n\nThe main function needed to achieve this is unnest_tokens. A token refers to a unit that we are considering to be a data point. The most common token will be words, but they can also be single characters, ngrams, sentences, lines, or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example:\n\npoem &lt;- c(\"Roses are red,\", \"Violets are blue,\", \n          \"Sugar is sweet,\", \"And so are you.\")\nexample &lt;- tibble(line = c(1, 2, 3, 4),\n                      text = poem)\nexample\n\n# A tibble: 4 × 2\n   line text             \n  &lt;dbl&gt; &lt;chr&gt;            \n1     1 Roses are red,   \n2     2 Violets are blue,\n3     3 Sugar is sweet,  \n4     4 And so are you.  \n\nexample |&gt; unnest_tokens(word, text)\n\n# A tibble: 13 × 2\n    line word   \n   &lt;dbl&gt; &lt;chr&gt;  \n 1     1 roses  \n 2     1 are    \n 3     1 red    \n 4     2 violets\n 5     2 are    \n 6     2 blue   \n 7     3 sugar  \n 8     3 is     \n 9     3 sweet  \n10     4 and    \n11     4 so     \n12     4 are    \n13     4 you    \n\n\nNow let’s look at an example from the tweets. We will look at tweet number 3008 because it will later permit us to illustrate a couple of points:\n\ni &lt;- 3008\ncampaign_tweets$text[i] |&gt; str_wrap(width = 65) |&gt; cat()\n\nGreat to be back in Iowa! #TBT with @JerryJrFalwell joining me in\nDavenport- this past winter. #MAGA https://t.co/A5IF0QHnic\n\ncampaign_tweets[i,] |&gt; \n  unnest_tokens(word, text) |&gt;\n  pull(word) \n\n [1] \"great\"          \"to\"             \"be\"             \"back\"          \n [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n[13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n[17] \"maga\"           \"https\"          \"t.co\"           \"a5if0qhnic\"    \n\n\nNote that the function tries to convert tokens into words. A minor adjustment is to remove the links to pictures:\n\nlinks &lt;- \"https://t.co/[A-Za-z\\\\d]+|&amp;\"\ncampaign_tweets[i,] |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  pull(word)\n\n [1] \"great\"          \"to\"             \"be\"             \"back\"          \n [5] \"in\"             \"iowa\"           \"tbt\"            \"with\"          \n [9] \"jerryjrfalwell\" \"joining\"        \"me\"             \"in\"            \n[13] \"davenport\"      \"this\"           \"past\"           \"winter\"        \n[17] \"maga\"          \n\n\nNow we are now ready to extract the words for all our tweets.\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text)\n\nAnd we can now answer questions such as “what are the most commonly used words?”:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  arrange(desc(n))\n\n# A tibble: 6,264 × 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the    2330\n 2 to     1413\n 3 and    1245\n 4 in     1190\n 5 i      1151\n 6 a      1121\n 7 you     999\n 8 of      982\n 9 is      944\n10 on      880\n# ℹ 6,254 more rows\n\n\nIt is not surprising that these are the top words. The top words are not informative. The tidytext package has a database of these commonly used words, referred to as stop words, in text mining:\n\nstop_words\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   &lt;chr&gt;       &lt;chr&gt;  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# ℹ 1,139 more rows\n\n\nIf we filter out rows representing stop words with filter(!word %in% stop_words$word):\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word ) \n\nwe end up with a much more informative set of top 10 tweeted words:\n\ntweet_words |&gt; \n  count(word) |&gt;\n  top_n(10, n) |&gt;\n  mutate(word = reorder(word, n)) |&gt;\n  arrange(desc(n))\n\n# A tibble: 10 × 2\n   word                      n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 trump2016               415\n 2 hillary                 407\n 3 people                  304\n 4 makeamericagreatagain   298\n 5 america                 255\n 6 clinton                 240\n 7 poll                    220\n 8 crooked                 205\n 9 trump                   204\n10 cruz                    161\n\n\nSome exploration of the resulting words (not shown here) reveals a couple of unwanted characteristics in our tokens. First, some of our tokens are just numbers (years, for example). We want to remove these and we can find them using the regex ^\\d+$. Second, some of our tokens come from a quote and they start with '. We want to remove the ' when it is at the start of a word so we will just str_replace. We add these two lines to the code above to generate our final table:\n\ntweet_words &lt;- campaign_tweets |&gt; \n  mutate(text = str_replace_all(text, links, \"\"))  |&gt;\n  unnest_tokens(word, text) |&gt;\n  filter(!word %in% stop_words$word &\n           !str_detect(word, \"^\\\\d+$\")) |&gt;\n  mutate(word = str_replace(word, \"^'\", \"\"))\n\nNow that we have all our words in a table, along with information about what device was used to compose the tweet they came from, we can start exploring which words are more common when comparing Android to iPhone.\nFor each word, we want to know if it is more likely to come from an Android tweet or an iPhone tweet. We therefore compute, for each word, what proportion of all words it represent for Android and iPhone, respectively.\n\nandroid_vs_iphone &lt;- tweet_words |&gt;\n  count(word, source) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\", values_fill = 0) |&gt;\n  mutate(p_a = Android / sum(Android), p_i = iPhone / sum(iPhone),\n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100)\n\nFor words appearing at least 100 times in total, here are the highest percent differences for Android\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 30 × 6\n   word        Android iPhone     p_a     p_i percent_diff\n   &lt;chr&gt;         &lt;int&gt;  &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 bad             104     26 0.00645 0.00188        110. \n 2 crooked         156     49 0.00968 0.00354         92.9\n 3 cnn             116     37 0.00720 0.00267         91.7\n 4 ted              86     28 0.00533 0.00202         90.1\n 5 interviewed      76     25 0.00471 0.00180         89.3\n 6 media            78     26 0.00484 0.00188         88.2\n 7 cruz            115     46 0.00713 0.00332         72.9\n 8 hillary         290    117 0.0180  0.00845         72.2\n 9 win              74     30 0.00459 0.00217         71.8\n10 president        84     35 0.00521 0.00253         69.4\n# ℹ 20 more rows\n\n\nand the top for iPhone:\n\nandroid_vs_iphone |&gt; filter(Android + iPhone &gt;= 100) |&gt; \n  arrange(percent_diff)\n\n# A tibble: 30 × 6\n   word                  Android iPhone       p_a     p_i percent_diff\n   &lt;chr&gt;                   &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1 makeamericagreatagain       0    298 0         0.0215       -200   \n 2 join                        1    157 0.0000620 0.0113       -198.  \n 3 trump2016                   3    412 0.000186  0.0297       -198.  \n 4 tomorrow                   24    101 0.00149   0.00729      -132.  \n 5 vote                       46     67 0.00285   0.00484       -51.6 \n 6 america                   114    141 0.00707   0.0102        -36.0 \n 7 tonight                    70     84 0.00434   0.00606       -33.1 \n 8 iowa                       62     65 0.00385   0.00469       -19.8 \n 9 poll                      117    103 0.00726   0.00744        -2.43\n10 trump                     112     92 0.00695   0.00664         4.49\n# ℹ 20 more rows\n\n\nWe already see somewhat of a pattern in the types of words that are being tweeted more from one device versus the other. However, we are not interested in specific words but rather in the tone. Vaziri’s assertion is that the Android tweets are more hyperbolic. So how can we check this with data? Hyperbolic is a hard sentiment to extract from words as it relies on interpreting phrases. However, words can be associated to more basic sentiment such as anger, fear, joy, and surprise. In the next section, we demonstrate basic sentiment analysis."
  },
  {
    "objectID": "16-text-mining.html#sentiment-analysis",
    "href": "16-text-mining.html#sentiment-analysis",
    "title": "16  Text mining",
    "section": "16.3 Sentiment analysis",
    "text": "16.3 Sentiment analysis\nIn sentiment analysis, we assign a word to one or more “sentiments”. Although this approach will miss context-dependent sentiments, such as sarcasm, when performed on large numbers of words, summaries can provide insights.\nThe first step in sentiment analysis is to assign a sentiment to each word. As we demonstrate, the tidytext package includes several maps or lexicons. We will also be using the textdata package.\n\nlibrary(tidytext)\nlibrary(textdata)\n\nThe bing lexicon divides words into positive and negative sentiments. We can see this using the tidytext function get_sentiments:\n\nget_sentiments(\"bing\")\n\nThe AFINN lexicon assigns a score between -5 and 5, with -5 the most negative and 5 the most positive. Note that this lexicon needs to be downloaded the first time you call the function get_sentiment:\n\nget_sentiments(\"afinn\")\n\nThe loughran and nrc lexicons provide several different sentiments. Note that these also have to be downloaded the first time you use them.\n\nget_sentiments(\"loughran\") |&gt; count(sentiment)\n\n# A tibble: 6 × 2\n  sentiment        n\n  &lt;chr&gt;        &lt;int&gt;\n1 constraining   184\n2 litigious      904\n3 negative      2355\n4 positive       354\n5 superfluous     56\n6 uncertainty    297\n\n\n\nget_sentiments(\"nrc\") |&gt; count(sentiment)\n\n# A tibble: 10 × 2\n   sentiment        n\n   &lt;chr&gt;        &lt;int&gt;\n 1 anger         1245\n 2 anticipation   837\n 3 disgust       1056\n 4 fear          1474\n 5 joy            687\n 6 negative      3316\n 7 positive      2308\n 8 sadness       1187\n 9 surprise       532\n10 trust         1230\n\n\nFor our analysis, we are interested in exploring the different sentiments of each tweet so we will use the nrc lexicon:\n\nnrc &lt;- get_sentiments(\"nrc\") |&gt;\n  select(word, sentiment)\n\nWe can combine the words and sentiments using inner_join, which will only keep words associated with a sentiment. Here are 10 random words extracted from the tweets:\n\ntweet_words |&gt; inner_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt; \n  select(source, word, sentiment) |&gt; \n  sample_n(5)\n\n# A tibble: 5 × 3\n  source  word     sentiment   \n  &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;       \n1 Android enjoy    joy         \n2 iPhone  terrific sadness     \n3 iPhone  tactics  trust       \n4 Android clue     anticipation\n5 iPhone  change   fear        \n\n\nNow we are ready to perform a quantitative analysis comparing Android and iPhone by comparing the sentiments of the tweets posted from each device. Here we could perform a tweet-by-tweet analysis, assigning a sentiment to each tweet. However, this will be challenging since each tweet will have several sentiments attached to it, one for each word appearing in the lexicon. For illustrative purposes, we will perform a much simpler analysis: we will count and compare the frequencies of each sentiment appearing in each device.\n\nsentiment_counts &lt;- tweet_words |&gt;\n  left_join(nrc, by = \"word\", relationship = \"many-to-many\") |&gt;\n  count(source, sentiment) |&gt;\n  pivot_wider(names_from = \"source\", values_from = \"n\") |&gt;\n  mutate(sentiment = replace_na(sentiment, replace = \"none\"))\nsentiment_counts\n\n# A tibble: 11 × 3\n   sentiment    Android iPhone\n   &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;\n 1 anger            962    527\n 2 anticipation     917    707\n 3 disgust          639    314\n 4 fear             799    486\n 5 joy              695    536\n 6 negative        1657    931\n 7 positive        1827   1494\n 8 sadness          901    514\n 9 surprise         530    365\n10 trust           1248   1001\n11 none           11834  10793\n\n\nFor each sentiment, we can compute the percent difference in proportion for Android compared to iPhone:\n\nsentiment_counts |&gt;\n  mutate(p_a = Android / sum(Android) , \n         p_i = iPhone / sum(iPhone), \n         percent_diff = (p_a - p_i) / ((p_a + p_i)/2) * 100) |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 11 × 6\n   sentiment    Android iPhone    p_a    p_i percent_diff\n   &lt;chr&gt;          &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;        &lt;dbl&gt;\n 1 disgust          639    314 0.0290 0.0178      48.1   \n 2 anger            962    527 0.0437 0.0298      37.8   \n 3 negative        1657    931 0.0753 0.0527      35.3   \n 4 sadness          901    514 0.0409 0.0291      33.8   \n 5 fear             799    486 0.0363 0.0275      27.6   \n 6 surprise         530    365 0.0241 0.0207      15.3   \n 7 anticipation     917    707 0.0417 0.0400       4.04  \n 8 joy              695    536 0.0316 0.0303       4.01  \n 9 trust           1248   1001 0.0567 0.0567       0.0846\n10 positive        1827   1494 0.0830 0.0846      -1.85  \n11 none           11834  10793 0.538  0.611      -12.7   \n\n\nSo we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative!\nIf we are interested in exploring which specific words are driving these differences, we can refer back to our android_iphone_or object:\n\nandroid_vs_iphone |&gt; inner_join(nrc, by = \"word\") |&gt;\n  filter(sentiment == \"disgust\") |&gt;\n  arrange(desc(percent_diff))\n\n# A tibble: 157 × 7\n   word      Android iPhone       p_a   p_i percent_diff sentiment\n   &lt;chr&gt;       &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;    \n 1 abuse           1      0 0.0000620     0          200 disgust  \n 2 angry          10      0 0.000620      0          200 disgust  \n 3 arrogant        2      0 0.000124      0          200 disgust  \n 4 attacking       5      0 0.000310      0          200 disgust  \n 5 belittle        2      0 0.000124      0          200 disgust  \n 6 blame           1      0 0.0000620     0          200 disgust  \n 7 bleeding        1      0 0.0000620     0          200 disgust  \n 8 bombed          5      0 0.000310      0          200 disgust  \n 9 clumsy          1      0 0.0000620     0          200 disgust  \n10 crushed         1      0 0.0000620     0          200 disgust  \n# ℹ 147 more rows\n\n\nand we can make a graph:\n\n\n\n\n\nThis is just a simple example of the many analyses one can perform with tidytext. To learn more, we again recommend the Tidy Text Mining book4."
  },
  {
    "objectID": "16-text-mining.html#exercises-will-not-be-included-in-midterm",
    "href": "16-text-mining.html#exercises-will-not-be-included-in-midterm",
    "title": "16  Text mining",
    "section": "16.4 Exercises (will not be included in midterm)",
    "text": "16.4 Exercises (will not be included in midterm)\nProject Gutenberg is a digital archive of public domain books. The R package gutenbergr facilitates the importation of these texts into R.\nYou can install and load by typing:\n\ninstall.packages(\"gutenbergr\")\nlibrary(gutenbergr)\n\nYou can see the books that are available like this:\n\ngutenberg_metadata\n\n\nUse str_detect to find the ID of the novel Pride and Prejudice.\nWe notice that there are several versions. The gutenberg_works() function filters this table to remove replicates and include only English language works. Read the help file and use this function to find the ID for Pride and Prejudice.\n\n\nUse the gutenberg_download function to download the text for Pride and Prejudice. Save it to an object called book.\n\n\nUse the tidytext package to create a tidy table with all the words in the text. Save the table in an object called words\nWe will later make a plot of sentiment versus location in the book. For this, it will be useful to add a column with the word number to the table.\nRemove the stop words and numbers from the words object. Hint: use the anti_join.\nNow use the AFINN lexicon to assign a sentiment value to each word.\nMake a plot of sentiment score versus location in the book and add a smoother.\nAssume there are 300 words per page. Convert the locations to pages and then compute the average sentiment in each page. Plot that average score by page. Add a smoother that appears to go through data."
  },
  {
    "objectID": "16-text-mining.html#footnotes",
    "href": "16-text-mining.html#footnotes",
    "title": "16  Text mining",
    "section": "",
    "text": "https://twitter.com/tvaziri/status/762005541388378112/photo/1↩︎\nhttp://varianceexplained.org/r/trump-tweets/↩︎\nhttps://www.tidytextmining.com/↩︎\nhttps://www.tidytextmining.com/↩︎"
  }
]